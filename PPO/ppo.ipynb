{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO\n",
    "\n",
    "In short: PPO (Proximal Policy Optimization) is a widely used policy gradient method for reinforcement learning. PPO is an improved variant of TRPO (Trust Region Policy Optimization) which is itself an improved variant of the original policy gradient method. PPO was released in 2017 by OpenAI in the paper [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347).\n",
    "\n",
    "The reason it's so popular is that it offers much better performance than the vanilla policy gradient, while still being relatively simple to implement and tune.\n",
    "\n",
    "In this article, we'll attempt to:\n",
    "1. Explain the flaws of REINFORCE, which TRPO and PPO attempt to solve.\n",
    "2. Explain the intuition behind TRPO and PPO.\n",
    "3. Explain how PPO improves upon TRPO.\n",
    "4. Explain the PPO loss function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flaws of REINFORCE\n",
    "\n",
    "We covered REINFORCE in [a previous entry](../PolicyGradient/policygradient.ipynb), so we won't go into too much detail here. However, let's briefly review how it works.\n",
    "\n",
    "#### REINFORCE Algorithm\n",
    "1. We initialize the policy parameters $\\theta$ randomly.\n",
    "2. We collect a batch of trajectories $D = \\{\\tau_1, \\tau_2, \\ldots, \\tau_n\\}$ by running the policy in the environment.\n",
    "3. We compute an estimate of the policy gradient, $\\hat{g}$:\n",
    "    * Recall that the policy gradient is given by:\n",
    "        $$\n",
    "        \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\hat{R}_t(\\tau_i) \\right]\n",
    "        $$\n",
    "    * REINFORCE estimates the policy gradient using Monte Carlo sampling:\n",
    "        $$\n",
    "        \\hat{g} = \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_{i,t}|s_{i,t}) \\hat{R}_t(\\tau_i)\n",
    "        $$\n",
    "4. We then update the policy using gradient ascent:\n",
    "    $$\n",
    "    \\theta \\leftarrow \\theta + \\alpha \\hat{g}\n",
    "    $$\n",
    "5. Repeat steps 2-4 until convergence.\n",
    "\n",
    "Reference implementations:\n",
    "* [Discrete](../PolicyGradient/policygradient_discrete_solution.ipynb)\n",
    "* [Continuous](../PolicyGradient/policygradient_continuous_solution.ipynb)\n",
    "\n",
    "#### Problems with REINFORCE\n",
    "As you may have observed when implementing REINFORCE on Metadrive, it is very slow to train, and the final policies we ended up with were often quite poor.\n",
    "\n",
    "This is because REINFORCE has two major flaws:\n",
    "1. The estimate of the policy gradient has high variance.\n",
    "2. We can't use old trajectories to improve the policy, so we need to collect new trajectories every time we want to update the policy.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Noisy Policy Gradient Estimates\n",
    "The first problem is that the estimate of the policy gradient has high variance. (When a value has high variance, it is often referred to as \"noisy\"). What this means in practice is that the policy gradient estimate can be very different from the true policy gradient, which can lead to poor performance, as we might be updating the policy in the wrong direction.\n",
    "\n",
    "Since we compute the policy gradient estimate using Monte Carlo sampling, we can mitigate this by increasing the number of samples we use. However, this comes at the cost of increased computation time.\n",
    "\n",
    "#### One Solution: Replacing $\\hat{R}_t(\\tau_i)$ with $A^{\\pi_\\theta}(s_t, a_t)$\n",
    "When we [first derived the policy gradient](../PolicyGradient/policygradient.ipynb) it had the form:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) R(\\tau_i) \\right]\n",
    "$$\n",
    "The return $R(\\tau_i)$ is the sum of the rewards in the trajectory $\\tau_i$:\n",
    "$$\n",
    "R(\\tau_i) = \\sum_{t=0}^T r_t\n",
    "$$\n",
    "However, we could replace it with the reward-to-go $\\hat{R}_t(\\tau_i)$, which is defined as:\n",
    "$$\n",
    "\\hat{R}_t(\\tau_i) = \\sum_{t'=t}^T r_{t'}\n",
    "$$.\n",
    "It turns out that there are other valid replacements for $R(\\tau_i)$ that can reduce the variance of the policy gradient estimate. One such replacement is the advantage function $A^{\\pi_\\theta}(s_t, a_t)$, which is defined as:\n",
    "$$\n",
    "A^{\\pi_\\theta}(s_t, a_t) = Q^{\\pi_\\theta}(s_t, a_t) - V^{\\pi_\\theta}(s_t)\n",
    "$$\n",
    "where $Q^{\\pi_\\theta}(s_t, a_t)$ is the state-action value function and $V^{\\pi_\\theta}(s_t)$ is the state value function. \n",
    "\n",
    "Recall that:\n",
    "* $V^{\\pi_\\theta}(s_t) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ R(\\tau) | s_0=s_t \\right]$\n",
    "    * Called the \"Value Function\".\n",
    "    * Measures how good it is to be in state $s_t$, assuming we follow policy $\\pi_\\theta$.\n",
    "* $Q^{\\pi_\\theta}(s_t, a_t) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ R(\\tau) | s_0=s_t, a_0=a_t \\right]$\n",
    "    * Called the \"Action-Value Function\", or \"Q-Function\".\n",
    "    * Measures how good it is to take action $a_t$ in state $s_t$, and then follow policy $\\pi_\\theta$ afterwards.\n",
    "\n",
    "So, what the advantage function measures is how much better it is to take action $a_t$ in state $s_t$ than it is to follow our policy. Let's take a look at an example to see why this is useful.\n",
    "\n",
    "#### Example: Advantage Function\n",
    "Assume that we are playing in the gridworld below, and that the game ends in exactly one step.\n",
    "\n",
    "![gridworld](./gridworld_example.png)\n",
    "\n",
    "If we choose to move left, we get a return of $+2$. If we choose to move right, we get a return of $-2$.\n",
    "\n",
    "Our policy is:\n",
    "$$\n",
    "\\pi_{\\text{rand}}(a_t|s_t) = \\begin{cases}\n",
    "    0.5 & \\text{if } a_t = \\text{left} \\\\\n",
    "    0.5 & \\text{if } a_t = \\text{right}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* What is $A^{\\pi_{\\text{rand}}}(s_0, \\text{left})$?\n",
    "    * $V^{\\pi_{\\text{rand}}}(s_0) = 0.5(2) + 0.5(-2) = 0$\n",
    "    * $Q^{\\pi_{\\text{rand}}}(s_0, \\text{left}) = 2$\n",
    "    * $A^{\\pi_{\\text{rand}}}(s_0, \\text{left}) = Q^{\\pi_{\\text{rand}}}(s_0, \\text{left}) - V^{\\pi_{\\text{rand}}}(s_0)$\n",
    "    * Therefore, $A^{\\pi_{\\text{rand}}}(s_0, \\text{left}) = 2 - 0 = 2$\n",
    "* What is $A^{\\pi_{\\text{rand}}}(s_0, \\text{right})$?\n",
    "    * $V^{\\pi_{\\text{rand}}}(s_0) = 0.5(2) + 0.5(-2) = 0$\n",
    "    * $Q^{\\pi_{\\text{rand}}}(s_0, \\text{right}) = -2$\n",
    "    * $A^{\\pi_{\\text{rand}}}(s_0, \\text{right}) = Q^{\\pi_{\\text{rand}}}(s_0, \\text{right}) - V^{\\pi_{\\text{rand}}}(s_0)$\n",
    "    * Therefore, $A^{\\pi_{\\text{rand}}}(s_0, \\text{right}) = -2 - 0 = -2$\n",
    "\n",
    "#### Why is the Advantage Function Useful?\n",
    "The advantage function is useful because it is a lower variance replacement for reward-to-go in the policy gradient algorithm. Let's take a moment to discuss a few reasons why this is the case.\n",
    "1. The Q-Function already takes into account the variance in the rewards we get from following our policy. This is because the Q-Function is the **expected** return we get from taking action $a_t$ in state $s_t$, and then following our policy afterwards.\n",
    "    * To illustrate this with an example, imagine that in our environment, there's a low probability chance that we can suffer a large negative reward at any point. If we take a good action, but then suffer a large negative reward, the return we get will be low, even though it was a good action. However, the Q-Function will be high, because it takes into account the fact that we will usually get a high reward for taking that action.\n",
    "2. By subtracting the value function from the Q-Function, we center the advantage function around zero. This means that the advantage function will be positive when the Q-Function is higher than the value function, and negative when the Q-Function is lower than the value function. This makes sense, because the model should reinforce actions that are better than expected, and discourage actions that are worse than expected. For actions that are as good as expected, the advantage function will be zero, and the policy gradient will not be affected.\n",
    "\n",
    "#### Calculating Advantage\n",
    "\n",
    "In practice, we cannot calculate the advantage function directly, because we do not know the true Q-Function or value function. However, we can estimate the advantage function using the following formula, as described by Equation 18 in [Schulman et al. (2015)](https://arxiv.org/abs/1506.02438):\n",
    "$$\n",
    "A^{\\pi_\\theta}(s_t, a_t) \\approx \\sum_{t'=t}^T \\gamma^{t'-t} r_{t'} - V^{\\pi_\\theta}(s_t)\n",
    "$$\n",
    "Here, $V^{\\pi_\\theta}(s_t)$ is the value function, which we can estimate using a neural network. This is called the **critic** network. The neural network takes in a state $s_t$ as input, and outputs an estimate of the value function $V^{\\pi_\\theta}(s_t)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Need to collect new trajectories after each policy update\n",
    "In RL, we often categorize algorithms as \"on-policy\" or \"off-policy\":\n",
    "* **On-policy**: The algorithm can only train on data that was collected using the current policy.\n",
    "* **Off-policy**: The algorithm can train on data that was collected using any policy.\n",
    "\n",
    "REINFORCE is an on-policy algorithm. Whenever we update the model, we can't use our trajectories from before the update, and we need to gather new ones. This is because we compute the policy gradient using the expected value over trajectories sampled from the current policy ($\\mathbb{E}_{\\tau \\sim \\pi_{\\theta_\\text{current}}}$). If we used trajectories from before the update, we would be computing the policy gradient using the expected value over trajectories sampled from the old policy ($\\mathbb{E}_{\\tau \\sim \\pi_{\\theta_\\text{old}}}$), which is not what we want.\n",
    "\n",
    "This makes REINFORCE sample-inefficient compared to other algorithms, like Deep Q-Learning, which we discussed in the previous notebook.\n",
    "\n",
    "If our environment is expensive to run, this can be a problem. For example, if we are training a robot to walk, we might need to run the robot in the real world, which is time consuming and costly. With REINFORCE though, the only solution would be to increase the learning rate. This would make the model learn faster, but it would also make the model more unstable.\n",
    "\n",
    "#### Why does a high learning rate make the model unstable?\n",
    "Recall the model update rule:\n",
    "$$\n",
    "\\theta_{\\text{new}} = \\theta_{\\text{old}} + \\alpha \\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "If we increase the learning rate $\\alpha$, then the model parameters $\\theta$ will change more with each update. The risk is that we might \"overshoot\" the optimal parameters, and end up with a worse model than we started with. We can visualize this below:\n",
    "\n",
    "![high learning rate](./high_learning_rate.png)\n",
    "\n",
    "This is the problem that TRPO and PPO try to solve. They attempt to find a way to increase the \"effective learning rate\" without making the model unstable, by limiting how much the model's behavior can change between updates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRPO: Trust Region Policy Optimization\n",
    "\n",
    "*Note: Check out this article: https://www.depthfirstlearning.com/2018/TRPO if you want to go more in depth than we do here.*\n",
    "\n",
    "We start off with TRPO, both because it came first chronologically, and because it has stronger conceptual foundations. TRPO is based on the idea of a **trust region**. A trust region is a region around the current policy, where we can be confident that the policy gradient is a good approximation of the true policy gradient. \n",
    "\n",
    "#### Trust Regions\n",
    "\n",
    "Illustration of the concept of a trust region:\n",
    "\n",
    "![trust region](./trust_region.png)\n",
    "\n",
    "In this example, if we wanted to find the value of $x$ that minimizes $y$, we could use the linear approximation only within the trust region (between the dotted lines). This would ensure that we don't overshoot the minimum. Assuming we can find a way to correctly determine the bounds of the trust region, this lets us optimize the function using gradient descent without fear of overshooting the minimum. We just have to make sure that the gradient doesn't take us outside of the trust region.\n",
    "\n",
    "Our optimization procedure would be:\n",
    "1. Compute the gradient of $y$ with respect to $x$.\n",
    "2. Compute the trust region bounds.\n",
    "3. Take a step in the direction of the gradient, but only within the trust region.\n",
    "4. Repeat until convergence.\n",
    "\n",
    "#### KL Divergence\n",
    "\n",
    "TRPO uses the notion of KL divergence to define the trust region. **KL divergence** is a measure of how different two probability distributions are.\n",
    "\n",
    "It is defined as:\n",
    "$$\n",
    "D_{KL}(P||Q) = \\sum_{x \\in X} P(x) \\log \\frac{P(x)}{Q(x)}\n",
    "$$\n",
    "where $P$ and $Q$ are two probability distributions over some set $X$.\n",
    "\n",
    "KL divergence has a lot of neat properties that you can read about on the [Wikipedia page](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).\n",
    "\n",
    "We won't go into too much depth here since it's not relevant to our understanding, but some of the more important points to note are:\n",
    "1. KL divergence is not symmetric, so $D_{KL}(P||Q) \\neq D_{KL}(Q||P)$.\n",
    "2. KL divergence is always non-negative, so $D_{KL}(P||Q) \\geq 0$.\n",
    "\n",
    "In this case, we want to measure how different the old policy $\\pi_{\\theta_\\text{old}}$ is from the new policy $\\pi_{\\theta_\\text{new}}$. We optimize the loss function with the hard constraint that the KL divergence between the old and new policies must be less than some threshold $\\delta$. The policies that satisfy this constraint are within the trust region.\n",
    "\n",
    "#### TRPO Algorithm\n",
    "With that in mind, here is the TRPO algorithm, as described by [Schulman et al. (2015)](https://arxiv.org/abs/1502.05477):\n",
    "\n",
    "1. Initialize policy parameters $\\theta_0$.\n",
    "2. Collect a set of state-action pairs $\\mathcal{D}$ using the current policy $\\pi_{\\theta_\\text{old}}$ along with Monte Carlo estimates of their Q-values.\n",
    "4. Approximately solve the following constrained optimization problem for $\\theta$:\n",
    "$$\n",
    "\\text{maximize } \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}, a \\sim \\pi_\\theta} \\left[ \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_\\text{old}}(a|s)} Q^{\\pi_{\\theta_\\text{old}}}(s, a) \\right] \\\\\n",
    "\\text{subject to } \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}} \\left[ D_{KL}(\\pi_{\\theta_\\text{old}}(\\cdot|s) || \\pi_\\theta(\\cdot|s)) \\right] \\leq \\delta\n",
    "$$\n",
    "    where $\\rho_{\\theta_\\text{old}}$ is the state distribution under the old policy $\\pi_{\\theta_\\text{old}}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
