{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Continous Policy Gradient Algorithm\n",
    "\n",
    "In the [previous notebook](policygradient_discrete_solution.ipynb), we implemented the policy gradient algorithm for a discrete action space. In this notebook, we will implement the policy gradient algorithm for a continous action space.\n",
    "\n",
    "First, let's review the central training loop of policy gradient algorithms:\n",
    "\n",
    "* Collect $N$ trajectories: \n",
    "    * Do $N$ times:\n",
    "        * For $T$ timesteps:\n",
    "            * Collect observation $s_t$ from the environment\n",
    "            * **(1)** Randomly sample an action $a_t$ from the policy $\\pi_\\theta(a|s)$\n",
    "            * Execute the action $a_t$ in the environment, collecting the reward $r_t$\n",
    "* For each trajectory $\\tau_i$:\n",
    "    * For each timestep $t$ in $\\tau_i$:\n",
    "        * Compute the reward-to-go:\n",
    "            * $\\hat{R}_t(\\tau_i) = \\sum_{t'=t}^T \\gamma^{t'-t}r_{t'}$\n",
    "* Compute the policy gradient estimate:\n",
    "    * For each trajectory $\\tau_i$:\n",
    "        * For each timestep $t$ in $\\tau_i$:\n",
    "            * **(2)** Compute the log probability of the action chosen at that timestep:\n",
    "                * $\\log\\pi_\\theta(a_t|s_t)$\n",
    "        * Using the log-probability of the action and the reward-to-go (both computed above) compute the policy gradient of this trajectory:\n",
    "            * $\\sum_{t=0}^T \\nabla_\\theta \\log\\pi_\\theta(a_t|s_t) \\hat{R}_t(\\tau_i)$\n",
    "    * Compute the mean of the policy gradient estimates (Monte Carlo estimate of the policy gradient):\n",
    "        * $\\nabla_{\\theta} J(\\pi_{\\theta}) \\approx \\frac{1}{N}\\sum_{i=0}^N \\sum_{t=0}^T \\nabla_\\theta \\log\\pi_\\theta(a_t|s_t) \\hat{R}_t(\\tau_i)$\n",
    "* Update the policy parameters $\\theta$ using the policy gradient estimate:\n",
    "    * $\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta} J(\\pi_{\\theta})$\n",
    "    \n",
    "This loop stays the same for continous action spaces. The only difference is that we need to change the way we do **(1)** and **(2)** above.\n",
    "\n",
    "#### Discrete Case\n",
    "Let's think back to the discrete case. Our neural network output a set of probabilities, one for each action we could take. Formally, these probabilities formed a [categorical probability distribution](https://en.wikipedia.org/wiki/Categorical_distribution) over the actions.\n",
    "\n",
    "Here's how we accomplished **(1)** and **(2)**:\n",
    "* **(1)**: we used the `torch.multinomial` function, which samples from a categorical distribution.\n",
    "* **(2)**: all we had to do was take the log of the probability of the chosen action.\n",
    "#### Continous Case\n",
    "We can't do this in the continuous case, because there are an infinite number of actions we could take. Instead, we need to use a different type of probability distribution: a [multivariate Gaussian distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution). Our network will output the mean and standard deviation of this Gaussian distribution, and we will sample from it to get an action.\n",
    "\n",
    "Here's how we will accomplish **(1)** and **(2)**:\n",
    "* **(1)**: we will use the `torch.normal` function, which samples from a multivariate Gaussian distribution.\n",
    "* **(2)**: we will use the `torch.distributions.MultivariateNormal` class, which computes the log probability of a sample from a multivariate Gaussian distribution.\n",
    "\n",
    "### Why Gaussians?\n",
    "The reason we're talking about Gaussians in the first place is that we want to get a distribution we can sample from to generate a real number.\n",
    "The choice of distribution is purely empirical. We could choose some other probability function to sample from, and it might work.\n",
    "However, Gaussians are a very common choice for probability distributions, for the following reasons:\n",
    "1. Gaussians have nice mathematical properties. For example, the sum of two Gaussian random variables is also a Gaussian random variable.\n",
    "2. Gaussians are very common in nature, due to the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)\n",
    "3. Gaussians are unimodal, which is important for the network to learn stable policies.\n",
    "\n",
    "## Gaussians and their Properties\n",
    "\n",
    "Before we dive too deep into coding the implementation, let's review some properties of Gaussians, starting with 1D Gaussians.\n",
    "\n",
    "#### Univariate Gaussians\n",
    "The formula for a 1D Gaussian distribution is:\n",
    "$$\n",
    "\\mathcal{N}(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "Here's what it would look like if we plotted it:\n",
    "\n",
    "![1D Gaussian Distribution with labeled mean and standard deviation](./normdist01_big.JPG)\n",
    "\n",
    "*(Image source: https://www.nohsteachers.info/rlinden/statistics/Sect5/section5_4.htm )*\n",
    "\n",
    "A 1D gaussian is completely described by two parameters: the mean and the standard deviation. The mean ($\\mu$) is the center of the distribution. The standard deviation ($\\sigma$) is a measure of how spread out the distribution is.\n",
    "\n",
    "Since the Gaussian is a probability distribution, we are able to sample from it.\n",
    "We denote a random variable sampled from a Gaussian distribution with mean $\\mu$ and standard deviation $\\sigma$ as:\n",
    "$$\n",
    "X \\sim \\mathcal{N}(\\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "You are less likely to sample values that are far away from the mean, and more likely to sample values that are close to the mean.\n",
    "For 1D Gaussian distributions, the **empirical rule** offers a short description of how likely samples are to fall within a certain number of standard deviations from the mean:\n",
    "- 68% of the samples will be within 1 standard deviation of the mean\n",
    "- 95% of the samples will be within 2 standard deviations of the mean\n",
    "- 99.7% of the samples will be within 3 standard deviations of the mean\n",
    "\n",
    "#### Multivariate Gaussians\n",
    "We can have more than one dimension in a Gaussian distribution. These types of distributions are called multivariate Gaussian distributions. The formula for a multivariate Gaussian distribution is:\n",
    "$$\n",
    "\\mathcal{N}(\\mathbf{x}; \\mathbf{\\mu}, \\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^n|\\Sigma|}}\\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu})^T\\Sigma^{-1}(\\mathbf{x}-\\mathbf{\\mu})\\right)\n",
    "$$\n",
    "\n",
    "There are a couple of differences between the formulas for univariate and multivariate Gaussian distributions:\n",
    "1. $x$ is now a vector instead of a scalar. This means that the Gaussian is now a distribution over n-dimensional space instead of a distribution over a line.\n",
    "    * In the context of reinforcement learning, each element of $x$ corresponds to an output of the policy.\n",
    "    * In a policy controlling a robot arm, $x$ would be a vector of joint angles, one for each joint.   \n",
    "2. The mean is now a vector instead of a scalar. This means that the mean is now a point in n-dimensional space instead of a point on a line.\n",
    "3. The standard deviation is now a covariance matrix, denoted $\\Sigma$. Briefly, the covariance matrix is a matrix that describes the covariance between each pair of elements in $x$.\n",
    "    \n",
    "#### Covariance Matrix\n",
    "\n",
    "A covariance matrix is constructed as:\n",
    "$$\n",
    "\\Sigma = \\begin{bmatrix}\n",
    "\\sigma_{1}^2 & \\sigma_{1}\\sigma_{2} & \\sigma_{1}\\sigma_{3} & \\dots  & \\sigma_{1}\\sigma_{n} \\\\\n",
    "\\sigma_{2}\\sigma_{1} & \\sigma_{2}^2 & \\sigma_{2}\\sigma_{3} & \\dots  & \\sigma_{2}\\sigma_{n} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\sigma_{n}\\sigma_{1} & \\sigma_{n}\\sigma_{2} & \\sigma_{n}\\sigma_{3} & \\dots  & \\sigma_{n}^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The diagonal elements of the covariance matrix are the variances of each element of $x$. The off-diagonal elements are the covariances between each pair of elements of $x$.\n",
    "\n",
    "The covariance between two elements of $x$ is a measure of how much they change together. If the covariance is positive, then the two elements change together. If the covariance is negative, then the two elements change in opposite directions. If the covariance is zero, then the two elements are independent of each other.\n",
    "\n",
    "#### Diagonal Gaussian Policies\n",
    "\n",
    "In this notebook, we will use a diagonal Gaussian policy. A diagonal Gaussian policy is a Gaussian distribution where the covariance matrix is diagonal, and all the off-diagonal elements are zero. This means that the covariance between any two actions is zero. This means that the actions are independent of each other. \n",
    "\n",
    "This is a simplifying assumption that makes the math easier. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Now that we have the requisite background, let's implement the policy gradient algorithm for a continous action space.\n",
    "\n",
    "We'll first copy over all of the code that is unchanged from the discrete policy gradient solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully registered the following environments: ['MetaDrive-validation-v0', 'MetaDrive-10env-v0', 'MetaDrive-100envs-v0', 'MetaDrive-1000envs-v0', 'SafeMetaDrive-validation-v0', 'SafeMetaDrive-10env-v0', 'SafeMetaDrive-100envs-v0', 'SafeMetaDrive-1000envs-v0', 'MARLTollgate-v0', 'MARLBottleneck-v0', 'MARLRoundabout-v0', 'MARLIntersection-v0', 'MARLParkingLot-v0', 'MARLMetaDrive-v0'].\n"
     ]
    }
   ],
   "source": [
    "# We need to import metadrive to register the environments\n",
    "import metadrive\n",
    "import gymnasium as gym\n",
    "import typing\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def collect_trajectory(env:gym.Env, policy:typing.Callable[[npt.NDArray], tuple[float, float]]) -> tuple[list[npt.NDArray], list[tuple[float, float]], list[float]]:\n",
    "    \"\"\"\n",
    "    Collect a trajectory from the environment using the given policy\n",
    "    \"\"\"\n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        observations.append(obs)\n",
    "        action = policy(obs)\n",
    "        actions.append(action)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return observations, actions, rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.85\n",
    "\n",
    "def rewards_to_go(trajectory_rewards: list[float]) -> list[float]:\n",
    "    \"\"\"\n",
    "    Computes the gamma discounted reward-to-go for each state in the trajectory.\n",
    "    \"\"\"\n",
    "\n",
    "    trajectory_len = len(trajectory_rewards)\n",
    "\n",
    "    v_batch = np.zeros(trajectory_len)\n",
    "\n",
    "    v_batch[-1] = trajectory_rewards[-1]\n",
    "\n",
    "    # Use GAMMA to decay the advantage\n",
    "    for t in reversed(range(trajectory_len - 1)):\n",
    "        v_batch[t] = trajectory_rewards[t] + GAMMA * v_batch[t + 1]\n",
    "\n",
    "    return list(v_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deviceof(m: nn.Module) -> torch.device:\n",
    "    \"\"\"\n",
    "    Get the device of the given module\n",
    "    \"\"\"\n",
    "    return next(m.parameters()).device\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the network that outputs a multivariate Gaussian distribution. This is a Pytorch object which we can sample from to get an action, and which we can call to get the log probability of an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple policy network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(259, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.mu = nn.Linear(128, 2)\n",
    "        self.sigma = nn.Linear(128, 2)\n",
    "        \n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.distributions.MultivariateNormal:\n",
    "        \"\"\"\n",
    "        Forward pass of the policy network, returns a multivariate normal distribution\n",
    "        parameterized by a diagonal covariance matrix\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = F.tanh(self.mu(x))\n",
    "        sigma = F.softplus(self.sigma(x))\n",
    "        return torch.distributions.MultivariateNormal(mu, torch.diag_embed(sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_policy(net:Policy, obs:npt.NDArray) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    A neural network policy that returns an action based on the given observation\n",
    "    \"\"\"\n",
    "    # convert observation to a tensor\n",
    "    obs_tensor = torch.from_numpy(obs).float().to(deviceof(net))\n",
    "    # add batch dimension\n",
    "    obs_tensor = obs_tensor.unsqueeze(0)\n",
    "    # sample from the policy network\n",
    "    throttle, steer = net(obs_tensor).sample().squeeze(0)\n",
    "    return throttle.item(), steer.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTROPY_BONUS = 0.1\n",
    "\n",
    "def compute_policy_gradient_loss(\n",
    "    # Current policy network's distribution of actions given a state\n",
    "    # inner shape = (Batch, 2)\n",
    "    pi_theta_given_st: torch.distributions.MultivariateNormal,\n",
    "    # The action chosen by the policy network\n",
    "    # in (Batch, 2)\n",
    "    a_t: torch.Tensor,\n",
    "    # Rewards To Go for the chosen action\n",
    "    # in (Batch,)\n",
    "    R_t: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Computes the policy gradient loss for a vector of examples, and reduces with mean.\n",
    "\n",
    "    The standard policy gradient is given by the expected value over trajectories of:\n",
    "\n",
    "    :math:`\\sum_{t=0}^{T} \\nabla_{\\theta} (\\log \\pi_{\\theta}(a_t|s_t))R_t`\n",
    "    \n",
    "    where:\n",
    "    * :math:`\\pi_{\\theta}(a_t|s_t)` is the current policy's probability to perform action :math:`a_t` given :math:`s_t`\n",
    "    * :math:`R_t` is the rewards-to-go from the state at time t to the end of the episode from which it came.\n",
    "    \"\"\"\n",
    "\n",
    "    # Note: this loss has doesn't actually represent whether the action was good or bad\n",
    "    # it is a dummy loss, that is only used to compute the gradient\n",
    "\n",
    "    # Recall that the policy gradient for a single transition (state-action pair) is given by:\n",
    "    # $\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)R_t$\n",
    "    # However, it's easier to work with losses, rather than raw gradients.\n",
    "    # Therefore we construct a loss, that when differentiated, gives us the policy gradient.\n",
    "    # this loss is given by:\n",
    "    # $-\\log \\pi_{\\theta}(a_t|s_t)R_t$\n",
    "\n",
    "    # in (Batch,)\n",
    "    policy_loss_per_example = -pi_theta_given_st.log_prob(a_t) * R_t\n",
    "\n",
    "    # in (Batch,)\n",
    "    entropy_per_example = pi_theta_given_st.entropy()\n",
    "\n",
    "    # we reward entropy, since excessive certainty indicate the model is 'overfitting'\n",
    "    loss_per_example = policy_loss_per_example - ENTROPY_BONUS * entropy_per_example\n",
    "\n",
    "    # we take the average loss over all examples\n",
    "    return loss_per_example.mean()\n",
    "\n",
    "\n",
    "def train_policygradient(\n",
    "    policy: Policy,\n",
    "    policy_optimizer: torch.optim.Optimizer,\n",
    "    observation_batch: list[npt.NDArray],\n",
    "    action_batch: list[tuple[float, float]],\n",
    "    rtg_batch: list[float],\n",
    ") -> float:\n",
    "    # assert that the batch_lengths are the same\n",
    "    assert len(observation_batch) == len(action_batch)\n",
    "    assert len(observation_batch) == len(rtg_batch)\n",
    "\n",
    "    # get device\n",
    "    device = deviceof(policy)\n",
    "\n",
    "    # convert data to tensors on correct device\n",
    "\n",
    "    # in (Batch, Width)\n",
    "    observation_batch_tensor = torch.from_numpy(np.stack(observation_batch)).to(device)\n",
    "\n",
    "    # in (Batch,)\n",
    "    rtg_batch_tensor = torch.tensor(\n",
    "        rtg_batch, dtype=torch.float32, device=device\n",
    "    )\n",
    "\n",
    "    # in (Batch, 2)\n",
    "    chosen_action_tensor = torch.tensor(action_batch, device=device)\n",
    "\n",
    "    # train policy\n",
    "    policy_optimizer.zero_grad()\n",
    "    action_probs = policy.forward(observation_batch_tensor)\n",
    "    policy_loss = compute_policy_gradient_loss(\n",
    "        action_probs, chosen_action_tensor, rtg_batch_tensor\n",
    "    )\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "\n",
    "    # return the respective losses\n",
    "    return policy_loss.item()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we're done. Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable logging from metadrive\n",
    "import logging\n",
    "import inspect\n",
    "logging.getLogger(inspect.getfile(metadrive.envs.base_env)).setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "policy = Policy().to(device)\n",
    "policy_optimizer = torch.optim.Adam(policy.parameters(), lr=2e-4)\n",
    "\n",
    "step = 0\n",
    "rewards = []\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MetaDrive-validation-v0\", config={\"use_render\": False, \"horizon\": 500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13, Policy Loss: 0.328, Avg. Returns: 14.204\n",
      "Step 14, Policy Loss: 0.337, Avg. Returns: 14.736\n",
      "Step 15, Policy Loss: 0.316, Avg. Returns: 12.037\n",
      "Step 16, Policy Loss: 0.303, Avg. Returns: 12.338\n",
      "Step 17, Policy Loss: 0.398, Avg. Returns: 14.808\n",
      "Step 18, Policy Loss: 0.391, Avg. Returns: 14.741\n",
      "Step 19, Policy Loss: 0.360, Avg. Returns: 13.611\n",
      "Step 20, Policy Loss: 0.357, Avg. Returns: 12.046\n",
      "Step 21, Policy Loss: 0.384, Avg. Returns: 13.075\n",
      "Step 22, Policy Loss: 0.466, Avg. Returns: 15.970\n",
      "Step 23, Policy Loss: 0.406, Avg. Returns: 11.640\n",
      "Step 24, Policy Loss: 0.356, Avg. Returns: 9.889\n",
      "Step 25, Policy Loss: 0.431, Avg. Returns: 11.977\n",
      "Step 26, Policy Loss: 0.429, Avg. Returns: 13.449\n",
      "Step 27, Policy Loss: 0.545, Avg. Returns: 17.154\n",
      "Step 28, Policy Loss: 0.348, Avg. Returns: 10.667\n",
      "Step 29, Policy Loss: 0.417, Avg. Returns: 14.707\n",
      "Step 30, Policy Loss: 0.393, Avg. Returns: 12.205\n",
      "Step 31, Policy Loss: 0.351, Avg. Returns: 12.606\n",
      "Step 32, Policy Loss: 0.434, Avg. Returns: 13.214\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m trajectory_returns \u001b[39m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPISODES_PER_BATCH):\n\u001b[1;32m     13\u001b[0m     \u001b[39m# Collect trajectory\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     obs_traj, act_traj, rew_traj \u001b[39m=\u001b[39m collect_trajectory(env, \u001b[39mlambda\u001b[39;49;00m obs: nn_policy(policy, obs))\n\u001b[1;32m     15\u001b[0m     rtg_traj \u001b[39m=\u001b[39m rewards_to_go(rew_traj)\n\u001b[1;32m     17\u001b[0m     \u001b[39m# Update batch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m, in \u001b[0;36mcollect_trajectory\u001b[0;34m(env, policy)\u001b[0m\n\u001b[1;32m     23\u001b[0m action \u001b[39m=\u001b[39m policy(obs)\n\u001b[1;32m     24\u001b[0m actions\u001b[39m.\u001b[39mappend(action)\n\u001b[0;32m---> 25\u001b[0m obs, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     26\u001b[0m rewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     27\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/venvs/metadrive/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/venvs/metadrive/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/myworkspace/metadrive/metadrive/envs/metadrive_env.py:312\u001b[0m, in \u001b[0;36mMetaDriveEnv.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(MetaDriveEnv, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mstep(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    313\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_stop:\n\u001b[1;32m    314\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine\u001b[39m.\u001b[39mtaskMgr\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/myworkspace/metadrive/metadrive/envs/base_env.py:297\u001b[0m, in \u001b[0;36mBaseEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, actions: Union[np\u001b[39m.\u001b[39mndarray, Dict[AnyStr, np\u001b[39m.\u001b[39mndarray], \u001b[39mint\u001b[39m]):\n\u001b[1;32m    296\u001b[0m     actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_actions(actions)\n\u001b[0;32m--> 297\u001b[0m     engine_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_simulator(actions)\n\u001b[1;32m    298\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_step_return(actions, engine_info\u001b[39m=\u001b[39mengine_info)\n",
      "File \u001b[0;32m~/myworkspace/metadrive/metadrive/envs/base_env.py:326\u001b[0m, in \u001b[0;36mBaseEnv._step_simulator\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine\u001b[39m.\u001b[39mstep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mdecision_repeat\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    325\u001b[0m \u001b[39m# update states, if restore from episode data, position and heading will be force set in update_state() function\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m scene_manager_after_step_infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine\u001b[39m.\u001b[39;49mafter_step()\n\u001b[1;32m    327\u001b[0m \u001b[39mreturn\u001b[39;00m merge_dicts(\n\u001b[1;32m    328\u001b[0m     scene_manager_after_step_infos, scene_manager_before_step_infos, allow_new_keys\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, without_copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m )\n",
      "File \u001b[0;32m~/myworkspace/metadrive/metadrive/engine/base_engine.py:388\u001b[0m, in \u001b[0;36mBaseEngine.after_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanagers\u001b[39m.\u001b[39mkeys())[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrecord_manager\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRecord Manager should have lowest priority\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    387\u001b[0m \u001b[39mfor\u001b[39;00m manager \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanagers\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m--> 388\u001b[0m     new_step_info \u001b[39m=\u001b[39m manager\u001b[39m.\u001b[39;49mafter_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    389\u001b[0m     step_infos \u001b[39m=\u001b[39m concat_step_infos([step_infos, new_step_info])\n\u001b[1;32m    390\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterface\u001b[39m.\u001b[39mafter_step()\n",
      "File \u001b[0;32m~/myworkspace/metadrive/metadrive/manager/traffic_manager.py:100\u001b[0m, in \u001b[0;36mPGTrafficManager.after_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m v_to_remove \u001b[39m=\u001b[39m []\n\u001b[1;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_traffic_vehicles:\n\u001b[0;32m--> 100\u001b[0m     v\u001b[39m.\u001b[39;49mafter_step()\n\u001b[1;32m    101\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m v\u001b[39m.\u001b[39mon_lane:\n\u001b[1;32m    102\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m TrafficMode\u001b[39m.\u001b[39mTrigger:\n",
      "File \u001b[0;32m~/myworkspace/metadrive/metadrive/component/vehicle/base_vehicle.py:323\u001b[0m, in \u001b[0;36mBaseVehicle.after_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state_check()\n\u001b[1;32m    322\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_dist_to_left_right()\n\u001b[0;32m--> 323\u001b[0m step_energy, episode_energy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_energy_consumption()\n\u001b[1;32m    324\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_of_route \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_out_of_route()\n\u001b[1;32m    325\u001b[0m step_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_overtake_stat()\n",
      "File \u001b[0;32m~/myworkspace/metadrive/metadrive/component/vehicle/base_vehicle.py:350\u001b[0m, in \u001b[0;36mBaseVehicle._update_energy_consumption\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_energy_consumption\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    344\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m    The calculation method is from\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m    https://www.researchgate.net/publication/262182035_Reduction_of_Fuel_Consumption_and_Exhaust_Pollutant_Using_Intelligent_Transport_System\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[39m    default: 3rd gear, try to use ae^bx to fit it, dp: (90, 8), (130, 12)\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39m    :return: None\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     distance \u001b[39m=\u001b[39m norm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_position[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_position[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mposition[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m \u001b[39m1000\u001b[39m  \u001b[39m# km\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     step_energy \u001b[39m=\u001b[39m \u001b[39m3.25\u001b[39m \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39mpow(np\u001b[39m.\u001b[39me, \u001b[39m0.01\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspeed_km_h) \u001b[39m*\u001b[39m distance \u001b[39m/\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m    352\u001b[0m     \u001b[39m# step_energy is in Liter, we return mL\u001b[39;00m\n",
      "File \u001b[0;32m~/myworkspace/metadrive/metadrive/base_class/base_object.py:283\u001b[0m, in \u001b[0;36mBaseObject.position\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mposition\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 283\u001b[0m     \u001b[39mreturn\u001b[39;00m metadrive_vector(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49morigin\u001b[39m.\u001b[39;49mgetPos())\n",
      "File \u001b[0;32m~/myworkspace/metadrive/metadrive/utils/coordinates_shift.py:56\u001b[0m, in \u001b[0;36mmetadrive_vector\u001b[0;34m(position)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mTransform the position in Panda3d to MetaDrive world\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m:param position: Vec3, position in Panda3d\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m:return: 2d position\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m# return np.array([position[0], -position[1]])\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m# return position[0], -position[1]\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m Vector([position[\u001b[39m0\u001b[39;49m], position[\u001b[39m1\u001b[39;49m]])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TRAIN_EPOCHS = 500\n",
    "EPISODES_PER_BATCH = 20\n",
    "\n",
    "# Train\n",
    "while step < TRAIN_EPOCHS:\n",
    "    obs_batch:list[npt.NDArray[np.float32]] = []\n",
    "    act_batch:list[int] = []\n",
    "    rtg_batch:list[float] = []\n",
    "    \n",
    "    trajectory_returns = []\n",
    "\n",
    "    for _ in range(EPISODES_PER_BATCH):\n",
    "        # Collect trajectory\n",
    "        obs_traj, act_traj, rew_traj = collect_trajectory(env, lambda obs: nn_policy(policy, obs))\n",
    "        rtg_traj = rewards_to_go(rew_traj)\n",
    "\n",
    "        # Update batch\n",
    "        obs_batch.extend(obs_traj)\n",
    "        act_batch.extend(act_traj)\n",
    "        rtg_batch.extend(rtg_traj)\n",
    "\n",
    "        # Update trajectory returns\n",
    "        trajectory_returns.append(sum(rew_traj))\n",
    "\n",
    "    policy_loss = train_policygradient(\n",
    "        policy,\n",
    "        policy_optimizer,\n",
    "        obs_batch,\n",
    "        act_batch,\n",
    "        rtg_batch\n",
    "    )\n",
    "\n",
    "    print(f\"Step {step}, Policy Loss: {policy_loss:.3f}, Avg. Returns: {np.mean(trajectory_returns):.3f}\")\n",
    "    rewards.append(np.mean(trajectory_returns))\n",
    "    losses.append(policy_loss)\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Known pipe types:\n",
      "  glxGraphicsPipe\n",
      "(1 aux display modules not yet loaded.)\n",
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 16.225898815377573\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MetaDrive-validation-v0\", config={\"use_render\": True, \"horizon\": 500})\n",
    "obs, act, rew = collect_trajectory(env, lambda obs: nn_policy(policy, obs))\n",
    "env.close()\n",
    "\n",
    "print(\"Reward:\", sum(rew))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metadrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
