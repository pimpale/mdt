{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Continous Policy Gradient Algorithm\n",
    "\n",
    "In the [previous notebook](policygradient_discrete_solution.ipynb), we implemented the policy gradient algorithm for a discrete action space. In this notebook, we will implement the policy gradient algorithm for a continous action space.\n",
    "\n",
    "First, let's review a couple of key concepts from the previous notebooks.\n",
    "1. Policy gradient is a stochastic policy. This means that the policy outputs a probability distribution over actions.\n",
    "2. When we collect a trajectory, we sample actions from the probability distribution output by the policy.\n",
    "3. When we compute the loss function, we use the log probability of the actions that were sampled.\n",
    "\n",
    "In general, the policy gradient algorithm for a continous action space is very similar to the policy gradient algorithm for a discrete action space. The main difference is that we need to use a different way to represent the probability distribution (and this will change the way we do 2. and 3. above).\n",
    "\n",
    "In the discrete case, we used a softmax function to convert the output of the neural network into a probability distribution over the actions. In the continous case, we will use a Gaussian distribution to convert the output of the neural network into a probability distribution over the actions.\n",
    "\n",
    "#### Why Gaussians?\n",
    "The reason we're talking about Gaussians in the first place is that we want to get a distribution we can sample from to generate a real number.\n",
    "The choice of distribution is purely empirical. We could choose some other probability function to sample from, and it might work.\n",
    "However, Gaussians are a very common choice for probability distributions, for the following reasons:\n",
    "1. Gaussians have nice mathematical properties. For example, the sum of two Gaussian random variables is also a Gaussian random variable.\n",
    "2. Gaussians are very common in nature, due to the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)\n",
    "3. Gaussians are unimodal, which is important for the network to learn stable policies.\n",
    "\n",
    "## Gaussians and their Properties\n",
    "\n",
    "Before we dive too deep into coding the implementation, let's review some properties of Gaussians, starting with 1D Gaussians.\n",
    "\n",
    "#### Univariate Gaussians\n",
    "The formula for a 1D Gaussian distribution is:\n",
    "$$\n",
    "\\mathcal{N}(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "Here's what it would look like if we plotted it:\n",
    "\n",
    "![1D Gaussian Distribution with labeled mean and standard deviation](./normdist01_big.JPG)\n",
    "\n",
    "*(Image source: https://www.nohsteachers.info/rlinden/statistics/Sect5/section5_4.htm )*\n",
    "\n",
    "A 1D gaussian is completely described by two parameters: the mean and the standard deviation. The mean ($\\mu$) is the center of the distribution. The standard deviation ($\\sigma$) is a measure of how spread out the distribution is.\n",
    "\n",
    "Since the Gaussian is a probability distribution, we are able to sample from it.\n",
    "We denote a random variable sampled from a Gaussian distribution with mean $\\mu$ and standard deviation $\\sigma$ as:\n",
    "$$\n",
    "X \\sim \\mathcal{N}(\\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "The **empirical rule** states that for a 1D Gaussian distribution:\n",
    "- 68% of the samples will be within 1 standard deviation of the mean\n",
    "- 95% of the samples will be within 2 standard deviations of the mean\n",
    "- 99.7% of the samples will be within 3 standard deviations of the mean\n",
    "\n",
    "#### Multivariate Gaussians\n",
    "We can have more than one dimension in a Gaussian distribution. These types of distributions are called multivariate Gaussian distributions. The formula for a multivariate Gaussian distribution is:\n",
    "$$\n",
    "\\mathcal{N}(\\mathbf{x}; \\mathbf{\\mu}, \\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^n|\\Sigma|}}\\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu})^T\\Sigma^{-1}(\\mathbf{x}-\\mathbf{\\mu})\\right)\n",
    "$$\n",
    "\n",
    "There are a couple of differences between the formulas for univariate and multivariate Gaussian distributions:\n",
    "1. $x$ is now a vector instead of a scalar. This means that the Gaussian is now a distribution over n-dimensional space instead of a distribution over a line.\n",
    "    * In the context of reinforcement learning, each element of $x$ corresponds to an output of the policy.\n",
    "    * In a policy controlling a robot arm, $x$ would be a vector of joint angles, one for each joint.   \n",
    "2. The mean is now a vector instead of a scalar. This means that the mean is now a point in n-dimensional space instead of a point on a line.\n",
    "3. The standard deviation is now a covariance matrix, denoted $\\Sigma$. Briefly, the covariance matrix is a matrix that describes the covariance between each pair of elements in $x$.\n",
    "    \n",
    "#### Covariance Matrix\n",
    "\n",
    "A covariance matrix is constructed as:\n",
    "$$\n",
    "\\Sigma = \\begin{bmatrix}\n",
    "\\sigma_{1}^2 & \\sigma_{1}\\sigma_{2} & \\sigma_{1}\\sigma_{3} & \\dots  & \\sigma_{1}\\sigma_{n} \\\\\n",
    "\\sigma_{2}\\sigma_{1} & \\sigma_{2}^2 & \\sigma_{2}\\sigma_{3} & \\dots  & \\sigma_{2}\\sigma_{n} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\sigma_{n}\\sigma_{1} & \\sigma_{n}\\sigma_{2} & \\sigma_{n}\\sigma_{3} & \\dots  & \\sigma_{n}^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The diagonal elements of the covariance matrix are the variances of each element of $x$. The off-diagonal elements are the covariances between each pair of elements of $x$.\n",
    "\n",
    "The covariance between two elements of $x$ is a measure of how much they change together. If the covariance is positive, then the two elements change together. If the covariance is negative, then the two elements change in opposite directions. If the covariance is zero, then the two elements are independent of each other.\n",
    "\n",
    "#### Diagonal Gaussian Policies\n",
    "\n",
    "In this notebook, we will use a diagonal Gaussian policy. A diagonal Gaussian policy is a Gaussian distribution where the covariance matrix is diagonal, and all the off-diagonal elements are zero. This means that the covariance between any two actions is zero. This means that the actions are independent of each other. \n",
    "\n",
    "This is a simplifying assumption that makes the math easier. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Now that we have the requisite background, let's implement the policy gradient algorithm for a continous action space.\n",
    "\n",
    "#### Brief Summary of Algorithm\n",
    "1. Our neural network predicts the parameters for a distribution given the observation.\n",
    "2. We sample a random variable from the distribution. This is our action.\n",
    "3. Repeat steps 1 and 2 to collect a trajectory.\n",
    "4. Repeat step 3 to collect multiple trajectories.\n",
    "5. Compute the gradient using the log probability of the actions that were sampled.\n",
    "\n",
    "There are 2 key operations that we need to implement differently from the discrete case:\n",
    "1. Sampling actions from the probability distribution output by the policy\n",
    "2. Computing the log-likelihood of a particular action.\n",
    "\n",
    "### Sampling Actions\n",
    "\n",
    "Sampling actions from a diagonal Gaussian distribution is very simple. We just sample each element of the action vector independently from a 1D Gaussian distribution. So:\n",
    "$$\n",
    "a_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
