{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadrive\n",
    "\n",
    "We now introduce the environment we will be using for this notebook: [Metadrive](https://github.com/metadriverse/metadrive)\n",
    "\n",
    "Check out the notebook [here](../quickstart.ipynb) for a quick introduction to Metadrive as well as how to install it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out the environment by creating an instance of it and taking a random action at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Known pipe types:\n",
      "  glxGraphicsPipe\n",
      "(1 aux display modules not yet loaded.)\n",
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.31221473623458e-05\n",
      "0.009233961947637252\n",
      "0.029430664503088177\n",
      "0.0385080765555831\n",
      "0.051269874340499226\n",
      "0.024825500350000494\n",
      "-0.004269349232754249\n",
      "-0.012220657157477671\n",
      "-0.011318983514367316\n",
      "0.010826522401143868\n",
      "0.019600871726443807\n",
      "0.00518795331222229\n",
      "0.0002647879602980098\n",
      "-0.00020965787401731797\n",
      "0.0027548640214803766\n",
      "0.0036451430530871957\n",
      "0.00203804888209766\n",
      "4.7831459837470954e-05\n",
      "-0.0008139261675983708\n",
      "0.0002649450169180858\n",
      "-0.004138602420487799\n",
      "-0.003833691689167148\n",
      "0.005658243306209003\n",
      "0.0019439392483036298\n",
      "0.0017501587413670697\n",
      "0.0013271993844638941\n",
      "-0.0012417981516064886\n",
      "0.00047796214394067146\n",
      "0.005576002975745207\n",
      "0.011957905409772306\n",
      "0.019367010686041318\n",
      "0.03269041932155331\n",
      "0.02264725586738955\n",
      "0.010750797150814074\n",
      "0.023785212217481203\n",
      "0.019831233304526522\n",
      "0.015112038625447117\n",
      "0.017808277012386936\n",
      "0.017960457098097456\n",
      "0.020937374686892773\n",
      "-0.0005746566562039816\n",
      "-0.001424047920491952\n",
      "0.0013622548321156132\n",
      "-0.0023082070131272346\n",
      "-0.0007034053034793181\n",
      "0.0011865168268845503\n",
      "0.0005010090409301674\n",
      "0.012206810812975241\n",
      "0.03652245909205208\n",
      "0.04765253144879414\n",
      "0.029664489278078188\n",
      "0.004300617162298999\n",
      "0.0014666318467402204\n",
      "0.012800981385227636\n",
      "0.010123072268522463\n",
      "-0.0024230380304729017\n",
      "0.00037724342136907904\n",
      "0.0013072377840583826\n",
      "-0.00028633926232021057\n",
      "0.0012443151921490746\n",
      "0.01162886324159298\n",
      "0.0022833634694858556\n",
      "4.366956398433447e-05\n",
      "-0.0012910242703932317\n",
      "0.007105826768465976\n",
      "0.024658240791679054\n",
      "0.014571685286777061\n",
      "0.0011403935210473164\n",
      "0.0008725730049453744\n",
      "0.005903569301639269\n",
      "0.019587070474912337\n",
      "0.014555904699544138\n",
      "0.010121196474990376\n",
      "0.011776427593165453\n",
      "0.0004850043527345383\n",
      "-0.0008056387412793711\n",
      "0.00026206555243062926\n",
      "-0.00020768700831920163\n",
      "0.0018697230414087109\n",
      "0.0007080512889384501\n",
      "-0.009423843900972438\n",
      "-0.00480935447272227\n",
      "0.0035404504915897118\n",
      "0.0173691706042679\n",
      "0.021871501246500943\n",
      "0.017955502764034743\n",
      "0.0009972584272051207\n",
      "-0.008338925047476915\n",
      "-0.009207792559852287\n",
      "0.005925174464493323\n",
      "0.004917707919282458\n",
      "0.0005699507039107237\n",
      "-0.0006289486554585232\n",
      "-0.004083058972635726\n",
      "3.389601245506628e-05\n",
      "0.005895378156669079\n",
      "0.01771905375347952\n",
      "0.015802050244844153\n",
      "0.004137103417070065\n",
      "0.0004935672780919453\n"
     ]
    }
   ],
   "source": [
    "# We need to import metadrive to register the environments\n",
    "import metadrive\n",
    "import gymnasium as gym \n",
    "\n",
    "env = gym.make(\"MetaDrive-validation-v0\", config={\"use_render\": True})\n",
    "env.reset()\n",
    "for i in range(100):\n",
    "    obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadrive uses the [Farama Gymnasium](https://gymnasium.farama.org/), which has a standard API for interacting with environments. There are a couple of functions and properties that are good to know about:\n",
    "1. `reset()`: Resets the environment to its initial state and returns the initial observation.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#gymnasium.Env.reset\n",
    "2. `step(action)`: Takes an action and returns the next observation, the reward for taking the action, whether the episode is terminated, whether the episode is truncated (ran out of time), and any additional information.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n",
    "3. `close()`: Closes the environment.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#g1ymnasium.Env.close\n",
    "4. `action_space`: The action space of the environment, which tells us the shape and bounds of the action space.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#gymnasium.Env.action_space\n",
    "5. `observation_space`: The observation space of the environment, which tells us the shape and bounds of the observation space.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#gymnasium.Env.observation_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at what our observation and action spaces are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Box(-1.0, 1.0, (2,), float32)\n",
      "Observation Space: Box(-0.0, 1.0, (259,), float32)\n"
     ]
    }
   ],
   "source": [
    "import metadrive\n",
    "import gymnasium as gym \n",
    "\n",
    "env = gym.make(\"MetaDrive-validation-v0\", config={\"use_render\": False})\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Action Space:\", env.action_space)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box spaces represent a continuous space. As the documentation states, a Box represents the Cartesian product of $n$ closed intervals.\n",
    "\n",
    "For our observation space, we have a 259 dimensional vector, where each element is in the range $[0.0, 1.0]$.\n",
    "* Documentation: https://metadrive-simulator.readthedocs.io/en/latest/observation.html\n",
    "\n",
    "For the action space, we have a 2 dimensional vector, where each element is in the range $[-1.0, 1.0]$. The first element represents the steering angle, and the second element represents the throttle.\n",
    "* Documentation: https://metadrive-simulator.readthedocs.io/en/latest/action_and_dynamics.html\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one problem you might see here: we previously stated that we were focusing on the discrete case of the policy gradient. Yet our action space seems to be continuous. So what gives?\n",
    "\n",
    "The answer is that we are going to discretize our action space. We will discretize the steering angle into 3 bins, and the throttle into 3 bins. This will give us a total of 9 actions. We'll write a function to convert our discrete action into a continuous action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "def discrete2continuous(action:int) -> npt.NDArray[np.float32]:\n",
    "    \"\"\"\n",
    "    Convert discrete action to continuous action\n",
    "    \"\"\"\n",
    "    assert 0 <= action < 9\n",
    "    throttle_magnitude = 1.0\n",
    "    steering_magnitude = 0.6\n",
    "    match action:\n",
    "        case 0:\n",
    "            return np.array([throttle_magnitude, steering_magnitude])\n",
    "        case 1:\n",
    "            return np.array([throttle_magnitude, 0])\n",
    "        case 2:\n",
    "            return np.array([throttle_magnitude, -steering_magnitude])\n",
    "        case 3:\n",
    "            return np.array([0, steering_magnitude])\n",
    "        case 4:\n",
    "            return np.array([0, 0])\n",
    "        case 5:\n",
    "            return np.array([0, -steering_magnitude])\n",
    "        case 6:\n",
    "            return np.array([-throttle_magnitude, steering_magnitude])\n",
    "        case 7:\n",
    "            return np.array([-throttle_magnitude, 0])\n",
    "        case 8:\n",
    "            return np.array([-throttle_magnitude, -steering_magnitude])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "GAMMA = 0.75  # Discount factor for advantage estimation and reward discounting\n",
    "ENTROPY_BONUS = 0.1\n",
    "\n",
    "def deviceof(m: nn.Module) -> torch.device:\n",
    "    return next(m.parameters()).device\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.num_actions = 9\n",
    "        self.fc1 = nn.Linear(259, 128)\n",
    "        self.fc2 = nn.Linear(128, self.num_actions)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        # output in (Batch, Width)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def compute_policy_gradient_loss(\n",
    "    # Current policy network's probability of choosing an action\n",
    "    # in (Batch, Action)\n",
    "    pi_theta_given_st: torch.Tensor,\n",
    "    # One hot encoding of which action was chosen\n",
    "    # in (Batch, Action)\n",
    "    a_t: torch.Tensor,\n",
    "    # Rewards To Go for the chosen action\n",
    "    # in (Batch,)\n",
    "    R_t: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Computes the policy gradient loss for a vector of examples, and reduces with mean.\n",
    "\n",
    "\n",
    "    https://spinningup.openai.com/en/latest/algorithms/vpg.html#key-equations\n",
    "\n",
    "    The standard policy gradient is given by the expected value over trajectories of:\n",
    "\n",
    "    :math:`\\sum_{t=0}^{T} \\nabla_{\\theta} (\\log \\pi_{\\theta}(a_t|s_t))R_t`\n",
    "    \n",
    "    where:\n",
    "    * :math:`\\pi_{\\theta}(a_t|s_t)` is the current policy's probability to perform action :math:`a_t` given :math:`s_t`\n",
    "    * :math:`R_t` is the rewards-to-go from the state at time t to the end of the episode from which it came.\n",
    "    \"\"\"\n",
    "\n",
    "    # here, the multiplication and sum is in order to extract the\n",
    "    # in (Batch,)\n",
    "    pi_theta_at_given_st = torch.sum(pi_theta_given_st * a_t, 1)\n",
    "\n",
    "    # Note: this loss has doesn't actually represent whether the action was good or bad\n",
    "    # it is a dummy loss, that is only used to compute the gradient\n",
    "\n",
    "    # Recall that the policy gradient for a single transition (state-action pair) is given by:\n",
    "    # $\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)R_t$\n",
    "    # However, it's easier to work with losses, rather than raw gradients.\n",
    "    # Therefore we construct a loss, that when differentiated, gives us the policy gradient.\n",
    "    # this loss is given by:\n",
    "    # $-\\log \\pi_{\\theta}(a_t|s_t)R_t$\n",
    "\n",
    "    # in (Batch,)\n",
    "    policy_loss_per_example = -torch.log(pi_theta_at_given_st) * R_t\n",
    "\n",
    "    # in (Batch,)\n",
    "    entropy_per_example = -torch.sum(\n",
    "        torch.log(pi_theta_given_st) * pi_theta_given_st, 1\n",
    "    )\n",
    "\n",
    "    # we reward entropy, since excessive certainty indicate the model is 'overfitting'\n",
    "    loss_per_example = policy_loss_per_example - ENTROPY_BONUS * entropy_per_example\n",
    "\n",
    "    # we take the average loss over all examples\n",
    "    return loss_per_example.mean()\n",
    "\n",
    "\n",
    "def train_policygradient(\n",
    "    actor: Actor,\n",
    "    actor_optimizer: torch.optim.Optimizer,\n",
    "    observation_batch: list[env.Observation],\n",
    "    action_batch: list[env.Action],\n",
    "    value_batch: list[env.Value],\n",
    ") -> float:\n",
    "    # assert that the batch_lengths are the same\n",
    "    assert len(observation_batch) == len(action_batch)\n",
    "    assert len(observation_batch) == len(value_batch)\n",
    "\n",
    "    # get device\n",
    "    device = deviceof(actor)\n",
    "\n",
    "    # convert data to tensors on correct device\n",
    "\n",
    "    # in (Batch, Width, Height)\n",
    "    observation_batch_tensor = obs_batch_to_tensor(observation_batch, device)\n",
    "\n",
    "    # in (Batch,)\n",
    "    value_batch_tensor = torch.tensor(\n",
    "        value_batch, dtype=torch.float32, device=device\n",
    "    )\n",
    "\n",
    "    # in (Batch, Action)\n",
    "    chosen_action_tensor = F.one_hot(\n",
    "        torch.tensor(action_batch).to(device).long(), num_classes=actor.num_actions\n",
    "    )\n",
    "\n",
    "    # train actor\n",
    "    actor_optimizer.zero_grad()\n",
    "    action_probs = actor.forward(observation_batch_tensor)\n",
    "    actor_loss = compute_policy_gradient_loss(\n",
    "        action_probs, chosen_action_tensor, value_batch_tensor\n",
    "    )\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "\n",
    "    # return the respective losses\n",
    "    return actor_loss.item()\n",
    "\n",
    "def compute_rtg(\n",
    "    trajectory_rewards: npt.NDArray[np.float32],\n",
    ") -> npt.NDArray[np.float32]:\n",
    "    \"\"\"\n",
    "    Computes the gamma discounted reward-to-go for each state in the trajectory.\n",
    "    \"\"\"\n",
    "\n",
    "    trajectory_len = len(trajectory_rewards)\n",
    "\n",
    "    v_batch = np.zeros(trajectory_len)\n",
    "\n",
    "    v_batch[-1] = trajectory_rewards[-1]\n",
    "\n",
    "    # Use GAMMA to decay the advantage\n",
    "    for t in reversed(range(trajectory_len - 1)):\n",
    "        v_batch[t] = trajectory_rewards[t] + GAMMA * v_batch[t + 1]\n",
    "\n",
    "    return v_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "EPISODES_PER_BATCH = 200\n",
    "TRAIN_EPOCHS = 500000\n",
    "MODEL_SAVE_INTERVAL = 100\n",
    "SUMMARY_STATS_INTERVAL = 10\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "SUMMARY_DIR = './summary'\n",
    "MODEL_DIR = './models'\n",
    "\n",
    "# create result directories\n",
    "if not os.path.exists(SUMMARY_DIR):\n",
    "    os.makedirs(SUMMARY_DIR)\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "cuda = torch.device(\"cuda\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "if use_cuda:\n",
    "    device = cuda\n",
    "else:\n",
    "    device = cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Network\n",
    "ACTOR_LR = 1e-4  # Lower lr stabilises training greatly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "for _ in range(TRAIN_EPOCHS):\n",
    "    s_batch:list[npt.NDArray[np.float32]] = []\n",
    "    a_batch:list[np.int8] = []\n",
    "    rtg_batch:list[np.float32] = []\n",
    "    \n",
    "    for _ in range(EPISODES_PER_BATCH):\n",
    "        # play the game\n",
    "        ep_s, ep_a, ep_r, ep_rtg = play(actor_player,opponent_player, go_first)\n",
    "\n",
    "        # now update the minibatch\n",
    "        s_batch += ep_s\n",
    "        a_batch += ep_a\n",
    "        rtg_batch += ep_rtg\n",
    "\n",
    "    actor_losses = train_policygradient(\n",
    "        actor,\n",
    "        actor_optimizer,\n",
    "        s_batch,\n",
    "        a_batch,\n",
    "        v_batch\n",
    "    )\n",
    "\n",
    "    for actor_loss in actor_losses:\n",
    "        writer.add_scalar('actor_loss', actor_loss, step)\n",
    "\n",
    "        for opponent_name, rewards in rewards_vs.items():\n",
    "            if len(rewards) > 400:\n",
    "                avg_reward = np.array(rewards).mean()\n",
    "                writer.add_scalar(f'reward_against_{opponent_name}', avg_reward, step)\n",
    "                rewards_vs[opponent_name] = []\n",
    "\n",
    "        if step % MODEL_SAVE_INTERVAL == 0:\n",
    "            # Save the neural net parameters to disk.\n",
    "            torch.save(actor.state_dict(), f\"{MODEL_DIR}/nn_model_ep_{step}_actor.ckpt\")\n",
    "        \n",
    "        step += 1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
