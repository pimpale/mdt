{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadrive\n",
    "\n",
    "We now introduce the environment we will be using for this notebook: [Metadrive](https://github.com/metadriverse/metadrive)\n",
    "\n",
    "Check out the notebook [here](../quickstart.ipynb) for a quick introduction to Metadrive as well as how to install it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started by doing a bunch of imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully registered the following environments: ['MetaDrive-validation-v0', 'MetaDrive-10env-v0', 'MetaDrive-100envs-v0', 'MetaDrive-1000envs-v0', 'SafeMetaDrive-validation-v0', 'SafeMetaDrive-10env-v0', 'SafeMetaDrive-100envs-v0', 'SafeMetaDrive-1000envs-v0', 'MARLTollgate-v0', 'MARLBottleneck-v0', 'MARLRoundabout-v0', 'MARLIntersection-v0', 'MARLParkingLot-v0', 'MARLMetaDrive-v0'].\n"
     ]
    }
   ],
   "source": [
    "# We need to import metadrive to register the environments\n",
    "import metadrive\n",
    "import gymnasium as gym\n",
    "import typing\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out the environment by creating an instance of it and taking a random action at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fidgetsinner/venvs/metadrive/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:210: DeprecationWarning: WARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\n",
      "  logger.deprecation(\n",
      "Known pipe types:\n",
      "  glxGraphicsPipe\n",
      "(1 aux display modules not yet loaded.)\n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n"
     ]
    }
   ],
   "source": [
    "# horizon represents the number of steps in an episode before truncation\n",
    "env = gym.make(\"MetaDrive-validation-v0\", config={\"use_render\": True, \"horizon\": 100})\n",
    "env.reset()\n",
    "while True:\n",
    "    obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadrive uses the [Farama Gymnasium](https://gymnasium.farama.org/), which has a standard API for interacting with environments. There are a couple of functions and properties that are good to know about:\n",
    "1. `reset()`: Resets the environment to its initial state and returns the initial observation.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#gymnasium.Env.reset\n",
    "2. `step(action)`: Takes an action and returns the next observation, the reward for taking the action, whether the episode is terminated, whether the episode is truncated (ran out of time), and any additional information.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n",
    "3. `close()`: Closes the environment.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#g1ymnasium.Env.close\n",
    "4. `action_space`: The action space of the environment, which tells us the shape and bounds of the action space.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#gymnasium.Env.action_space\n",
    "5. `observation_space`: The observation space of the environment, which tells us the shape and bounds of the observation space.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#gymnasium.Env.observation_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at what our observation and action spaces are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: Box(-0.0, 1.0, (259,), float32)\n",
      "Action Space: Box(-1.0, 1.0, (2,), float32)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MetaDrive-validation-v0\", config={\"use_render\": False, \"horizon\": 100})\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Action Space:\", env.action_space)\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box spaces represent a continuous space. As the documentation states, a Box represents the Cartesian product of $n$ closed intervals.\n",
    "\n",
    "For our observation space, we have a 259 dimensional vector, where each element is in the range $[0.0, 1.0]$.\n",
    "* Documentation: https://metadrive-simulator.readthedocs.io/en/latest/observation.html\n",
    "\n",
    "For the action space, we have a 2 dimensional vector, where each element is in the range $[-1.0, 1.0]$. The first element represents the steering angle, and the second element represents the throttle.\n",
    "* Documentation: https://metadrive-simulator.readthedocs.io/en/latest/action_and_dynamics.html\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one problem you might see here: we previously stated that we were focusing on the discrete case of the policy gradient. Yet our action space seems to be continuous. So what gives?\n",
    "\n",
    "The answer is that we are going to discretize our action space. We will discretize the steering angle into 2 bins, and the throttle into 2 bins. This will give us a total of 4 actions. We'll write a function to convert our discrete action into a continuous action, which we can provide to the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = 4\n",
    "\n",
    "def discrete2continuous(action:int) -> npt.NDArray[np.float32]:\n",
    "    \"\"\"\n",
    "    Convert discrete action to continuous action\n",
    "    \"\"\"\n",
    "    assert 0 <= action < 4\n",
    "    throttle_magnitude = 1.0\n",
    "    steering_magnitude = 0.6\n",
    "    match action:\n",
    "        case 0:\n",
    "            return np.array([throttle_magnitude, 0.0])\n",
    "        case 1:\n",
    "            return np.array([0.0, steering_magnitude])\n",
    "        case 2:\n",
    "            return np.array([0.0, -steering_magnitude])\n",
    "        case 3:\n",
    "            return np.array([-throttle_magnitude, 0.0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can write a function to collect a trajectory given a policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectory(env:gym.Env, policy:typing.Callable[[npt.NDArray], int]) -> tuple[list[npt.NDArray], list[int], list[float]]:\n",
    "    \"\"\"\n",
    "    Collect a trajectory from the environment using the given policy\n",
    "    \"\"\"\n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        observations.append(obs)\n",
    "        action = policy(obs)\n",
    "        actions.append(action)\n",
    "        obs, reward, terminated, truncated, info = env.step(discrete2continuous(action))\n",
    "        rewards.append(reward)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return observations, actions, rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: [array([0.09722222, 0.4861111 , 0.5       , 0.01234568, 0.5       ,\n",
      "       0.5       , 0.5       , 0.        , 0.5       , 0.55      ,\n",
      "       0.465     , 0.        , 0.5       , 0.5       , 0.95      ,\n",
      "       0.46500003, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.09722222, 0.4861111 , 0.5       , 0.01329535, 0.5       ,\n",
      "       0.5       , 0.2       , 0.        , 0.5       , 0.55      ,\n",
      "       0.465     , 0.        , 0.5       , 0.5       , 0.95      ,\n",
      "       0.46500003, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([9.7222224e-02, 4.8611110e-01, 4.9999994e-01, 2.0659843e-02,\n",
      "       5.0000000e-01, 5.0000000e-01, 8.0000001e-01, 1.3328004e-06,\n",
      "       5.0000000e-01, 5.4994303e-01, 4.6500000e-01, 0.0000000e+00,\n",
      "       5.0000000e-01, 5.0000000e-01, 9.4994217e-01, 4.6499991e-01,\n",
      "       0.0000000e+00, 5.0000000e-01, 5.0000000e-01, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00], dtype=float32), array([0.09722222, 0.4861111 , 0.49999994, 0.02802433, 0.5       ,\n",
      "       0.5       , 0.8       , 0.        , 0.5       , 0.54972005,\n",
      "       0.465     , 0.        , 0.5       , 0.5       , 0.949697  ,\n",
      "       0.4649999 , 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([9.7222224e-02, 4.8611110e-01, 5.0000000e-01, 1.5802270e-02,\n",
      "       5.0000000e-01, 5.0000000e-01, 2.0000000e-01, 1.3328004e-06,\n",
      "       5.0000000e-01, 5.4953891e-01, 4.6500000e-01, 0.0000000e+00,\n",
      "       5.0000000e-01, 5.0000000e-01, 9.4949538e-01, 4.6500003e-01,\n",
      "       0.0000000e+00, 5.0000000e-01, 5.0000000e-01, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00], dtype=float32), array([0.09717692, 0.4861564 , 0.49980658, 0.01349327, 0.5083333 ,\n",
      "       1.        , 0.5       , 0.00386824, 0.4998188 , 0.54950225,\n",
      "       0.4649728 , 0.        , 0.5       , 0.5       , 0.9494776 ,\n",
      "       0.4648188 , 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.09711141, 0.4862219 , 0.49965373, 0.01584091, 0.5       ,\n",
      "       0.5       , 0.2       , 0.00305731, 0.49955675, 0.5494914 ,\n",
      "       0.4649457 , 0.        , 0.5       , 0.5       , 0.94947773,\n",
      "       0.4646681 , 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([9.7100817e-02, 4.8623252e-01, 4.9966151e-01, 1.6216910e-02,\n",
      "       5.0000000e-01, 5.0000000e-01, 8.0000001e-01, 1.5579490e-04,\n",
      "       4.9951437e-01, 5.4949230e-01, 4.6494457e-01, 0.0000000e+00,\n",
      "       5.0000000e-01, 5.0000000e-01, 9.4948298e-01, 4.6467328e-01,\n",
      "       0.0000000e+00, 5.0000000e-01, 5.0000000e-01, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00], dtype=float32), array([0.09713803, 0.4861953 , 0.4998131 , 0.01394398, 0.49166667,\n",
      "       0.        , 0.5       , 0.00303201, 0.49966323, 0.5494386 ,\n",
      "       0.4649663 , 0.        , 0.5       , 0.5       , 0.94943225,\n",
      "       0.46481636, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.0971907 , 0.48614264, 0.5000533 , 0.01259826, 0.49166667,\n",
      "       0.        , 0.5       , 0.00480301, 0.4998739 , 0.54942757,\n",
      "       0.4649997 , 0.        , 0.5       , 0.5       , 0.9494172 ,\n",
      "       0.46504325, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32)]\n",
      "Actions: [2, 1, 1, 2, 0, 2, 1, 3, 3, 2]\n",
      "Rewards: [9.615377336740495e-05, 0.006527615427970997, 0.02360780376195911, 0.018214683927595683, 0.0026634579184152047, 0.0005007583368208425, 0.00041198926326599643, 0.006632507865392497, 0.0027597579427950684, 0.0006700951160284]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MetaDrive-validation-v0\", config={\"use_render\": False, \"horizon\": 100})\n",
    "# horizon is the max number of steps in a trajectory\n",
    "def random_policy(obs:npt.NDArray) -> int:\n",
    "    \"\"\"\n",
    "    A random policy that returns a random action\n",
    "    \"\"\"\n",
    "    return np.random.randint(0, NUM_ACTIONS)\n",
    "\n",
    "obs, actions, rewards = collect_trajectory(env, random_policy)\n",
    "# print the first 10 observations, actions, and rewards\n",
    "print(\"Observations:\", obs[:10])\n",
    "print(\"Actions:\", actions[:10])\n",
    "print(\"Rewards:\", rewards[:10])\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `collect_trajectory` function allows us to gather rewards from the trajectory. However, recall that what we'll actually train the network on is the reward-to-go. Let's now create the function./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "\n",
    "def rewards_to_go(trajectory_rewards: list[float]) -> list[float]:\n",
    "    \"\"\"\n",
    "    Computes the gamma discounted reward-to-go for each state in the trajectory.\n",
    "    \"\"\"\n",
    "\n",
    "    trajectory_len = len(trajectory_rewards)\n",
    "\n",
    "    v_batch = np.zeros(trajectory_len)\n",
    "\n",
    "    v_batch[-1] = trajectory_rewards[-1]\n",
    "\n",
    "    # Use GAMMA to decay the advantage\n",
    "    for t in reversed(range(trajectory_len - 1)):\n",
    "        v_batch[t] = trajectory_rewards[t] + GAMMA * v_batch[t + 1]\n",
    "\n",
    "    return list(v_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now that we have a function that lets us go from policies to trajectories, we should work on creating a neural network based policy. The network should take in an observation, and return a probability for each number between 0 and 9.\n",
    "\n",
    "We're going to keep the network fairly small for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy network\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(259, 128)\n",
    "        self.fc2 = nn.Linear(128, NUM_ACTIONS)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        # output in (Batch, Width)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a policy function that allows us to sample an action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deviceof(m: nn.Module) -> torch.device:\n",
    "    \"\"\"\n",
    "    Get the device of the given module\n",
    "    \"\"\"\n",
    "    return next(m.parameters()).device\n",
    "\n",
    "def nn_policy(net:Policy, obs:npt.NDArray) -> int:\n",
    "    \"\"\"\n",
    "    A neural network policy that returns an action based on the given observation\n",
    "    \"\"\"\n",
    "    # convert observation to a tensor\n",
    "    obs_tensor = torch.from_numpy(obs).float().to(deviceof(net))\n",
    "    # add batch dimension\n",
    "    obs_tensor = obs_tensor.unsqueeze(0)\n",
    "    # get the action probabilities\n",
    "    action_probs = net(obs_tensor)\n",
    "    # sample an action from the action probabilities\n",
    "    action = torch.multinomial(action_probs, 1)\n",
    "    return action.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out (untrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: [array([0.09722222, 0.4861111 , 0.5       , 0.01234568, 0.5       ,\n",
      "       0.5       , 0.5       , 0.        , 0.5       , 0.55      ,\n",
      "       0.465     , 0.        , 0.5       , 0.5       , 0.95      ,\n",
      "       0.46500003, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.09722222, 0.4861111 , 0.5       , 0.01266881, 0.49166667,\n",
      "       0.        , 0.5       , 0.        , 0.5       , 0.55      ,\n",
      "       0.465     , 0.        , 0.5       , 0.5       , 0.95      ,\n",
      "       0.46500003, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([9.7225294e-02, 4.8610803e-01, 5.0000799e-01, 1.8599609e-02,\n",
      "       5.0000000e-01, 5.0000000e-01, 2.0000000e-01, 1.5978965e-04,\n",
      "       5.0001228e-01, 5.4997110e-01, 4.6500134e-01, 0.0000000e+00,\n",
      "       5.0000000e-01, 5.0000000e-01, 9.4996953e-01, 4.6500763e-01,\n",
      "       0.0000000e+00, 5.0000000e-01, 5.0000000e-01, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00], dtype=float32), array([9.7227149e-02, 4.8610619e-01, 5.0000757e-01, 2.5964092e-02,\n",
      "       5.0000000e-01, 5.0000000e-01, 8.0000001e-01, 7.9898573e-06,\n",
      "       5.0001973e-01, 5.4982626e-01, 4.6500164e-01, 0.0000000e+00,\n",
      "       5.0000000e-01, 5.0000000e-01, 9.4979733e-01, 4.6500763e-01,\n",
      "       0.0000000e+00, 5.0000000e-01, 5.0000000e-01, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00], dtype=float32), array([9.7228184e-02, 4.8610514e-01, 5.0000787e-01, 1.5340286e-02,\n",
      "       5.0000000e-01, 5.0000000e-01, 2.0000000e-01, 5.3270351e-06,\n",
      "       5.0002384e-01, 5.4968601e-01, 4.6500185e-01, 0.0000000e+00,\n",
      "       5.0000000e-01, 5.0000000e-01, 9.4964081e-01, 4.6500814e-01,\n",
      "       0.0000000e+00, 5.0000000e-01, 5.0000000e-01, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00], dtype=float32), array([0.09718784, 0.4861455 , 0.49983516, 0.01315523, 0.5083333 ,\n",
      "       1.        , 0.5       , 0.00345412, 0.49986246, 0.5496618 ,\n",
      "       0.4649775 , 0.        , 0.5       , 0.5       , 0.9496387 ,\n",
      "       0.4648462 , 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.0971325 , 0.48620084, 0.49955446, 0.0202508 , 0.5       ,\n",
      "       0.5       , 0.8       , 0.00561394, 0.4996411 , 0.5495904 ,\n",
      "       0.4649396 , 0.        , 0.5       , 0.5       , 0.949581  ,\n",
      "       0.46458265, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.09703369, 0.48629963, 0.49907988, 0.01718852, 0.5083333 ,\n",
      "       1.        , 0.5       , 0.00949151, 0.49924585, 0.5494098 ,\n",
      "       0.46487516, 0.        , 0.5       , 0.5       , 0.9494043 ,\n",
      "       0.46413973, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.09683365, 0.48649967, 0.49805117, 0.01548687, 0.5083333 ,\n",
      "       1.        , 0.5       , 0.02057425, 0.49844572, 0.54924226,\n",
      "       0.46473786, 0.        , 0.5       , 0.5       , 0.9492296 ,\n",
      "       0.46317884, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.09671749, 0.48661584, 0.49739578, 0.01280163, 0.49166667,\n",
      "       0.        , 0.5       , 0.01310808, 0.49798107, 0.5491461 ,\n",
      "       0.46465245, 0.        , 0.5       , 0.5       , 0.94912475,\n",
      "       0.46256697, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32)]\n",
      "Actions: [3, 2, 1, 2, 0, 1, 0, 0, 3, 3]\n",
      "Rewards: [3.271655938232812e-05, 0.0035562221479854984, 0.015523285038177707, 0.014128620584381454, 0.0015692217651156024, 0.006145738151190885, 0.015268475179452198, 0.009759420969067905, 0.004953296131791456, 0.0005263119795825993]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MetaDrive-validation-v0\", config={\"use_render\": False, \"horizon\": 100})\n",
    "\n",
    "policy = Policy()\n",
    "obs, actions, rewards = collect_trajectory(env, lambda obs: nn_policy(policy, obs))\n",
    "# print the first 10 observations, actions, and rewards\n",
    "print(\"Observations:\", obs[:10])\n",
    "print(\"Actions:\", actions[:10])\n",
    "print(\"Rewards:\", rewards[:10])\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it performs basically random actions, since it hasn't been trained yet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now work on the meat of the problem: the Policy Gradient algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTROPY_BONUS = 0.1\n",
    "\n",
    "def compute_policy_gradient_loss(\n",
    "    # Current policy network's probability of choosing an action\n",
    "    # in (Batch, Action)\n",
    "    pi_theta_given_st: torch.Tensor,\n",
    "    # One hot encoding of which action was chosen\n",
    "    # in (Batch, Action)\n",
    "    a_t: torch.Tensor,\n",
    "    # Rewards To Go for the chosen action\n",
    "    # in (Batch,)\n",
    "    R_t: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Computes the policy gradient loss for a vector of examples, and reduces with mean.\n",
    "\n",
    "    The standard policy gradient is given by the expected value over trajectories of:\n",
    "\n",
    "    :math:`\\sum_{t=0}^{T} \\nabla_{\\theta} (\\log \\pi_{\\theta}(a_t|s_t))R_t`\n",
    "    \n",
    "    where:\n",
    "    * :math:`\\pi_{\\theta}(a_t|s_t)` is the current policy's probability to perform action :math:`a_t` given :math:`s_t`\n",
    "    * :math:`R_t` is the rewards-to-go from the state at time t to the end of the episode from which it came.\n",
    "    \"\"\"\n",
    "\n",
    "    # here, the multiplication and sum is in order to extract the\n",
    "    # in (Batch,)\n",
    "    pi_theta_at_given_st = torch.sum(pi_theta_given_st * a_t, 1)\n",
    "\n",
    "    # Note: this loss has doesn't actually represent whether the action was good or bad\n",
    "    # it is a dummy loss, that is only used to compute the gradient\n",
    "\n",
    "    # Recall that the policy gradient for a single transition (state-action pair) is given by:\n",
    "    # $\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)R_t$\n",
    "    # However, it's easier to work with losses, rather than raw gradients.\n",
    "    # Therefore we construct a loss, that when differentiated, gives us the policy gradient.\n",
    "    # this loss is given by:\n",
    "    # $-\\log \\pi_{\\theta}(a_t|s_t)R_t$\n",
    "\n",
    "    # in (Batch,)\n",
    "    policy_loss_per_example = -torch.log(pi_theta_at_given_st) * R_t\n",
    "\n",
    "    # in (Batch,)\n",
    "    entropy_per_example = -torch.sum(\n",
    "        torch.log(pi_theta_given_st) * pi_theta_given_st, 1\n",
    "    )\n",
    "\n",
    "    # we reward entropy, since excessive certainty indicate the model is 'overfitting'\n",
    "    loss_per_example = policy_loss_per_example - ENTROPY_BONUS * entropy_per_example\n",
    "\n",
    "    # we take the average loss over all examples\n",
    "    return loss_per_example.mean()\n",
    "\n",
    "\n",
    "def train_policygradient(\n",
    "    policy: Policy,\n",
    "    actor_optimizer: torch.optim.Optimizer,\n",
    "    observation_batch: list[npt.NDArray],\n",
    "    action_batch: list[int],\n",
    "    rtg_batch: list[float],\n",
    ") -> float:\n",
    "    # assert that the batch_lengths are the same\n",
    "    assert len(observation_batch) == len(action_batch)\n",
    "    assert len(observation_batch) == len(rtg_batch)\n",
    "\n",
    "    # get device\n",
    "    device = deviceof(policy)\n",
    "\n",
    "    # convert data to tensors on correct device\n",
    "\n",
    "    # in (Batch, Width)\n",
    "    observation_batch_tensor = torch.from_numpy(np.stack(observation_batch)).to(device)\n",
    "\n",
    "    # in (Batch,)\n",
    "    rtg_batch_tensor = torch.tensor(\n",
    "        rtg_batch, dtype=torch.float32, device=device\n",
    "    )\n",
    "\n",
    "    # in (Batch, Action)\n",
    "    chosen_action_tensor = F.one_hot(\n",
    "        torch.tensor(action_batch).to(device).long(), num_classes=NUM_ACTIONS\n",
    "    )\n",
    "\n",
    "    # train actor\n",
    "    actor_optimizer.zero_grad()\n",
    "    action_probs = policy.forward(observation_batch_tensor)\n",
    "    actor_loss = compute_policy_gradient_loss(\n",
    "        action_probs, chosen_action_tensor, rtg_batch_tensor\n",
    "    )\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "\n",
    "    # return the respective losses\n",
    "    return actor_loss.item()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we're done. Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "policy = Policy().to(device)\n",
    "actor_optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4)\n",
    "\n",
    "step = 0\n",
    "rewards = []\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MetaDrive-validation-v0\", config={\"use_render\": False, \"horizon\": 500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Policy Loss: -0.091, Avg. Returns: 1.303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Policy Loss: -0.087, Avg. Returns: 1.663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2, Policy Loss: -0.075, Avg. Returns: 2.203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3, Policy Loss: -0.047, Avg. Returns: 3.314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4, Policy Loss: -0.017, Avg. Returns: 4.501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5, Policy Loss: 0.036, Avg. Returns: 6.589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6, Policy Loss: 0.094, Avg. Returns: 9.013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7, Policy Loss: 0.196, Avg. Returns: 13.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: out_of_road.\n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: out_of_road.\n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8, Policy Loss: 0.230, Avg. Returns: 14.223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: out_of_road.\n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: out_of_road.\n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: out_of_road.\n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9, Policy Loss: 0.309, Avg. Returns: 17.272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: out_of_road.\n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: out_of_road.\n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: out_of_road.\n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: out_of_road.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10, Policy Loss: 0.315, Avg. Returns: 16.580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: max step \n"
     ]
    }
   ],
   "source": [
    "TRAIN_EPOCHS = 500\n",
    "EPISODES_PER_BATCH = 10\n",
    "\n",
    "# Train\n",
    "while step < TRAIN_EPOCHS:\n",
    "    obs_batch:list[npt.NDArray[np.float32]] = []\n",
    "    act_batch:list[int] = []\n",
    "    rtg_batch:list[float] = []\n",
    "    \n",
    "    trajectory_returns = []\n",
    "\n",
    "    for _ in range(EPISODES_PER_BATCH):\n",
    "        # Collect trajectory\n",
    "        obs_traj, act_traj, rew_traj = collect_trajectory(env, lambda obs: nn_policy(policy, obs))\n",
    "        rtg_traj = rewards_to_go(rew_traj)\n",
    "\n",
    "        # Update batch\n",
    "        obs_batch.extend(obs_traj)\n",
    "        act_batch.extend(act_traj)\n",
    "        rtg_batch.extend(rtg_traj)\n",
    "\n",
    "        # Update trajectory returns\n",
    "        trajectory_returns.append(sum(rew_traj))\n",
    "\n",
    "    policy_loss = train_policygradient(\n",
    "        policy,\n",
    "        actor_optimizer,\n",
    "        obs_batch,\n",
    "        act_batch,\n",
    "        rtg_batch\n",
    "    )\n",
    "\n",
    "    print(f\"Step {step}, Policy Loss: {policy_loss:.3f}, Avg. Returns: {np.mean(trajectory_returns):.3f}\")\n",
    "    rewards.append(np.mean(trajectory_returns))\n",
    "    losses.append(policy_loss)\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how the policy drives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MetaDrive-validation-v0\", config={\"use_render\": True, \"horizon\": 500})\n",
    "obs, act, rew = collect_trajectory(env, lambda obs: nn_policy(policy, obs))\n",
    "env.close()\n",
    "\n",
    "print(\"Reward:\", sum(rew))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For us, we got returns of around 12-14 after training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metadrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
