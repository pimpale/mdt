{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadrive Intro\n",
    "\n",
    "We now introduce the environment we will be using for this notebook: [Metadrive](https://github.com/metadriverse/metadrive)\n",
    "\n",
    "Check out the notebook [here](../quickstart.ipynb) for a quick introduction to Metadrive as well as how to install it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started by doing a bunch of imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully registered the following environments: ['MetaDrive-validation-v0', 'MetaDrive-10env-v0', 'MetaDrive-100envs-v0', 'MetaDrive-1000envs-v0', 'SafeMetaDrive-validation-v0', 'SafeMetaDrive-10env-v0', 'SafeMetaDrive-100envs-v0', 'SafeMetaDrive-1000envs-v0', 'MARLTollgate-v0', 'MARLBottleneck-v0', 'MARLRoundabout-v0', 'MARLIntersection-v0', 'MARLParkingLot-v0', 'MARLMetaDrive-v0'].\n"
     ]
    }
   ],
   "source": [
    "# We need to import metadrive to register the environments\n",
    "import metadrive\n",
    "import gymnasium as gym\n",
    "import typing\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out the environment by creating an instance of it and taking a random action at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fidgetsinner/venvs/metadrive/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:210: DeprecationWarning: WARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\n",
      "  logger.deprecation(\n",
      "Known pipe types:\n",
      "  glxGraphicsPipe\n",
      "(1 aux display modules not yet loaded.)\n"
     ]
    }
   ],
   "source": [
    "# horizon represents the number of steps in an episode before truncation\n",
    "env = gym.make(\"MetaDrive-validation-v0\", config={\"use_render\": True, \"horizon\": 100})\n",
    "env.reset()\n",
    "while True:\n",
    "    obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadrive uses the [Farama Gymnasium](https://gymnasium.farama.org/), which has a standard API for interacting with environments. There are a couple of functions and properties that are good to know about:\n",
    "1. `reset()`: Resets the environment to its initial state and returns the initial observation.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#gymnasium.Env.reset\n",
    "2. `step(action)`: Takes an action and returns the next observation, the reward for taking the action, whether the episode is terminated, whether the episode is truncated (ran out of time), and any additional information.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n",
    "3. `close()`: Closes the environment.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#g1ymnasium.Env.close\n",
    "4. `action_space`: The action space of the environment, which tells us the shape and bounds of the action space.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#gymnasium.Env.action_space\n",
    "5. `observation_space`: The observation space of the environment, which tells us the shape and bounds of the observation space.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#gymnasium.Env.observation_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at what our observation and action spaces are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: Box(-0.0, 1.0, (259,), float32)\n",
      "Action Space: Box(-1.0, 1.0, (2,), float32)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MetaDrive-validation-v0\", config={\"use_render\": False, \"horizon\": 100})\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Action Space:\", env.action_space)\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box spaces represent a continuous space. As the documentation states, a Box represents the Cartesian product of $n$ closed intervals.\n",
    "\n",
    "For our observation space, we have a 259 dimensional vector, where each element is in the range $[0.0, 1.0]$.\n",
    "* Documentation: https://metadrive-simulator.readthedocs.io/en/latest/observation.html\n",
    "\n",
    "For the action space, we have a 2 dimensional vector, where each element is in the range $[-1.0, 1.0]$. The first element represents the steering angle, and the second element represents the throttle.\n",
    "* Documentation: https://metadrive-simulator.readthedocs.io/en/latest/action_and_dynamics.html\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "Now that we've seen what the environment looks like, let's implement REINFORCE.\n",
    "\n",
    "The REINFORCE algorithm is as follows:\n",
    "1. Initialize the policy parameters $\\theta$.\n",
    "2. Create an initially empty set of trajectories $D$.\n",
    "3. Create a trajectory $\\tau = (s_1, a_1, r_1, \\ldots, s_T, a_T, r_T)$ by running the policy $\\pi_\\theta$ in the environment and append it to $D$.\n",
    "4. Compute the return $\\hat{R}_t$ for each timestep $t$ in the trajectory.\n",
    "5. Repeat steps 2-4 until $|D| = N$.\n",
    "6. Compute the policy gradient estimate $\\hat{g} = \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^{T_i} \\nabla_\\theta \\log \\pi_\\theta(a_{i,t} | s_{i,t}) \\hat{R}_t$.\n",
    "7. Update the policy parameters $\\theta \\leftarrow \\theta + \\alpha \\hat{g}$.\n",
    "8. Repeat steps 2-7 until the policy converges.\n",
    "\n",
    "All things considered, this is a pretty simple algorithm. Let's implement it!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Trajectories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one problem you might see here: we previously stated that we were focusing on the discrete case. This means our policy will produce discrete actions. However, our action space seems to be continuous.\n",
    "\n",
    "How can we solve this problem?\n",
    "\n",
    "Our solution is going to be to **discretize** our action space. This means that we will provide a finite number of actions, and each action corresponds to a fixed vector of real numbers.\n",
    "\n",
    "We will discretize the steering angle into 2 bins, and the throttle into 2 bins. This will give us a total of 4 actions. We'll write a function to convert our discrete action into a continuous action, which we can provide to the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = 4\n",
    "\n",
    "def discrete2continuous(action:int) -> npt.NDArray[np.float32]:\n",
    "    \"\"\"\n",
    "    Convert discrete action to continuous action\n",
    "    \"\"\"\n",
    "    assert 0 <= action < 4\n",
    "    throttle_magnitude = 1.0\n",
    "    steering_magnitude = 0.6\n",
    "    match action:\n",
    "        case 0:\n",
    "            return np.array([steering_magnitude, 0.0])\n",
    "        case 1:\n",
    "            return np.array([0.0, throttle_magnitude])\n",
    "        case 2:\n",
    "            return np.array([0.0, -throttle_magnitude])\n",
    "        case 3:\n",
    "            return np.array([-steering_magnitude, 0.0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this discretization function, we can write a function to collect a trajectory given a policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectory(env:gym.Env, policy:typing.Callable[[npt.NDArray], int]) -> tuple[list[npt.NDArray], list[int], list[float]]:\n",
    "    \"\"\"\n",
    "    Collect a trajectory from the environment using the given policy\n",
    "    \"\"\"\n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        observations.append(obs)\n",
    "        action = policy(obs)\n",
    "        actions.append(action)\n",
    "        obs, reward, terminated, truncated, info = env.step(discrete2continuous(action))\n",
    "        rewards.append(reward)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return observations, actions, rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we define a policy as any function that takes in an observation and returns an action. This means that we can be more general than just using a neural net as a policy.\n",
    "For example, let's first test out our `collect_trajectory` function with a random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: [array([0.09722222, 0.4861111 , 0.5       , 0.01234568, 0.5       ,\n",
      "       0.5       , 0.5       , 0.        , 0.5       , 0.55      ,\n",
      "       0.465     , 0.        , 0.5       , 0.5       , 0.95      ,\n",
      "       0.46500003, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.09722222, 0.4861111 , 0.5       , 0.01266841, 0.495     ,\n",
      "       0.2       , 0.5       , 0.        , 0.5       , 0.55      ,\n",
      "       0.465     , 0.        , 0.5       , 0.5       , 0.95      ,\n",
      "       0.46500003, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([9.7221747e-02, 4.8611158e-01, 4.9999806e-01, 1.4333932e-02,\n",
      "       5.0500000e-01, 8.0000001e-01, 5.0000000e-01, 3.8615864e-05,\n",
      "       4.9999809e-01, 5.4998505e-01, 4.6499974e-01, 0.0000000e+00,\n",
      "       5.0000000e-01, 5.0000000e-01, 9.4998294e-01, 4.6499825e-01,\n",
      "       0.0000000e+00, 5.0000000e-01, 5.0000000e-01, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00], dtype=float32), array([9.7222358e-02, 4.8611099e-01, 4.9997997e-01, 2.6608096e-02,\n",
      "       5.0000000e-01, 5.0000000e-01, 1.0000000e+00, 3.6218989e-04,\n",
      "       5.0000054e-01, 5.4987037e-01, 4.6499816e-01, 0.0000000e+00,\n",
      "       5.0000000e-01, 5.0000000e-01, 9.4983679e-01, 4.6498317e-01,\n",
      "       0.0000000e+00, 5.0000000e-01, 5.0000000e-01, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00], dtype=float32), array([9.7220525e-02, 4.8611280e-01, 4.9997789e-01, 1.9352190e-02,\n",
      "       5.0000000e-01, 5.0000000e-01, 0.0000000e+00, 4.1278636e-05,\n",
      "       4.9999321e-01, 5.4975384e-01, 4.6499756e-01, 0.0000000e+00,\n",
      "       5.0000000e-01, 5.0000000e-01, 9.4970411e-01, 4.6498051e-01,\n",
      "       0.0000000e+00, 5.0000000e-01, 5.0000000e-01, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
      "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00], dtype=float32), array([0.09726307, 0.48607028, 0.50019497, 0.01732474, 0.505     ,\n",
      "       0.8       , 0.5       , 0.00434095, 0.5001634 , 0.54987454,\n",
      "       0.4650267 , 0.        , 0.5       , 0.5       , 0.94984174,\n",
      "       0.46518183, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.09733468, 0.48599866, 0.50060314, 0.0197083 , 0.5       ,\n",
      "       0.5       , 1.        , 0.00816392, 0.50044984, 0.54995185,\n",
      "       0.46508053, 0.        , 0.5       , 0.5       , 0.94994235,\n",
      "       0.46556354, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.09735118, 0.48598215, 0.5007216 , 0.03198074, 0.5       ,\n",
      "       0.5       , 1.        , 0.00236888, 0.5005158 , 0.54974043,\n",
      "       0.46509498, 0.        , 0.5       , 0.5       , 0.94973993,\n",
      "       0.4656723 , 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.09720162, 0.48613173, 0.5000044 , 0.02902384, 0.505     ,\n",
      "       0.8       , 0.5       , 0.01434379, 0.49991757, 0.5492814 ,\n",
      "       0.46499678, 0.        , 0.5       , 0.5       , 0.9492813 ,\n",
      "       0.46500072, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.09691866, 0.48641467, 0.4988951 , 0.01501823, 0.5       ,\n",
      "       0.5       , 0.        , 0.02218546, 0.49878576, 0.54902166,\n",
      "       0.4648369 , 0.        , 0.5       , 0.5       , 0.9489988 ,\n",
      "       0.46395266, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32)]\n",
      "Actions: [3, 0, 1, 2, 0, 1, 1, 0, 2, 1]\n",
      "Rewards: [3.267702413156286e-05, 0.0016551870548410204, 0.012392250831141906, 0.012147302156513131, -0.00983322239213314, -0.003842184795637739, 0.02407323990927443, 0.04257458869166735, 0.018193950872328554, 0.004410809936722164]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MetaDrive-validation-v0\", config={\"use_render\": False, \"horizon\": 100})\n",
    "# horizon is the max number of steps in a trajectory\n",
    "def random_policy(obs:npt.NDArray) -> int:\n",
    "    \"\"\"\n",
    "    A random policy that returns a random action\n",
    "    \"\"\"\n",
    "    return np.random.randint(0, NUM_ACTIONS)\n",
    "\n",
    "obs, actions, rewards = collect_trajectory(env, random_policy)\n",
    "\n",
    "# print the first 10 observations, actions, and rewards\n",
    "print(\"Observations:\", obs[:10])\n",
    "print(\"Actions:\", actions[:10])\n",
    "print(\"Rewards:\", rewards[:10])\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward-to-go\n",
    "\n",
    "Our `collect_trajectory` function allows us to gather rewards from the trajectory. However, recall that what we'll actually train the network on is the reward-to-go. Let's now create the function `rewardd_to_go` to compute the reward-to-go for each timestep in the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.85\n",
    "\n",
    "def rewards_to_go(trajectory_rewards: list[float]) -> list[float]:\n",
    "    \"\"\"\n",
    "    Computes the gamma discounted reward-to-go for each state in the trajectory.\n",
    "    \"\"\"\n",
    "\n",
    "    trajectory_len = len(trajectory_rewards)\n",
    "\n",
    "    v_batch = np.zeros(trajectory_len)\n",
    "\n",
    "    v_batch[-1] = trajectory_rewards[-1]\n",
    "\n",
    "    # Use GAMMA to decay the advantage\n",
    "    for t in reversed(range(trajectory_len - 1)):\n",
    "        v_batch[t] = trajectory_rewards[t] + GAMMA * v_batch[t + 1]\n",
    "\n",
    "    return list(v_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network\n",
    "\n",
    "So, now that we have a function that lets us go from policies to trajectories, we should work on creating a neural network based policy. The network should take in an observation, and return a probability for each number between 0 and 9.\n",
    "\n",
    "We're going to keep the network fairly small for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy network\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(259, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, NUM_ACTIONS)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # output in (Batch, Width)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a policy function that allows us to sample an action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deviceof(m: nn.Module) -> torch.device:\n",
    "    \"\"\"\n",
    "    Get the device of the given module\n",
    "    \"\"\"\n",
    "    return next(m.parameters()).device\n",
    "\n",
    "def nn_policy(net:Policy, obs:npt.NDArray) -> int:\n",
    "    \"\"\"\n",
    "    A neural network policy that returns an action based on the given observation\n",
    "    \"\"\"\n",
    "    # convert observation to a tensor\n",
    "    obs_tensor = torch.from_numpy(obs).float().to(deviceof(net))\n",
    "    # add batch dimension\n",
    "    obs_tensor = obs_tensor.unsqueeze(0)\n",
    "    # get the action probabilities\n",
    "    action_probs = net(obs_tensor)\n",
    "    # sample an action from the action probabilities\n",
    "    action = torch.multinomial(action_probs, 1)\n",
    "    return action.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out (untrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: [array([0.09722222, 0.4861111 , 0.5       , 0.01234568, 0.5       ,\n",
      "       0.5       , 0.5       , 0.        , 0.5       , 0.55      ,\n",
      "       0.465     , 0.        , 0.5       , 0.5       , 0.95      ,\n",
      "       0.46500003, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.09722222, 0.4861111 , 0.5       , 0.01480051, 0.5       ,\n",
      "       0.5       , 1.        , 0.        , 0.5       , 0.55      ,\n",
      "       0.465     , 0.        , 0.5       , 0.5       , 0.95      ,\n",
      "       0.46500003, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.09722222, 0.4861111 , 0.5       , 0.02707467, 0.5       ,\n",
      "       0.5       , 1.        , 0.        , 0.5       , 0.5498895 ,\n",
      "       0.465     , 0.        , 0.5       , 0.5       , 0.9498894 ,\n",
      "       0.46500003, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.09722222, 0.4861111 , 0.5       , 0.03934883, 0.5       ,\n",
      "       0.5       , 1.        , 0.        , 0.5       , 0.54950154,\n",
      "       0.465     , 0.        , 0.5       , 0.5       , 0.94949055,\n",
      "       0.46500003, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.0969944 , 0.48633894, 0.49889234, 0.03577001, 0.505     ,\n",
      "       0.8       , 0.5       , 0.02215351, 0.4990887 , 0.54885453,\n",
      "       0.4648513 , 0.        , 0.5       , 0.5       , 0.94883573,\n",
      "       0.46396992, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.09656011, 0.48677322, 0.4967758 , 0.01814219, 0.5       ,\n",
      "       0.5       , 0.        , 0.04233095, 0.49735153, 0.5484337 ,\n",
      "       0.46456763, 0.        , 0.5       , 0.5       , 0.9483904 ,\n",
      "       0.46198756, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.09641809, 0.48691523, 0.49659044, 0.01622056, 0.505     ,\n",
      "       0.8       , 0.5       , 0.00370712, 0.49678347, 0.5483514 ,\n",
      "       0.46452445, 0.        , 0.5       , 0.5       , 0.9483205 ,\n",
      "       0.4617948 , 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.09629234, 0.487041  , 0.49594352, 0.01460665, 0.505     ,\n",
      "       0.8       , 0.5       , 0.01293897, 0.49628046, 0.54823   ,\n",
      "       0.46444002, 0.        , 0.5       , 0.5       , 0.9482058 ,\n",
      "       0.46119383, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.09623753, 0.4870958 , 0.49554604, 0.01255806, 0.495     ,\n",
      "       0.2       , 0.5       , 0.00794954, 0.4960612 , 0.54816353,\n",
      "       0.46439216, 0.        , 0.5       , 0.5       , 0.9481374 ,\n",
      "       0.46082792, 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32), array([0.09623456, 0.48709878, 0.49548188, 0.01252151, 0.495     ,\n",
      "       0.2       , 0.5       , 0.00128364, 0.49604934, 0.54815394,\n",
      "       0.46438557, 0.        , 0.5       , 0.5       , 0.9481257 ,\n",
      "       0.4607708 , 0.        , 0.5       , 0.5       , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        ], dtype=float32)]\n",
      "Actions: [1, 1, 1, 0, 2, 0, 0, 3, 3, 3]\n",
      "Rewards: [0.0002485517263412476, 0.012538196802139308, 0.041397456288337735, 0.059216425101088994, 0.02749195674285225, 0.007452859498355888, 0.007884067474200222, 0.0038395391742109598, 0.0005032230892495803, -3.073510800127997e-05]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MetaDrive-validation-v0\", config={\"use_render\": False, \"horizon\": 100})\n",
    "\n",
    "policy = Policy()\n",
    "obs, actions, rewards = collect_trajectory(env, lambda obs: nn_policy(policy, obs))\n",
    "# print the first 10 observations, actions, and rewards\n",
    "print(\"Observations:\", obs[:10])\n",
    "print(\"Actions:\", actions[:10])\n",
    "print(\"Rewards:\", rewards[:10])\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it performs basically random actions, since it hasn't been trained yet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Policy Gradient\n",
    "\n",
    "Let's now work on the meat of the problem: computing the policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTROPY_BONUS = 0.1\n",
    "\n",
    "def compute_policy_gradient_loss(\n",
    "    # Current policy network's probability of choosing an action\n",
    "    # in (Batch, Action)\n",
    "    pi_theta_given_st: torch.Tensor,\n",
    "    # One hot encoding of which action was chosen\n",
    "    # in (Batch, Action)\n",
    "    a_t: torch.Tensor,\n",
    "    # Rewards To Go for the chosen action\n",
    "    # in (Batch,)\n",
    "    R_t: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Computes the policy gradient loss for a vector of examples, and reduces with mean.\n",
    "\n",
    "    The standard policy gradient is given by the expected value over trajectories of:\n",
    "\n",
    "    :math:`\\sum_{t=0}^{T} \\nabla_{\\theta} (\\log \\pi_{\\theta}(a_t|s_t))R_t`\n",
    "    \n",
    "    where:\n",
    "    * :math:`\\pi_{\\theta}(a_t|s_t)` is the current policy's probability to perform action :math:`a_t` given :math:`s_t`\n",
    "    * :math:`R_t` is the rewards-to-go from the state at time t to the end of the episode from which it came.\n",
    "    \"\"\"\n",
    "\n",
    "    # here, the multiplication and sum is in order to extract the\n",
    "    # in (Batch,)\n",
    "    pi_theta_at_given_st = torch.sum(pi_theta_given_st * a_t, 1)\n",
    "\n",
    "    # Note: this loss has doesn't actually represent whether the action was good or bad\n",
    "    # it is a dummy loss, that is only used to compute the gradient\n",
    "\n",
    "    # Recall that the policy gradient for a single transition (state-action pair) is given by:\n",
    "    # $\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)R_t$\n",
    "    # However, it's easier to work with losses, rather than raw gradients.\n",
    "    # Therefore we construct a loss, that when differentiated, gives us the policy gradient.\n",
    "    # this loss is given by:\n",
    "    # $-\\log \\pi_{\\theta}(a_t|s_t)R_t$\n",
    "\n",
    "    # in (Batch,)\n",
    "    policy_loss_per_example = -torch.log(pi_theta_at_given_st) * R_t\n",
    "\n",
    "    # in (Batch,)\n",
    "    entropy_per_example = -torch.sum(\n",
    "        torch.log(pi_theta_given_st) * pi_theta_given_st, 1\n",
    "    )\n",
    "\n",
    "    # we reward entropy, since excessive certainty indicate the model is 'overfitting'\n",
    "    loss_per_example = policy_loss_per_example - ENTROPY_BONUS * entropy_per_example\n",
    "\n",
    "    # we take the average loss over all examples\n",
    "    return loss_per_example.mean()\n",
    "\n",
    "\n",
    "def train_policygradient(\n",
    "    policy: Policy,\n",
    "    policy_optimizer: torch.optim.Optimizer,\n",
    "    observation_batch: list[npt.NDArray],\n",
    "    action_batch: list[int],\n",
    "    rtg_batch: list[float],\n",
    ") -> float:\n",
    "    # assert that the batch_lengths are the same\n",
    "    assert len(observation_batch) == len(action_batch)\n",
    "    assert len(observation_batch) == len(rtg_batch)\n",
    "\n",
    "    # get device\n",
    "    device = deviceof(policy)\n",
    "\n",
    "    # convert data to tensors on correct device\n",
    "\n",
    "    # in (Batch, Width)\n",
    "    observation_batch_tensor = torch.from_numpy(np.stack(observation_batch)).to(device)\n",
    "\n",
    "    # in (Batch,)\n",
    "    rtg_batch_tensor = torch.tensor(\n",
    "        rtg_batch, dtype=torch.float32, device=device\n",
    "    )\n",
    "\n",
    "    # in (Batch, Action)\n",
    "    chosen_action_tensor = F.one_hot(\n",
    "        torch.tensor(action_batch).to(device).long(), num_classes=NUM_ACTIONS\n",
    "    )\n",
    "\n",
    "    # train policy network\n",
    "    policy_optimizer.zero_grad()\n",
    "    action_probs = policy.forward(observation_batch_tensor)\n",
    "    policy_loss = compute_policy_gradient_loss(\n",
    "        action_probs, chosen_action_tensor, rtg_batch_tensor\n",
    "    )\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "\n",
    "    # return the respective losses\n",
    "    return policy_loss.item()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Policy Network\n",
    "\n",
    "With that, we're done. Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable logging from metadrive\n",
    "import logging\n",
    "import inspect\n",
    "logging.getLogger(inspect.getfile(metadrive.envs.base_env)).setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "policy = Policy().to(device)\n",
    "policy_optimizer = torch.optim.Adam(policy.parameters(), lr=2e-4)\n",
    "\n",
    "step = 0\n",
    "rewards = []\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MetaDrive-validation-v0\", config={\"use_render\": False, \"horizon\": 500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14, Policy Loss: 0.921, Avg. Returns: 37.040\n",
      "Step 15, Policy Loss: 1.359, Avg. Returns: 34.912\n",
      "Step 16, Policy Loss: 1.114, Avg. Returns: 29.423\n",
      "Step 17, Policy Loss: 1.909, Avg. Returns: 53.920\n",
      "Step 18, Policy Loss: 1.525, Avg. Returns: 29.942\n",
      "Step 19, Policy Loss: 1.460, Avg. Returns: 18.885\n",
      "Step 20, Policy Loss: 2.528, Avg. Returns: 43.482\n",
      "Step 21, Policy Loss: 2.295, Avg. Returns: 37.894\n",
      "Step 22, Policy Loss: 2.222, Avg. Returns: 30.216\n",
      "Step 23, Policy Loss: 2.604, Avg. Returns: 44.553\n",
      "Step 24, Policy Loss: 2.974, Avg. Returns: 54.903\n",
      "Step 25, Policy Loss: 1.962, Avg. Returns: 28.092\n",
      "Step 26, Policy Loss: 2.772, Avg. Returns: 48.713\n",
      "Step 27, Policy Loss: 1.942, Avg. Returns: 27.530\n",
      "Step 28, Policy Loss: 2.161, Avg. Returns: 33.700\n",
      "Step 29, Policy Loss: 2.479, Avg. Returns: 39.786\n",
      "Step 30, Policy Loss: 2.581, Avg. Returns: 47.125\n",
      "Step 31, Policy Loss: 2.445, Avg. Returns: 50.106\n",
      "Step 32, Policy Loss: 2.459, Avg. Returns: 49.964\n",
      "Step 33, Policy Loss: 2.420, Avg. Returns: 53.013\n",
      "Step 34, Policy Loss: 2.605, Avg. Returns: 68.419\n",
      "Step 35, Policy Loss: 2.223, Avg. Returns: 41.677\n",
      "Step 36, Policy Loss: 2.256, Avg. Returns: 57.473\n",
      "Step 37, Policy Loss: 2.115, Avg. Returns: 50.381\n",
      "Step 38, Policy Loss: 2.156, Avg. Returns: 56.126\n",
      "Step 39, Policy Loss: 2.113, Avg. Returns: 56.383\n",
      "Step 40, Policy Loss: 2.021, Avg. Returns: 64.794\n",
      "Step 41, Policy Loss: 2.359, Avg. Returns: 58.760\n",
      "Step 42, Policy Loss: 1.990, Avg. Returns: 56.846\n",
      "Step 43, Policy Loss: 2.385, Avg. Returns: 64.907\n",
      "Step 44, Policy Loss: 1.691, Avg. Returns: 47.432\n",
      "Step 45, Policy Loss: 2.268, Avg. Returns: 53.606\n",
      "Step 46, Policy Loss: 2.024, Avg. Returns: 63.453\n",
      "Step 47, Policy Loss: 2.270, Avg. Returns: 65.196\n",
      "Step 48, Policy Loss: 2.114, Avg. Returns: 53.485\n",
      "Step 49, Policy Loss: 2.487, Avg. Returns: 61.647\n",
      "Step 50, Policy Loss: 2.466, Avg. Returns: 66.326\n",
      "Step 51, Policy Loss: 1.984, Avg. Returns: 55.195\n",
      "Step 52, Policy Loss: 2.122, Avg. Returns: 52.686\n",
      "Step 53, Policy Loss: 2.239, Avg. Returns: 57.866\n",
      "Step 54, Policy Loss: 2.819, Avg. Returns: 67.542\n",
      "Step 55, Policy Loss: 2.440, Avg. Returns: 59.997\n",
      "Step 56, Policy Loss: 2.329, Avg. Returns: 60.862\n",
      "Step 57, Policy Loss: 2.362, Avg. Returns: 60.456\n",
      "Step 58, Policy Loss: 2.226, Avg. Returns: 56.254\n",
      "Step 59, Policy Loss: 2.460, Avg. Returns: 59.234\n",
      "Step 60, Policy Loss: 2.566, Avg. Returns: 65.018\n",
      "Step 61, Policy Loss: 2.161, Avg. Returns: 57.181\n",
      "Step 62, Policy Loss: 1.830, Avg. Returns: 44.467\n",
      "Step 63, Policy Loss: 1.705, Avg. Returns: 39.939\n",
      "Step 64, Policy Loss: 2.303, Avg. Returns: 64.042\n",
      "Step 65, Policy Loss: 2.383, Avg. Returns: 56.428\n",
      "Step 66, Policy Loss: 2.305, Avg. Returns: 61.989\n",
      "Step 67, Policy Loss: 1.728, Avg. Returns: 44.823\n",
      "Step 68, Policy Loss: 2.100, Avg. Returns: 55.912\n",
      "Step 69, Policy Loss: 1.725, Avg. Returns: 54.041\n",
      "Step 70, Policy Loss: 2.257, Avg. Returns: 55.769\n",
      "Step 71, Policy Loss: 2.299, Avg. Returns: 56.699\n",
      "Step 72, Policy Loss: 2.045, Avg. Returns: 54.356\n",
      "Step 73, Policy Loss: 1.994, Avg. Returns: 47.200\n",
      "Step 74, Policy Loss: 2.099, Avg. Returns: 53.984\n",
      "Step 75, Policy Loss: 1.975, Avg. Returns: 43.756\n",
      "Step 76, Policy Loss: 2.286, Avg. Returns: 58.105\n",
      "Step 77, Policy Loss: 2.089, Avg. Returns: 58.149\n",
      "Step 78, Policy Loss: 2.189, Avg. Returns: 51.327\n",
      "Step 79, Policy Loss: 2.922, Avg. Returns: 70.035\n",
      "Step 80, Policy Loss: 2.539, Avg. Returns: 66.802\n",
      "Step 81, Policy Loss: 2.788, Avg. Returns: 68.477\n",
      "Step 82, Policy Loss: 2.597, Avg. Returns: 65.837\n",
      "Step 83, Policy Loss: 2.774, Avg. Returns: 54.492\n",
      "Step 84, Policy Loss: 2.412, Avg. Returns: 54.375\n",
      "Step 85, Policy Loss: 2.371, Avg. Returns: 48.604\n",
      "Step 86, Policy Loss: 2.697, Avg. Returns: 58.972\n",
      "Step 87, Policy Loss: 1.968, Avg. Returns: 37.689\n",
      "Step 88, Policy Loss: 2.186, Avg. Returns: 49.021\n",
      "Step 89, Policy Loss: 2.314, Avg. Returns: 47.725\n",
      "Step 90, Policy Loss: 2.597, Avg. Returns: 49.443\n",
      "Step 91, Policy Loss: 2.783, Avg. Returns: 60.210\n",
      "Step 92, Policy Loss: 2.681, Avg. Returns: 56.239\n",
      "Step 93, Policy Loss: 2.717, Avg. Returns: 57.375\n",
      "Step 94, Policy Loss: 2.185, Avg. Returns: 44.194\n",
      "Step 95, Policy Loss: 2.532, Avg. Returns: 51.490\n",
      "Step 96, Policy Loss: 2.578, Avg. Returns: 57.863\n",
      "Step 97, Policy Loss: 2.734, Avg. Returns: 64.272\n",
      "Step 98, Policy Loss: 2.528, Avg. Returns: 55.502\n",
      "Step 99, Policy Loss: 2.676, Avg. Returns: 60.899\n",
      "Step 100, Policy Loss: 2.369, Avg. Returns: 45.676\n",
      "Step 101, Policy Loss: 2.526, Avg. Returns: 47.678\n",
      "Step 102, Policy Loss: 2.752, Avg. Returns: 58.074\n",
      "Step 103, Policy Loss: 2.194, Avg. Returns: 43.639\n",
      "Step 104, Policy Loss: 2.180, Avg. Returns: 47.261\n",
      "Step 105, Policy Loss: 2.252, Avg. Returns: 57.073\n",
      "Step 106, Policy Loss: 2.232, Avg. Returns: 51.671\n",
      "Step 107, Policy Loss: 2.662, Avg. Returns: 62.544\n",
      "Step 108, Policy Loss: 2.566, Avg. Returns: 61.621\n",
      "Step 109, Policy Loss: 2.303, Avg. Returns: 59.260\n",
      "Step 110, Policy Loss: 2.293, Avg. Returns: 69.067\n",
      "Step 111, Policy Loss: 2.012, Avg. Returns: 59.676\n",
      "Step 112, Policy Loss: 1.919, Avg. Returns: 51.309\n",
      "Step 113, Policy Loss: 2.003, Avg. Returns: 60.840\n",
      "Step 114, Policy Loss: 2.069, Avg. Returns: 56.346\n",
      "Step 115, Policy Loss: 1.970, Avg. Returns: 67.726\n",
      "Step 116, Policy Loss: 1.373, Avg. Returns: 48.313\n",
      "Step 117, Policy Loss: 2.016, Avg. Returns: 61.927\n",
      "Step 118, Policy Loss: 2.030, Avg. Returns: 64.563\n",
      "Step 119, Policy Loss: 1.362, Avg. Returns: 48.886\n",
      "Step 120, Policy Loss: 1.354, Avg. Returns: 58.710\n",
      "Step 121, Policy Loss: 1.479, Avg. Returns: 65.554\n",
      "Step 122, Policy Loss: 1.459, Avg. Returns: 54.574\n",
      "Step 123, Policy Loss: 1.460, Avg. Returns: 55.515\n",
      "Step 124, Policy Loss: 1.857, Avg. Returns: 63.291\n",
      "Step 125, Policy Loss: 1.283, Avg. Returns: 53.276\n",
      "Step 126, Policy Loss: 1.631, Avg. Returns: 62.186\n",
      "Step 127, Policy Loss: 1.286, Avg. Returns: 57.962\n",
      "Step 128, Policy Loss: 1.584, Avg. Returns: 63.942\n",
      "Step 129, Policy Loss: 1.544, Avg. Returns: 61.998\n",
      "Step 130, Policy Loss: 1.392, Avg. Returns: 59.410\n",
      "Step 131, Policy Loss: 1.583, Avg. Returns: 60.370\n",
      "Step 132, Policy Loss: 1.687, Avg. Returns: 66.671\n",
      "Step 133, Policy Loss: 1.544, Avg. Returns: 55.020\n",
      "Step 134, Policy Loss: 1.605, Avg. Returns: 63.229\n",
      "Step 135, Policy Loss: 1.497, Avg. Returns: 61.333\n",
      "Step 136, Policy Loss: 1.633, Avg. Returns: 58.045\n",
      "Step 137, Policy Loss: 1.476, Avg. Returns: 54.698\n",
      "Step 138, Policy Loss: 1.579, Avg. Returns: 62.050\n",
      "Step 139, Policy Loss: 1.593, Avg. Returns: 63.401\n",
      "Step 140, Policy Loss: 1.847, Avg. Returns: 63.940\n",
      "Step 141, Policy Loss: 1.737, Avg. Returns: 66.601\n",
      "Step 142, Policy Loss: 1.304, Avg. Returns: 62.614\n",
      "Step 143, Policy Loss: 1.592, Avg. Returns: 60.808\n",
      "Step 144, Policy Loss: 1.513, Avg. Returns: 59.664\n",
      "Step 145, Policy Loss: 1.541, Avg. Returns: 62.039\n",
      "Step 146, Policy Loss: 1.785, Avg. Returns: 69.762\n",
      "Step 147, Policy Loss: 1.489, Avg. Returns: 67.760\n",
      "Step 148, Policy Loss: 1.522, Avg. Returns: 62.631\n",
      "Step 149, Policy Loss: 1.670, Avg. Returns: 60.722\n",
      "Step 150, Policy Loss: 1.481, Avg. Returns: 63.002\n",
      "Step 151, Policy Loss: 1.852, Avg. Returns: 61.535\n",
      "Step 152, Policy Loss: 1.775, Avg. Returns: 63.930\n",
      "Step 153, Policy Loss: 1.498, Avg. Returns: 57.967\n",
      "Step 154, Policy Loss: 1.904, Avg. Returns: 61.148\n",
      "Step 155, Policy Loss: 1.632, Avg. Returns: 63.338\n",
      "Step 156, Policy Loss: 1.953, Avg. Returns: 55.027\n",
      "Step 157, Policy Loss: 1.829, Avg. Returns: 66.722\n",
      "Step 158, Policy Loss: 2.331, Avg. Returns: 67.472\n",
      "Step 159, Policy Loss: 1.790, Avg. Returns: 57.869\n",
      "Step 160, Policy Loss: 2.527, Avg. Returns: 53.779\n",
      "Step 161, Policy Loss: 2.454, Avg. Returns: 59.982\n",
      "Step 162, Policy Loss: 2.273, Avg. Returns: 61.045\n",
      "Step 163, Policy Loss: 2.262, Avg. Returns: 47.577\n",
      "Step 164, Policy Loss: 2.191, Avg. Returns: 43.351\n",
      "Step 165, Policy Loss: 2.478, Avg. Returns: 57.192\n",
      "Step 166, Policy Loss: 2.141, Avg. Returns: 41.107\n",
      "Step 167, Policy Loss: 2.123, Avg. Returns: 37.458\n",
      "Step 168, Policy Loss: 1.743, Avg. Returns: 28.090\n",
      "Step 169, Policy Loss: 1.631, Avg. Returns: 26.976\n",
      "Step 170, Policy Loss: 2.080, Avg. Returns: 36.852\n",
      "Step 171, Policy Loss: 2.414, Avg. Returns: 42.022\n",
      "Step 172, Policy Loss: 2.014, Avg. Returns: 31.373\n",
      "Step 173, Policy Loss: 1.977, Avg. Returns: 35.433\n",
      "Step 174, Policy Loss: 2.053, Avg. Returns: 31.809\n",
      "Step 175, Policy Loss: 2.105, Avg. Returns: 34.903\n",
      "Step 176, Policy Loss: 2.201, Avg. Returns: 41.845\n",
      "Step 177, Policy Loss: 2.207, Avg. Returns: 45.643\n",
      "Step 178, Policy Loss: 2.761, Avg. Returns: 53.800\n",
      "Step 179, Policy Loss: 2.641, Avg. Returns: 52.214\n",
      "Step 180, Policy Loss: 2.531, Avg. Returns: 47.062\n",
      "Step 181, Policy Loss: 2.125, Avg. Returns: 39.761\n",
      "Step 182, Policy Loss: 2.515, Avg. Returns: 49.160\n",
      "Step 183, Policy Loss: 2.444, Avg. Returns: 55.100\n",
      "Step 184, Policy Loss: 2.631, Avg. Returns: 61.323\n",
      "Step 185, Policy Loss: 2.788, Avg. Returns: 64.610\n",
      "Step 186, Policy Loss: 2.333, Avg. Returns: 57.174\n",
      "Step 187, Policy Loss: 2.089, Avg. Returns: 42.585\n",
      "Step 188, Policy Loss: 2.433, Avg. Returns: 62.671\n",
      "Step 189, Policy Loss: 2.478, Avg. Returns: 54.222\n",
      "Step 190, Policy Loss: 2.034, Avg. Returns: 47.736\n",
      "Step 191, Policy Loss: 2.180, Avg. Returns: 44.086\n",
      "Step 192, Policy Loss: 2.151, Avg. Returns: 50.635\n",
      "Step 193, Policy Loss: 2.170, Avg. Returns: 50.882\n",
      "Step 194, Policy Loss: 2.318, Avg. Returns: 54.748\n",
      "Step 195, Policy Loss: 2.632, Avg. Returns: 62.715\n",
      "Step 196, Policy Loss: 2.250, Avg. Returns: 53.532\n",
      "Step 197, Policy Loss: 2.132, Avg. Returns: 47.359\n",
      "Step 198, Policy Loss: 2.385, Avg. Returns: 46.274\n",
      "Step 199, Policy Loss: 2.057, Avg. Returns: 50.867\n",
      "Step 200, Policy Loss: 2.306, Avg. Returns: 51.221\n",
      "Step 201, Policy Loss: 2.443, Avg. Returns: 60.111\n",
      "Step 202, Policy Loss: 2.505, Avg. Returns: 69.837\n",
      "Step 203, Policy Loss: 2.497, Avg. Returns: 55.913\n",
      "Step 204, Policy Loss: 2.352, Avg. Returns: 60.968\n",
      "Step 205, Policy Loss: 2.276, Avg. Returns: 48.125\n",
      "Step 206, Policy Loss: 2.429, Avg. Returns: 65.552\n",
      "Step 207, Policy Loss: 2.510, Avg. Returns: 60.658\n",
      "Step 208, Policy Loss: 2.735, Avg. Returns: 68.173\n",
      "Step 209, Policy Loss: 2.541, Avg. Returns: 52.351\n",
      "Step 210, Policy Loss: 2.410, Avg. Returns: 54.749\n",
      "Step 211, Policy Loss: 2.523, Avg. Returns: 60.570\n",
      "Step 212, Policy Loss: 2.504, Avg. Returns: 56.590\n",
      "Step 213, Policy Loss: 2.523, Avg. Returns: 53.706\n",
      "Step 214, Policy Loss: 2.569, Avg. Returns: 59.077\n",
      "Step 215, Policy Loss: 2.812, Avg. Returns: 65.220\n",
      "Step 216, Policy Loss: 2.464, Avg. Returns: 56.882\n",
      "Step 217, Policy Loss: 2.863, Avg. Returns: 60.834\n",
      "Step 218, Policy Loss: 2.732, Avg. Returns: 58.928\n",
      "Step 219, Policy Loss: 2.780, Avg. Returns: 65.986\n",
      "Step 220, Policy Loss: 2.619, Avg. Returns: 67.378\n",
      "Step 221, Policy Loss: 2.650, Avg. Returns: 54.959\n",
      "Step 222, Policy Loss: 2.492, Avg. Returns: 53.222\n",
      "Step 223, Policy Loss: 2.463, Avg. Returns: 52.168\n",
      "Step 224, Policy Loss: 2.687, Avg. Returns: 60.900\n",
      "Step 225, Policy Loss: 2.330, Avg. Returns: 49.629\n",
      "Step 226, Policy Loss: 2.634, Avg. Returns: 62.252\n",
      "Step 227, Policy Loss: 2.374, Avg. Returns: 55.178\n",
      "Step 228, Policy Loss: 2.769, Avg. Returns: 58.791\n",
      "Step 229, Policy Loss: 2.327, Avg. Returns: 58.580\n",
      "Step 230, Policy Loss: 2.750, Avg. Returns: 68.146\n",
      "Step 231, Policy Loss: 2.278, Avg. Returns: 54.602\n",
      "Step 232, Policy Loss: 2.732, Avg. Returns: 59.463\n",
      "Step 233, Policy Loss: 2.160, Avg. Returns: 47.613\n",
      "Step 234, Policy Loss: 2.720, Avg. Returns: 66.611\n",
      "Step 235, Policy Loss: 2.240, Avg. Returns: 45.597\n",
      "Step 236, Policy Loss: 2.683, Avg. Returns: 62.655\n",
      "Step 237, Policy Loss: 2.577, Avg. Returns: 59.590\n",
      "Step 238, Policy Loss: 2.267, Avg. Returns: 51.146\n",
      "Step 239, Policy Loss: 2.488, Avg. Returns: 55.248\n",
      "Step 240, Policy Loss: 2.698, Avg. Returns: 70.107\n",
      "Step 241, Policy Loss: 2.427, Avg. Returns: 55.006\n",
      "Step 242, Policy Loss: 2.784, Avg. Returns: 69.908\n",
      "Step 243, Policy Loss: 2.129, Avg. Returns: 63.071\n",
      "Step 244, Policy Loss: 2.344, Avg. Returns: 51.524\n",
      "Step 245, Policy Loss: 2.103, Avg. Returns: 45.639\n",
      "Step 246, Policy Loss: 2.322, Avg. Returns: 48.949\n",
      "Step 247, Policy Loss: 2.352, Avg. Returns: 59.893\n",
      "Step 248, Policy Loss: 2.255, Avg. Returns: 55.774\n",
      "Step 249, Policy Loss: 2.201, Avg. Returns: 58.993\n",
      "Step 250, Policy Loss: 2.277, Avg. Returns: 57.020\n",
      "Step 251, Policy Loss: 2.246, Avg. Returns: 67.759\n",
      "Step 252, Policy Loss: 2.313, Avg. Returns: 57.749\n",
      "Step 253, Policy Loss: 2.390, Avg. Returns: 57.511\n",
      "Step 254, Policy Loss: 2.431, Avg. Returns: 70.625\n",
      "Step 255, Policy Loss: 2.220, Avg. Returns: 59.694\n",
      "Step 256, Policy Loss: 2.205, Avg. Returns: 53.094\n",
      "Step 257, Policy Loss: 1.806, Avg. Returns: 50.760\n",
      "Step 258, Policy Loss: 1.928, Avg. Returns: 60.083\n",
      "Step 259, Policy Loss: 2.257, Avg. Returns: 63.575\n",
      "Step 260, Policy Loss: 2.356, Avg. Returns: 56.275\n",
      "Step 261, Policy Loss: 2.173, Avg. Returns: 56.273\n",
      "Step 262, Policy Loss: 1.822, Avg. Returns: 60.877\n",
      "Step 263, Policy Loss: 1.984, Avg. Returns: 62.432\n",
      "Step 264, Policy Loss: 2.209, Avg. Returns: 61.559\n",
      "Step 265, Policy Loss: 2.095, Avg. Returns: 56.583\n",
      "Step 266, Policy Loss: 2.111, Avg. Returns: 60.432\n",
      "Step 267, Policy Loss: 2.171, Avg. Returns: 64.424\n",
      "Step 268, Policy Loss: 2.108, Avg. Returns: 64.150\n",
      "Step 269, Policy Loss: 1.804, Avg. Returns: 61.086\n",
      "Step 270, Policy Loss: 2.062, Avg. Returns: 65.648\n",
      "Step 271, Policy Loss: 2.081, Avg. Returns: 59.569\n",
      "Step 272, Policy Loss: 1.739, Avg. Returns: 53.514\n",
      "Step 273, Policy Loss: 1.742, Avg. Returns: 58.046\n",
      "Step 274, Policy Loss: 1.974, Avg. Returns: 68.208\n",
      "Step 275, Policy Loss: 1.478, Avg. Returns: 60.709\n",
      "Step 276, Policy Loss: 2.193, Avg. Returns: 66.045\n",
      "Step 277, Policy Loss: 1.735, Avg. Returns: 65.478\n",
      "Step 278, Policy Loss: 1.892, Avg. Returns: 63.544\n",
      "Step 279, Policy Loss: 1.989, Avg. Returns: 61.248\n",
      "Step 280, Policy Loss: 1.514, Avg. Returns: 62.213\n",
      "Step 281, Policy Loss: 1.693, Avg. Returns: 64.976\n",
      "Step 282, Policy Loss: 1.652, Avg. Returns: 62.738\n",
      "Step 283, Policy Loss: 1.600, Avg. Returns: 67.586\n",
      "Step 284, Policy Loss: 1.598, Avg. Returns: 59.008\n",
      "Step 285, Policy Loss: 1.561, Avg. Returns: 58.787\n",
      "Step 286, Policy Loss: 1.511, Avg. Returns: 61.088\n",
      "Step 287, Policy Loss: 1.689, Avg. Returns: 69.527\n",
      "Step 288, Policy Loss: 1.711, Avg. Returns: 66.503\n",
      "Step 289, Policy Loss: 1.578, Avg. Returns: 68.125\n",
      "Step 290, Policy Loss: 1.372, Avg. Returns: 63.302\n",
      "Step 291, Policy Loss: 1.785, Avg. Returns: 63.735\n",
      "Step 292, Policy Loss: 1.512, Avg. Returns: 61.925\n",
      "Step 293, Policy Loss: 1.521, Avg. Returns: 61.797\n",
      "Step 294, Policy Loss: 1.774, Avg. Returns: 65.856\n",
      "Step 295, Policy Loss: 1.298, Avg. Returns: 55.556\n",
      "Step 296, Policy Loss: 1.835, Avg. Returns: 66.949\n",
      "Step 297, Policy Loss: 1.526, Avg. Returns: 64.357\n",
      "Step 298, Policy Loss: 1.668, Avg. Returns: 71.607\n",
      "Step 299, Policy Loss: 1.552, Avg. Returns: 65.673\n",
      "Step 300, Policy Loss: 1.493, Avg. Returns: 59.051\n",
      "Step 301, Policy Loss: 2.064, Avg. Returns: 63.167\n",
      "Step 302, Policy Loss: 1.813, Avg. Returns: 54.152\n",
      "Step 303, Policy Loss: 1.701, Avg. Returns: 65.403\n",
      "Step 304, Policy Loss: 1.687, Avg. Returns: 71.959\n",
      "Step 305, Policy Loss: 2.087, Avg. Returns: 68.753\n",
      "Step 306, Policy Loss: 1.463, Avg. Returns: 58.006\n",
      "Step 307, Policy Loss: 1.968, Avg. Returns: 64.994\n",
      "Step 308, Policy Loss: 1.834, Avg. Returns: 55.765\n",
      "Step 309, Policy Loss: 1.904, Avg. Returns: 58.622\n",
      "Step 310, Policy Loss: 1.837, Avg. Returns: 54.704\n",
      "Step 311, Policy Loss: 1.555, Avg. Returns: 53.309\n",
      "Step 312, Policy Loss: 1.728, Avg. Returns: 62.782\n",
      "Step 313, Policy Loss: 1.991, Avg. Returns: 63.364\n",
      "Step 314, Policy Loss: 1.632, Avg. Returns: 58.186\n",
      "Step 315, Policy Loss: 1.980, Avg. Returns: 49.851\n",
      "Step 316, Policy Loss: 1.852, Avg. Returns: 57.265\n",
      "Step 317, Policy Loss: 1.793, Avg. Returns: 65.426\n",
      "Step 318, Policy Loss: 1.868, Avg. Returns: 59.128\n",
      "Step 319, Policy Loss: 1.660, Avg. Returns: 56.981\n",
      "Step 320, Policy Loss: 1.809, Avg. Returns: 57.852\n",
      "Step 321, Policy Loss: 1.949, Avg. Returns: 63.728\n",
      "Step 322, Policy Loss: 1.973, Avg. Returns: 58.957\n",
      "Step 323, Policy Loss: 2.095, Avg. Returns: 68.689\n",
      "Step 324, Policy Loss: 2.069, Avg. Returns: 65.581\n",
      "Step 325, Policy Loss: 1.887, Avg. Returns: 61.535\n",
      "Step 326, Policy Loss: 1.545, Avg. Returns: 58.759\n",
      "Step 327, Policy Loss: 1.791, Avg. Returns: 66.205\n",
      "Step 328, Policy Loss: 1.675, Avg. Returns: 68.167\n",
      "Step 329, Policy Loss: 1.666, Avg. Returns: 65.974\n",
      "Step 330, Policy Loss: 1.465, Avg. Returns: 52.150\n",
      "Step 331, Policy Loss: 1.847, Avg. Returns: 68.311\n",
      "Step 332, Policy Loss: 1.231, Avg. Returns: 61.719\n",
      "Step 333, Policy Loss: 1.800, Avg. Returns: 68.701\n",
      "Step 334, Policy Loss: 1.645, Avg. Returns: 64.482\n",
      "Step 335, Policy Loss: 1.613, Avg. Returns: 60.906\n",
      "Step 336, Policy Loss: 1.541, Avg. Returns: 62.217\n",
      "Step 337, Policy Loss: 1.275, Avg. Returns: 64.846\n",
      "Step 338, Policy Loss: 1.272, Avg. Returns: 60.187\n",
      "Step 339, Policy Loss: 1.403, Avg. Returns: 59.958\n",
      "Step 340, Policy Loss: 1.329, Avg. Returns: 65.286\n",
      "Step 341, Policy Loss: 1.149, Avg. Returns: 57.368\n",
      "Step 342, Policy Loss: 1.035, Avg. Returns: 59.639\n",
      "Step 343, Policy Loss: 1.230, Avg. Returns: 64.346\n",
      "Step 344, Policy Loss: 1.391, Avg. Returns: 64.545\n",
      "Step 345, Policy Loss: 1.317, Avg. Returns: 58.265\n",
      "Step 346, Policy Loss: 1.467, Avg. Returns: 66.537\n",
      "Step 347, Policy Loss: 1.230, Avg. Returns: 59.071\n",
      "Step 348, Policy Loss: 0.810, Avg. Returns: 56.468\n",
      "Step 349, Policy Loss: 1.246, Avg. Returns: 63.041\n",
      "Step 350, Policy Loss: 1.116, Avg. Returns: 52.184\n",
      "Step 351, Policy Loss: 1.016, Avg. Returns: 59.646\n",
      "Step 352, Policy Loss: 0.941, Avg. Returns: 55.611\n",
      "Step 353, Policy Loss: 1.027, Avg. Returns: 63.888\n",
      "Step 354, Policy Loss: 0.900, Avg. Returns: 58.819\n",
      "Step 355, Policy Loss: 1.260, Avg. Returns: 61.391\n",
      "Step 356, Policy Loss: 1.279, Avg. Returns: 59.833\n",
      "Step 357, Policy Loss: 0.633, Avg. Returns: 48.219\n",
      "Step 358, Policy Loss: 1.330, Avg. Returns: 58.646\n",
      "Step 359, Policy Loss: 0.905, Avg. Returns: 53.296\n",
      "Step 360, Policy Loss: 1.415, Avg. Returns: 67.317\n",
      "Step 361, Policy Loss: 1.210, Avg. Returns: 60.835\n",
      "Step 362, Policy Loss: 1.161, Avg. Returns: 55.033\n",
      "Step 363, Policy Loss: 1.122, Avg. Returns: 54.522\n",
      "Step 364, Policy Loss: 1.289, Avg. Returns: 61.791\n",
      "Step 365, Policy Loss: 1.133, Avg. Returns: 61.683\n",
      "Step 366, Policy Loss: 1.408, Avg. Returns: 62.811\n",
      "Step 367, Policy Loss: 1.435, Avg. Returns: 63.191\n",
      "Step 368, Policy Loss: 1.503, Avg. Returns: 69.600\n",
      "Step 369, Policy Loss: 1.227, Avg. Returns: 55.332\n",
      "Step 370, Policy Loss: 1.614, Avg. Returns: 64.089\n",
      "Step 371, Policy Loss: 1.467, Avg. Returns: 66.206\n",
      "Step 372, Policy Loss: 1.718, Avg. Returns: 55.281\n",
      "Step 373, Policy Loss: 1.982, Avg. Returns: 69.055\n",
      "Step 374, Policy Loss: 1.537, Avg. Returns: 63.476\n",
      "Step 375, Policy Loss: 1.879, Avg. Returns: 65.333\n",
      "Step 376, Policy Loss: 2.270, Avg. Returns: 71.807\n",
      "Step 377, Policy Loss: 1.888, Avg. Returns: 68.592\n",
      "Step 378, Policy Loss: 1.653, Avg. Returns: 50.504\n",
      "Step 379, Policy Loss: 1.903, Avg. Returns: 61.589\n",
      "Step 380, Policy Loss: 2.041, Avg. Returns: 62.949\n",
      "Step 381, Policy Loss: 2.056, Avg. Returns: 65.501\n",
      "Step 382, Policy Loss: 1.998, Avg. Returns: 72.812\n",
      "Step 383, Policy Loss: 1.885, Avg. Returns: 61.846\n",
      "Step 384, Policy Loss: 2.208, Avg. Returns: 70.325\n",
      "Step 385, Policy Loss: 2.115, Avg. Returns: 67.826\n",
      "Step 386, Policy Loss: 2.292, Avg. Returns: 61.599\n",
      "Step 387, Policy Loss: 2.316, Avg. Returns: 69.077\n",
      "Step 388, Policy Loss: 2.158, Avg. Returns: 58.885\n",
      "Step 389, Policy Loss: 2.068, Avg. Returns: 67.225\n",
      "Step 390, Policy Loss: 2.161, Avg. Returns: 69.864\n",
      "Step 391, Policy Loss: 2.369, Avg. Returns: 61.059\n",
      "Step 392, Policy Loss: 1.849, Avg. Returns: 54.329\n",
      "Step 393, Policy Loss: 2.360, Avg. Returns: 73.178\n",
      "Step 394, Policy Loss: 1.851, Avg. Returns: 62.631\n",
      "Step 395, Policy Loss: 1.958, Avg. Returns: 65.528\n",
      "Step 396, Policy Loss: 1.690, Avg. Returns: 51.106\n",
      "Step 397, Policy Loss: 2.107, Avg. Returns: 56.875\n",
      "Step 398, Policy Loss: 2.160, Avg. Returns: 60.151\n",
      "Step 399, Policy Loss: 1.946, Avg. Returns: 62.446\n",
      "Step 400, Policy Loss: 1.862, Avg. Returns: 65.002\n",
      "Step 401, Policy Loss: 1.986, Avg. Returns: 62.201\n",
      "Step 402, Policy Loss: 1.827, Avg. Returns: 63.697\n",
      "Step 403, Policy Loss: 2.292, Avg. Returns: 69.105\n",
      "Step 404, Policy Loss: 2.015, Avg. Returns: 67.383\n",
      "Step 405, Policy Loss: 1.962, Avg. Returns: 58.828\n",
      "Step 406, Policy Loss: 1.928, Avg. Returns: 64.493\n",
      "Step 407, Policy Loss: 1.938, Avg. Returns: 67.172\n",
      "Step 408, Policy Loss: 2.164, Avg. Returns: 61.169\n",
      "Step 409, Policy Loss: 2.415, Avg. Returns: 73.706\n",
      "Step 410, Policy Loss: 1.756, Avg. Returns: 58.604\n",
      "Step 411, Policy Loss: 2.080, Avg. Returns: 61.575\n",
      "Step 412, Policy Loss: 2.124, Avg. Returns: 63.119\n",
      "Step 413, Policy Loss: 2.186, Avg. Returns: 69.258\n",
      "Step 414, Policy Loss: 2.369, Avg. Returns: 69.650\n",
      "Step 415, Policy Loss: 2.059, Avg. Returns: 60.428\n",
      "Step 416, Policy Loss: 2.027, Avg. Returns: 48.704\n",
      "Step 417, Policy Loss: 2.480, Avg. Returns: 64.593\n",
      "Step 418, Policy Loss: 2.196, Avg. Returns: 58.814\n",
      "Step 419, Policy Loss: 2.059, Avg. Returns: 51.426\n",
      "Step 420, Policy Loss: 2.303, Avg. Returns: 53.613\n",
      "Step 421, Policy Loss: 2.183, Avg. Returns: 48.264\n",
      "Step 422, Policy Loss: 2.464, Avg. Returns: 58.730\n",
      "Step 423, Policy Loss: 2.233, Avg. Returns: 53.750\n",
      "Step 424, Policy Loss: 2.124, Avg. Returns: 47.125\n",
      "Step 425, Policy Loss: 2.247, Avg. Returns: 47.168\n",
      "Step 426, Policy Loss: 2.055, Avg. Returns: 41.465\n",
      "Step 427, Policy Loss: 1.965, Avg. Returns: 42.083\n",
      "Step 428, Policy Loss: 2.155, Avg. Returns: 49.391\n",
      "Step 429, Policy Loss: 2.593, Avg. Returns: 59.312\n",
      "Step 430, Policy Loss: 2.337, Avg. Returns: 51.209\n",
      "Step 431, Policy Loss: 1.942, Avg. Returns: 44.443\n",
      "Step 432, Policy Loss: 2.258, Avg. Returns: 58.737\n",
      "Step 433, Policy Loss: 2.782, Avg. Returns: 63.052\n",
      "Step 434, Policy Loss: 2.256, Avg. Returns: 54.912\n",
      "Step 435, Policy Loss: 1.981, Avg. Returns: 55.190\n",
      "Step 436, Policy Loss: 2.225, Avg. Returns: 54.539\n",
      "Step 437, Policy Loss: 2.125, Avg. Returns: 48.003\n",
      "Step 438, Policy Loss: 2.148, Avg. Returns: 53.604\n",
      "Step 439, Policy Loss: 2.105, Avg. Returns: 63.483\n",
      "Step 440, Policy Loss: 2.278, Avg. Returns: 58.497\n",
      "Step 441, Policy Loss: 2.099, Avg. Returns: 64.031\n",
      "Step 442, Policy Loss: 2.046, Avg. Returns: 62.779\n",
      "Step 443, Policy Loss: 1.664, Avg. Returns: 53.782\n",
      "Step 444, Policy Loss: 2.342, Avg. Returns: 61.993\n",
      "Step 445, Policy Loss: 2.012, Avg. Returns: 70.421\n",
      "Step 446, Policy Loss: 2.097, Avg. Returns: 61.532\n",
      "Step 447, Policy Loss: 1.804, Avg. Returns: 65.756\n",
      "Step 448, Policy Loss: 1.854, Avg. Returns: 60.973\n",
      "Step 449, Policy Loss: 1.787, Avg. Returns: 57.290\n",
      "Step 450, Policy Loss: 2.016, Avg. Returns: 62.595\n",
      "Step 451, Policy Loss: 1.807, Avg. Returns: 68.630\n",
      "Step 452, Policy Loss: 1.862, Avg. Returns: 66.342\n",
      "Step 453, Policy Loss: 1.986, Avg. Returns: 63.231\n",
      "Step 454, Policy Loss: 1.746, Avg. Returns: 66.285\n",
      "Step 455, Policy Loss: 1.794, Avg. Returns: 60.200\n",
      "Step 456, Policy Loss: 1.565, Avg. Returns: 61.904\n",
      "Step 457, Policy Loss: 1.781, Avg. Returns: 66.937\n",
      "Step 458, Policy Loss: 1.627, Avg. Returns: 59.703\n",
      "Step 459, Policy Loss: 1.788, Avg. Returns: 70.140\n",
      "Step 460, Policy Loss: 1.628, Avg. Returns: 59.407\n",
      "Step 461, Policy Loss: 1.445, Avg. Returns: 64.487\n",
      "Step 462, Policy Loss: 1.685, Avg. Returns: 66.926\n",
      "Step 463, Policy Loss: 1.480, Avg. Returns: 69.899\n",
      "Step 464, Policy Loss: 1.393, Avg. Returns: 57.411\n",
      "Step 465, Policy Loss: 1.729, Avg. Returns: 64.032\n",
      "Step 466, Policy Loss: 1.747, Avg. Returns: 64.767\n",
      "Step 467, Policy Loss: 1.175, Avg. Returns: 63.482\n",
      "Step 468, Policy Loss: 1.445, Avg. Returns: 63.839\n",
      "Step 469, Policy Loss: 1.325, Avg. Returns: 62.884\n",
      "Step 470, Policy Loss: 1.403, Avg. Returns: 64.566\n",
      "Step 471, Policy Loss: 1.188, Avg. Returns: 61.071\n",
      "Step 472, Policy Loss: 1.391, Avg. Returns: 61.633\n",
      "Step 473, Policy Loss: 1.063, Avg. Returns: 54.346\n",
      "Step 474, Policy Loss: 1.084, Avg. Returns: 61.233\n",
      "Step 475, Policy Loss: 1.012, Avg. Returns: 55.405\n",
      "Step 476, Policy Loss: 1.089, Avg. Returns: 55.699\n",
      "Step 477, Policy Loss: 1.125, Avg. Returns: 53.241\n",
      "Step 478, Policy Loss: 1.237, Avg. Returns: 58.873\n",
      "Step 479, Policy Loss: 1.074, Avg. Returns: 58.420\n",
      "Step 480, Policy Loss: 1.083, Avg. Returns: 55.993\n",
      "Step 481, Policy Loss: 1.028, Avg. Returns: 51.686\n",
      "Step 482, Policy Loss: 0.988, Avg. Returns: 53.983\n",
      "Step 483, Policy Loss: 0.978, Avg. Returns: 52.895\n",
      "Step 484, Policy Loss: 0.713, Avg. Returns: 56.977\n",
      "Step 485, Policy Loss: 0.928, Avg. Returns: 58.186\n",
      "Step 486, Policy Loss: 1.011, Avg. Returns: 52.014\n",
      "Step 487, Policy Loss: 1.025, Avg. Returns: 53.311\n",
      "Step 488, Policy Loss: 0.674, Avg. Returns: 54.159\n",
      "Step 489, Policy Loss: 0.799, Avg. Returns: 54.192\n",
      "Step 490, Policy Loss: 0.817, Avg. Returns: 51.693\n",
      "Step 491, Policy Loss: 0.766, Avg. Returns: 55.518\n",
      "Step 492, Policy Loss: 1.149, Avg. Returns: 58.749\n",
      "Step 493, Policy Loss: 1.061, Avg. Returns: 58.506\n",
      "Step 494, Policy Loss: 0.851, Avg. Returns: 55.092\n",
      "Step 495, Policy Loss: 0.891, Avg. Returns: 53.842\n",
      "Step 496, Policy Loss: 0.858, Avg. Returns: 53.581\n",
      "Step 497, Policy Loss: 0.708, Avg. Returns: 52.877\n",
      "Step 498, Policy Loss: 0.811, Avg. Returns: 59.186\n",
      "Step 499, Policy Loss: 0.750, Avg. Returns: 57.225\n"
     ]
    }
   ],
   "source": [
    "TRAIN_EPOCHS = 500\n",
    "EPISODES_PER_BATCH = 10\n",
    "\n",
    "# Train\n",
    "while step < TRAIN_EPOCHS:\n",
    "    obs_batch:list[npt.NDArray[np.float32]] = []\n",
    "    act_batch:list[int] = []\n",
    "    rtg_batch:list[float] = []\n",
    "    \n",
    "    trajectory_returns = []\n",
    "\n",
    "    for _ in range(EPISODES_PER_BATCH):\n",
    "        # Collect trajectory\n",
    "        obs_traj, act_traj, rew_traj = collect_trajectory(env, lambda obs: nn_policy(policy, obs))\n",
    "        rtg_traj = rewards_to_go(rew_traj)\n",
    "\n",
    "        # Update batch\n",
    "        obs_batch.extend(obs_traj)\n",
    "        act_batch.extend(act_traj)\n",
    "        rtg_batch.extend(rtg_traj)\n",
    "\n",
    "        # Update trajectory returns\n",
    "        trajectory_returns.append(sum(rew_traj))\n",
    "\n",
    "    policy_loss = train_policygradient(\n",
    "        policy,\n",
    "        policy_optimizer,\n",
    "        obs_batch,\n",
    "        act_batch,\n",
    "        rtg_batch\n",
    "    )\n",
    "\n",
    "    print(f\"Step {step}, Policy Loss: {policy_loss:.3f}, Avg. Returns: {np.mean(trajectory_returns):.3f}\")\n",
    "    rewards.append(np.mean(trajectory_returns))\n",
    "    losses.append(policy_loss)\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how the policy drives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Known pipe types:\n",
      "  glxGraphicsPipe\n",
      "(1 aux display modules not yet loaded.)\n",
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      "INFO:/home/fidgetsinner/myworkspace/metadrive/metadrive/envs/base_env.py:Episode ended! Index: 0 Reason: out_of_road.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 48.55791074619068\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MetaDrive-validation-v0\", config={\"use_render\": True, \"horizon\": 500})\n",
    "obs, act, rew = collect_trajectory(env, lambda obs: nn_policy(policy, obs))\n",
    "env.close()\n",
    "\n",
    "print(\"Reward:\", sum(rew))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For us, we got returns of around 12-14 after training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metadrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
