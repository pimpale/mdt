{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient in MetaDrive\n",
    "\n",
    "The goals of this notebook are the following:\n",
    "* Provide a brief theoretical background on the\n",
    "    * discrete policy gradient\n",
    "    * continuous policy gradient\n",
    "* Introduce Metadrive, a RL environment to train self-driving automobiles.\n",
    "* Implement a simple policy gradient algorithm (REINFORCE) for the problem of self driving cars. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Policy Gradient?\n",
    "There are a plethora of RL algorithms out there. Why do we focus on the policy gradient?\n",
    "1. **Simplicity:** The policy gradient is one of the simplest RL algorithms out there. It's easy to understand, and it's easy to implement.\n",
    "2. **Direct Optimization:** The policy gradient directly optimizes the objective that we care about: the expected return. (Read on if you want to understand what this means.) Other algorithms, such as Q-learning, optimize a proxy objective, and then use that to optimize the expected return. This can lead to suboptimal performance.\n",
    "3. **Both Continuous and Discrete Actions:** The policy gradient can be used to optimize policies with both continuous and discrete action spaces. Other algorithms, such as Q-learning, can only be used to optimize policies with discrete action spaces. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "In order to keep the length of these tutorials short, and their scope focused, we assume you have some prerequisite knowledge:\n",
    "\n",
    "Here's a list of the topics you should be familiar with, alongside some sources that you can review for \n",
    "* Partial Derivatives and Gradients\n",
    "    * https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives\n",
    "    * https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/the-gradient\n",
    "* Probability Distributions\n",
    "    * https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/random-variables-discrete/v/discrete-and-continuous-random-variables\n",
    "    * https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/random-variables-continuous/v/random-variables\n",
    "* Backprop\n",
    "    * 3Blue1Brown's Backprop Video: https://www.youtube.com/watch?v=tIeHLnjs5U8\n",
    "    * Andrei Karpathy's Micrograd Video: https://www.youtube.com/watch?v=VMj-3S1tku0 \n",
    "* PyTorch\n",
    "    * https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "    * https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Background\n",
    "\n",
    "*Note: Part of these notes are adapted from the OpenAI Spinning Up [lecture notes](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html).*\n",
    "\n",
    "Before we start our derivation of the discrete policy gradient, we need to define our problem as well as some key terminology that we'll be using throughout the article. \n",
    "\n",
    "\n",
    "### Problem Definition\n",
    "Any problem that we solve using RL is defined as a repeated interaction between an agent and an environment. At each time step $t$, the agent receives an observation $o_t$ from the environment, and then selects an action $a_t$ to perform. After performing the action, the agent receives a reward $r_t$ from the environment, and the environment transitions to a new state $s_{t+1}$. The goal of the agent is to maximize the sum of rewards it receives over the course of the interaction.\n",
    "\n",
    "This is illustrated in the figure below:\n",
    "\n",
    "![rl_diagram](./agentenv_whitebg.png)\n",
    "\n",
    "*(Image Source: Barto & Sutton, 2018)*\n",
    "\n",
    "### Markov Decision Process\n",
    "Mathematically, we can define the problem as a Markov Decision Process (MDP). An MDP is a tuple $(S, A, P, R, \\gamma)$, where:\n",
    "* $S$ is the set of possible states of the environment.\n",
    "* $A$ is the set of possible actions of the agent.\n",
    "* $P$ is the transition probability matrix, where $P_a(s, s') = \\Pr(s_{t+1} = s' | s_t = s, a_t = a)$.\n",
    "    * It represents the probability that state $s$ transitions to state $s'$ when the agent performs action $a$. \n",
    "* $R$ is the reward function, where $R_a(s, s') = \\mathbb{E}[r_{t+1} | s_t = s, a_t = a, s_{t+1} = s']$.\n",
    "    * It represents the expected reward that the agent receives when it transitions from state $s$ to state $s'$ after performing action $a$.\n",
    "* $\\gamma$ is the discount factor, where $\\gamma \\in [0, 1]$.\n",
    "    * It represents the agent's preference for immediate rewards over future rewards.\n",
    "    * The reward recieved $n$ timesteps into the future is multiplied by $\\gamma^n$. Since $\\gamma < 1$, this reduces their value at an exponential rate, the farther away they are from the present."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a toy example to examine how we can break down problems into MDPs.\n",
    "\n",
    "#### Gridworld as an MDP\n",
    "\n",
    "**Problem Description**: The agent is placed in a $4 \\times 4$ gridworld, and it can move up, down, left, or right. The agent receives a reward of $+1$ for reaching the goal state, and a reward of $-1$ for falling into the pit. The agent receives a reward of $-0.1$ for every other state. The agent's goal is to reach the goal state as quickly as possible.\n",
    "\n",
    "Here's how we would define this problem as an MDP:\n",
    "* $S = \\{s_{1,1}, s_{1,2}, \\dots, s_{4,4}\\}$\n",
    "* $A = \\{up, down, left, right\\}$\n",
    "* $P_a(s, s') = \\begin{cases} 1 & \\text{if } s' = \\text{next\\_state}(s, a) \\\\ 0 & \\text{otherwise} \\end{cases}$\n",
    "    * Where $\\text{next\\_state}(s, a)$ is defined as: \n",
    "    \n",
    "        $\n",
    "            \\text{next\\_state}(s_{x,y}, a) =\n",
    "            \\begin{cases}\n",
    "                s_{x-1,y} & \\text{if } a = \\text{up} \\text{ and } x > 1 \\\\\n",
    "                s_{x+1,y} & \\text{if } a = \\text{down} \\text{ and } x < 4 \\\\\n",
    "                s_{x,y-1} & \\text{if } a = \\text{left} \\text{ and } y > 1 \\\\\n",
    "                s_{x,y+1} & \\text{if } a = \\text{right} \\text{ and } y < 4 \\\\\n",
    "                s_{x,y} & \\text{otherwise}\n",
    "            \\end{cases}\n",
    "        $\n",
    "\n",
    "* $R_a(s, s') = \\begin{cases} +1 & \\text{if } s' = \\text{goal} \\\\ -1 & \\text{if } s' = \\text{pit} \\\\ -0.1 & \\text{otherwise} \\end{cases}$\n",
    "* $\\gamma = 1$\n",
    "    * The agent does not have a preference for immediate rewards over future rewards."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policies, Trajectories, and Returns\n",
    "\n",
    "**Policy**: The agent is fully described by a policy. A policy is a function (usually denoted $\\pi$) that takes in a state $s$ and returns a probability distribution over actions $a$.\n",
    "* $\\pi(a | s) = \\Pr(a_t = a | s_t = s)$\n",
    "\n",
    "**Trajectory**: A trajectory, usually denoted $\\tau$ is a sequence of actions and states that the agent experiences over the course of an interaction with the environment.\n",
    "* $\\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \\dots, s_T, a_T, r_T)$\n",
    "\n",
    "**Return**: A return, usually denoted $R(\\tau)$ is the sum of rewards that the agent receives over the course of a trajectory. This is what we want to optimize.\n",
    "* $R(\\tau) = \\sum_{t=0}^T r_t$\n",
    "\n",
    "When we take into account the discount factor $\\gamma$, we get the discounted return:\n",
    "* $R_{\\gamma}(\\tau) = \\sum_{t=0}^T \\gamma^t r_t$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of the Discrete Policy Gradient\n",
    "\n",
    "With the above background in mind, we are now ready to directly tackle the derivation of the policy gradient.\n",
    "\n",
    "For this derivation, we're assuming that $\\gamma = 1$, so the return is undiscounted.\n",
    "\n",
    "#### The Policy\n",
    "Our policy is denoted $\\pi_{\\theta}$. The subscript $\\theta$ indicates that our policy is parametrized by a set of neural network parameters, denoted $\\theta$. We're trying to find a value for $\\theta$ that makes the policy get a high reward. \n",
    "\n",
    "#### Objective Function\n",
    "Let's start by writing down our **objective function**.\n",
    "This function, typically denoted $J$, describes how good a particular policy $\\pi_{\\theta}$ is.\n",
    "The objective function is what we want to maximize.\n",
    "\n",
    "For the policy gradient, $J(\\pi_{\\theta})$ is just the expected value of the return over the distribution of trajectories that would be visited by the policy. Mathematically:\n",
    "$$\n",
    "    J(\\pi_{\\theta}) = \\mathop{\\mathbb{E}}_{\\tau \\sim \\pi_{\\theta}}\\left[R(\\tau)\\right]\n",
    "$$\n",
    "\n",
    "Let's take a minute to expand this out so we understand it better.\n",
    "\n",
    "Let:\n",
    "$$\n",
    "    \\mathcal{T} = \\text{the set of all possible trajectories that could be produced by following $\\pi_{\\theta}$}\\\n",
    "$$\n",
    "Then:\n",
    "$$\n",
    "    J(\\pi_{\\theta}) = \\sum_{\\tau \\in \\mathcal{T}} R(\\tau) \\Pr(\\tau|\\pi_{\\theta})\n",
    "$$\n",
    "\n",
    "Remember that both the policy and environment can be stochastic, so even if the starting condition is the same, there may be many possible trajectories."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we know what the objective function is, how do we optimize $\\theta$ to maximize $J$? Since most problems have absurdly complex environments, there's no straightforward equation we can write and solve analytically. In the absence of an analytical solution, a good choice is gradient ascent. *(Note: since we want to maximize $J$, we're using gradient ascent. If we wanted to minimze loss, we would use gradient descent)*.\n",
    "\n",
    "If you're confused about how gradient ascent (or descent) works, check out the linked resources in [Prequisites](#prerequisites).\n",
    "\n",
    "When we're doing gradient ascent, our update rule for $\\theta$ is:\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_{k} + \\alpha \\nabla_{\\theta} J(\\pi_{\\theta_k})\n",
    "$$\n",
    "Where $\\theta_k$ current set of parameters, and $\\theta_{k+1}$ is the next set of parameters.\n",
    "\n",
    "The quantity $\\nabla_{\\theta} J(\\pi_{\\theta_k})$ is called the **policy gradient** and lends its name to this algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the Policy Gradient\n",
    "\n",
    "Expand definition of J:\n",
    "$$\n",
    "\\nabla_{\\theta} J(\\pi_{\\theta}) = \\nabla_{\\theta} \\mathop{\\mathbb{E}}_{\\tau \\sim \\pi_{\\theta}}\\left[R(\\tau)\\right]\n",
    "$$\n",
    "Expand definition of expectation:\n",
    "$$\n",
    "\\nabla_{\\theta} J(\\pi_{\\theta}) = \\nabla_{\\theta} \\sum_{\\tau \\in \\mathcal{T}} R(\\tau) \\Pr(\\tau|\\pi_{\\theta})\n",
    "$$\n",
    "Move gradient inside sum:\n",
    "$$\n",
    "\\nabla_{\\theta} J(\\pi_{\\theta}) = \\sum_{\\tau \\in \\mathcal{T}} R(\\tau) \\nabla_{\\theta} \\Pr(\\tau|\\pi_{\\theta})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detour: The Log Trick\n",
    "We're going to take a quick detour to talk about the log trick. This is a useful trick that we'll use to simplify our derivation.\n",
    "\n",
    "We start with the following basic identity:\n",
    "$$\n",
    "\\frac{d}{dx} \\ln f(x) = \\frac{1}{f(x)} \\left(\\frac{d}{dx} f(x)\\right)\n",
    "$$\n",
    "Rearrange the terms to isolate $\\frac{d}{dx} f(x)$:\n",
    "$$\n",
    "\\frac{d}{dx} f(x) = f(x) \\frac{d}{dx} \\ln f(x)\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to the Derivation\n",
    "Applying this identity to our policy gradient, we get:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} J(\\pi_{\\theta}) &= \\sum_{\\tau \\in \\mathcal{T}} R(\\tau) \\left(\\nabla_{\\theta} \\Pr(\\tau|\\pi_{\\theta})\\right)\\\\\n",
    "&= \\sum_{\\tau \\in \\mathcal{T}} R(\\tau) \\left(\\Pr(\\tau|\\pi_{\\theta}) \\nabla_{\\theta} \\ln \\Pr(\\tau|\\pi_{\\theta})\\right)\\\\\n",
    "&= \\sum_{\\tau \\in \\mathcal{T}} R(\\tau) \\nabla_{\\theta} \\ln \\Pr(\\tau|\\pi_{\\theta}) \\Pr(\\tau|\\pi_{\\theta}) \n",
    "\\end{align*}\n",
    "$$\n",
    "Apply definition of expectation:\n",
    "$$\n",
    "\\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathop{\\mathbb{E}}_{\\tau \\sim \\pi_{\\theta}}\\left[R(\\tau) \\nabla_{\\theta} \\ln \\Pr(\\tau|\\pi_{\\theta})\\right]\n",
    "$$\n",
    "\n",
    "So, what we've discovered is that the policy gradient is equal to the expected value of the reward of a given trajectory times gradient of the log-probs of that same trajectory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving the Log Probability of a Trajectory\n",
    "\n",
    "Let's focus on the term $\\nabla_{\\theta}\\ln Pr(\\tau|\\pi_{\\theta})$, from the previous equation. This is the gradient of the log probability of a trajectory. \n",
    "\n",
    "The probability of a given trajectory $\\tau$ given $\\pi_{\\theta}$ is:\n",
    "$$\n",
    "\\Pr(\\tau|\\pi_{\\theta}) = \\Pr(s_0) \\prod_{t=0}^{T-1} \\Pr(s_{t+1}|s_t, a_t) \\pi_{\\theta}(a_t|s_t)\n",
    "$$\n",
    "Where $T$ is the length of the trajectory.\n",
    "\n",
    "\n",
    "Substituting this in, we can expand this out as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} \\ln \\Pr(\\tau|\\pi_{\\theta}) &= \\nabla_{\\theta} \\ln \\left(\\Pr(s_0) \\prod_{t=0}^{T-1} \\Pr(s_{t+1}|s_t, a_t) \\pi_{\\theta}(a_t|s_t)\\right)\\\\\n",
    "&= \\nabla_{\\theta} \\left(\\ln \\Pr(s_0) + \\sum_{t=0}^{T-1} \\ln \\Pr(s_{t+1}|s_t, a_t) + \\ln \\pi_{\\theta}(a_t|s_t)\\right)\\\\\n",
    "&= \\nabla_{\\theta} \\sum_{t=0}^{T-1} \\ln \\pi_{\\theta}(a_t|s_t)\\\\\n",
    "&= \\sum_{t=0}^{T-1} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t|s_t)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finishing off the Policy Gradient\n",
    "\n",
    "Let's substitute the above result back into our equation for the policy gradient.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} J(\\pi_{\\theta}) &= \\mathop{\\mathbb{E}}_{\\tau \\sim \\pi_{\\theta}}\\left[R(\\tau) \\nabla_{\\theta} \\ln \\Pr(\\tau|\\pi_{\\theta})\\right]\\\\\n",
    "&= \\mathop{\\mathbb{E}}_{\\tau \\sim \\pi_{\\theta}}\\left[R(\\tau)\\sum_{t=0}^{T-1} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t|s_t)\\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "Move the $R(\\tau)$ inside the sum.\n",
    "$$\n",
    "\\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathop{\\mathbb{E}}_{\\tau \\sim \\pi_{\\theta}}\\left[\\sum_{t=0}^{T-1} R(\\tau) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t|s_t)\\right]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Result\n",
    "\n",
    "And that's it! We're done! Let's now take a minute to analyze our result.\n",
    "\n",
    "The policy gradient takes the form of an expectation over a distribution.\n",
    "This means that we can estimate it using Monte-Carlo sampling.\n",
    "\n",
    "To do this, we'll collect dozens of trajectories, and for each one calculate the value inside of the expecation, and then average them together to get an estimate of the gradient. The more samples (trajectories in this case) we collect, the closer our gradient will be to the true value.\n",
    "\n",
    "**But what does the expression mean though?**\n",
    "\n",
    "There is an intuition behind each of the operations in the equation. Let's go through each of the operations in the equation from the inside-out.\n",
    "1. $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t|s_t)$\n",
    "    * This term \"selects\" the part of the network that represents a particular action we took in a particular state.\n",
    "        * Remember that the gradient of a scalar (in this case $\\ln \\pi_{\\theta}(a_t|s_t)$) with respect to $\\theta$ is a vector with the same dimensions as $\\theta$.\n",
    "        * You can conceptually think of this part of the network returning a vector with positive values for neurons that positively influenced the network's output $\\pi_{\\theta}(a_t|s_t)$, and negative values for neurons that negatively influenced the network's output.\n",
    "        * Remember that changing any parameter in $\\theta$ affects $\\pi_{\\theta}(a_t|s_t)$ for many different states. This is good, as it allows the network to generalize to states it hasn't seen before.\n",
    "2. $R(\\tau)\\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t|s_t)$\n",
    "    * $R(\\tau)$ is a scalar that represents how much we want to want to encourage or discourage this type of action. \n",
    "        * In the future we'll look at different things we could put here instead of $R(\\tau)$ that improve performance. However, all of these replacements must have the property that good actions should have positive values, and bad actions have negative (or less positive) values.\n",
    "    * We multiply $R(\\tau)$ by the gradient of the log probability of the action we took in the state we were in. This produces a vector that represents an update in a direction that would have increased the network's reward when making the choice $\\pi_{\\theta}(a_t|s_t)$.\n",
    "3. $\\sum_{t=0}^{T-1} R(\\tau)\\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t|s_t)$\n",
    "    * We sum the update vectors from each choice over all the choices that were made in this rollout. This produces a single update vector that represents the sum of all the updates that would have increased the network's reward when making the choices $\\pi_{\\theta}(a_t|s_t)$.\n",
    "    * If the model made multiple good choices, then we can reward them all. If the model made multiple bad choices, then we can punish them all.\n",
    "4. $\\mathop{\\mathbb{E}}_{\\tau \\sim \\pi_{\\theta}}\\left[\\sum_{t=0}^{T-1} R(\\tau)\\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t|s_t)\\right]$\n",
    "    * We take the expectation of the update vector over all the trajectories we collected. In practice, we use Monte-Carlo sampling to estimate this expectation.\n",
    "    * We do this in order to reduce the effect of random chance on our gradient.\n",
    "    * Since the environment is stochastic, a bad choice may have a good return for a particular trajectory. This is bad, because it will encourage the network to make that bad choice again. However, if we collect enough trajectories, then the network will be able to clearly see that making that choice will result in a bad return on average, and the network will be discouraged from making that choice again."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward-to-Go\n",
    "\n",
    "In the above derivation, we used the total return $R(\\tau)$ to decide which actions to reinforce.\n",
    "\n",
    "Recall that $R(\\tau)$ is the sum of all the rewards we received during the trajectory:\n",
    "$$\n",
    "R(\\tau) = \\sum_{t=0}^{T-1} r_t\n",
    "$$\n",
    "\n",
    "Actions that resulted in a higher total return were encouraged, and actions that resulted in a lower total return were discouraged. This is good, but it ignores that an action can only influence the future, not the past. To put it another way, we only care about reinforcing actions that have good consequences. If we recieved a high reward *before* we took a particular action $a_t$, then we shouldn't reinforce action $a_t$ based on that.\n",
    "\n",
    "To fix this, we can use the **reward-to-go** $\\hat{R}_t(\\tau)$ instead of the total return $R(\\tau)$. The reward-to-go is the sum of all the rewards we received after taking action $a_t$. Mathematically:\n",
    "$$\n",
    "\\hat{R}_t(\\tau) = \\sum_{t'=t}^{T-1} r_{t'}\n",
    "$$\n",
    "\n",
    "Our overall policy gradient then becomes:\n",
    "$$\n",
    "\\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathop{\\mathbb{E}}_{\\tau \\sim \\pi_{\\theta}}\\left[\\sum_{t=0}^{T-1} \\hat{R}_t(\\tau) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t|s_t)\\right]\n",
    "$$\n",
    "\n",
    "Empirically, using the reward-to-go instead of the total return improves performance. This is because the rewards accumulated in the past are not relevant to the action we are currently taking, and only serve to add noise to the gradient."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing Back Gamma\n",
    "\n",
    "Thus far, we have been ignoring gamma in order to simplify the math. Recall that gamma represents our discount factor, and is used to reduce the effect of rewards that are far in the future. If we recieve a reward $r$ $n$ timesteps into the future and our discount factor is $\\gamma$, then we only count that reward as $\\gamma^n r$. \n",
    "\n",
    "We can adjust the reward-to-go to account for gamma:\n",
    "$$\n",
    "\\hat{R}_t(\\tau) = \\sum_{t'=t}^{T-1} \\gamma^{t'-t} r_{t'}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between Discrete and Continuous Problems\n",
    "\n",
    "In **discrete** action spaces, the agent chooses between a finite number of actions. Examples of discrete action spaces are:\n",
    "1. Gridworld.\n",
    "2. Choosing a move when playing chess.\n",
    "\n",
    "In **continuous** action spaces, the agent must generate a value for each dimension of the action space. Examples of continuous action spaces are:\n",
    "1. Controlling the angle and thrust of a rocket.\n",
    "2. Choosing how much torque to apply to the joint of a robot. \n",
    "\n",
    "In some cases, the action space may be a mix of discrete and continuous actions. For example, in a self driving car, the agent may choose between discrete actions such as \"activate left blinker\", \"activate right blinker\", and \"do not activate blinker\", and continuous actions such as how much to turn the steering wheel.\n",
    "\n",
    "Thus far, the derivation of the policy gradient has been identical for both discrete and continuous action spaces. However, in the following sections, we will see that there are some differences in how we implement the policy gradient for discrete and continuous action spaces.\n",
    "\n",
    "This particular notebook focuses on the discrete case, and the next notebook will focus on the continuous case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadrive\n",
    "\n",
    "We now introduce the environment we will be using for this notebook: [Metadrive](https://github.com/metadriverse/metadrive)\n",
    "\n",
    "Check out the notebook [here](../quickstart.ipynb) for a quick introduction to Metadrive as well as how to install it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out the environment by creating an instance of it and taking a random action at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Known pipe types:\n",
      "  glxGraphicsPipe\n",
      "(1 aux display modules not yet loaded.)\n",
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.31221473623458e-05\n",
      "0.009233961947637252\n",
      "0.029430664503088177\n",
      "0.0385080765555831\n",
      "0.051269874340499226\n",
      "0.024825500350000494\n",
      "-0.004269349232754249\n",
      "-0.012220657157477671\n",
      "-0.011318983514367316\n",
      "0.010826522401143868\n",
      "0.019600871726443807\n",
      "0.00518795331222229\n",
      "0.0002647879602980098\n",
      "-0.00020965787401731797\n",
      "0.0027548640214803766\n",
      "0.0036451430530871957\n",
      "0.00203804888209766\n",
      "4.7831459837470954e-05\n",
      "-0.0008139261675983708\n",
      "0.0002649450169180858\n",
      "-0.004138602420487799\n",
      "-0.003833691689167148\n",
      "0.005658243306209003\n",
      "0.0019439392483036298\n",
      "0.0017501587413670697\n",
      "0.0013271993844638941\n",
      "-0.0012417981516064886\n",
      "0.00047796214394067146\n",
      "0.005576002975745207\n",
      "0.011957905409772306\n",
      "0.019367010686041318\n",
      "0.03269041932155331\n",
      "0.02264725586738955\n",
      "0.010750797150814074\n",
      "0.023785212217481203\n",
      "0.019831233304526522\n",
      "0.015112038625447117\n",
      "0.017808277012386936\n",
      "0.017960457098097456\n",
      "0.020937374686892773\n",
      "-0.0005746566562039816\n",
      "-0.001424047920491952\n",
      "0.0013622548321156132\n",
      "-0.0023082070131272346\n",
      "-0.0007034053034793181\n",
      "0.0011865168268845503\n",
      "0.0005010090409301674\n",
      "0.012206810812975241\n",
      "0.03652245909205208\n",
      "0.04765253144879414\n",
      "0.029664489278078188\n",
      "0.004300617162298999\n",
      "0.0014666318467402204\n",
      "0.012800981385227636\n",
      "0.010123072268522463\n",
      "-0.0024230380304729017\n",
      "0.00037724342136907904\n",
      "0.0013072377840583826\n",
      "-0.00028633926232021057\n",
      "0.0012443151921490746\n",
      "0.01162886324159298\n",
      "0.0022833634694858556\n",
      "4.366956398433447e-05\n",
      "-0.0012910242703932317\n",
      "0.007105826768465976\n",
      "0.024658240791679054\n",
      "0.014571685286777061\n",
      "0.0011403935210473164\n",
      "0.0008725730049453744\n",
      "0.005903569301639269\n",
      "0.019587070474912337\n",
      "0.014555904699544138\n",
      "0.010121196474990376\n",
      "0.011776427593165453\n",
      "0.0004850043527345383\n",
      "-0.0008056387412793711\n",
      "0.00026206555243062926\n",
      "-0.00020768700831920163\n",
      "0.0018697230414087109\n",
      "0.0007080512889384501\n",
      "-0.009423843900972438\n",
      "-0.00480935447272227\n",
      "0.0035404504915897118\n",
      "0.0173691706042679\n",
      "0.021871501246500943\n",
      "0.017955502764034743\n",
      "0.0009972584272051207\n",
      "-0.008338925047476915\n",
      "-0.009207792559852287\n",
      "0.005925174464493323\n",
      "0.004917707919282458\n",
      "0.0005699507039107237\n",
      "-0.0006289486554585232\n",
      "-0.004083058972635726\n",
      "3.389601245506628e-05\n",
      "0.005895378156669079\n",
      "0.01771905375347952\n",
      "0.015802050244844153\n",
      "0.004137103417070065\n",
      "0.0004935672780919453\n"
     ]
    }
   ],
   "source": [
    "# We need to import metadrive to register the environments\n",
    "import metadrive\n",
    "import gymnasium as gym \n",
    "\n",
    "env = gym.make(\"MetaDrive-validation-v0\", config={\"use_render\": True})\n",
    "env.reset()\n",
    "for i in range(100):\n",
    "    obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadrive uses the [Farama Gymnasium](https://gymnasium.farama.org/), which has a standard API for interacting with environments. There are a couple of functions and properties that are good to know about:\n",
    "1. `reset()`: Resets the environment to its initial state and returns the initial observation.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#gymnasium.Env.reset\n",
    "2. `step(action)`: Takes an action and returns the next observation, the reward for taking the action, whether the episode is terminated, whether the episode is truncated (ran out of time), and any additional information.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n",
    "3. `close()`: Closes the environment.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#g1ymnasium.Env.close\n",
    "4. `action_space`: The action space of the environment, which tells us the shape and bounds of the action space.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#gymnasium.Env.action_space\n",
    "5. `observation_space`: The observation space of the environment, which tells us the shape and bounds of the observation space.\n",
    "    * Documentation: https://gymnasium.farama.org/api/env/#gymnasium.Env.observation_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at what our observation and action spaces are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Box(-1.0, 1.0, (2,), float32)\n",
      "Observation Space: Box(-0.0, 1.0, (259,), float32)\n"
     ]
    }
   ],
   "source": [
    "import metadrive\n",
    "import gymnasium as gym \n",
    "\n",
    "env = gym.make(\"MetaDrive-validation-v0\", config={\"use_render\": False})\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Action Space:\", env.action_space)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the observation space, we see that it is a `Box` observation space, with a low of 0 and a high of 1. It has a shape of (259,). This means that our observation is a vector of 259 numbers, each of which is between 0 and 1.\n",
    "\n",
    "If we print out the action space, we see that it is a `Box` action space, with a low of -1 and a high of 1. It has a shape of (2,)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metadrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
