{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient in MetaDrive\n",
    "\n",
    "The goals of this notebook are the following:\n",
    "* Provide a brief theoretical background on the\n",
    "    * discrete policy gradient\n",
    "    * continuous policy gradient\n",
    "* Introduce Metadrive, a RL environment to train self-driving automobiles.\n",
    "* Implement a simple policy gradient algorithm (REINFORCE) for the problem of self driving cars. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Policy Gradient?\n",
    "There are a plethora of RL algorithms out there. Why do we focus on the policy gradient?\n",
    "1. **Simplicity:** The policy gradient is one of the simplest RL algorithms out there. It's easy to understand, and it's easy to implement.\n",
    "2. **Direct Optimization:** The policy gradient directly optimizes the objective that we care about: the expected return. (Read on if you want to understand what this means.) Other algorithms, such as Q-learning, optimize a proxy objective, and then use that to optimize the expected return. This can lead to suboptimal performance.\n",
    "3. **Both Continuous and Discrete Actions:** The policy gradient can be used to optimize policies with both continuous and discrete action spaces. Other algorithms, such as Q-learning, can only be used to optimize policies with discrete action spaces. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "In order to keep the length of these tutorials short, and their scope focused, we assume you have some prerequisite knowledge:\n",
    "\n",
    "Here's a list of the topics you should be familiar with, alongside some sources that you can review for \n",
    "* Partial Derivatives and Gradients\n",
    "    * https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives\n",
    "    * https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/the-gradient\n",
    "* Probability Distributions\n",
    "    * https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/random-variables-discrete/v/discrete-and-continuous-random-variables\n",
    "    * https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/random-variables-continuous/v/random-variables\n",
    "* Backprop\n",
    "    * 3Blue1Brown's Backprop Video: https://www.youtube.com/watch?v=tIeHLnjs5U8\n",
    "    * Andrei Karpathy's Micrograd Video: https://www.youtube.com/watch?v=VMj-3S1tku0 \n",
    "* PyTorch\n",
    "    * https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "    * https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Background\n",
    "\n",
    "*Note: Part of these notes are adapted from the OpenAI Spinning Up [lecture notes](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html).*\n",
    "\n",
    "Before we start our derivation of the discrete policy gradient, we need to define our problem as well as some key terminology that we'll be using throughout the article. \n",
    "\n",
    "\n",
    "### Problem Definition\n",
    "Any problem that we solve using RL is defined as a repeated interaction between an agent and an environment. At each time step $t$, the agent receives an observation $o_t$ from the environment, and then selects an action $a_t$ to perform. After performing the action, the agent receives a reward $r_t$ from the environment, and the environment transitions to a new state $s_{t+1}$. The goal of the agent is to maximize the sum of rewards it receives over the course of the interaction.\n",
    "\n",
    "This is illustrated in the figure below:\n",
    "\n",
    "![rl_diagram](./agentenv_whitebg.png)\n",
    "\n",
    "*(Image Source: Barto & Sutton, 2018)*\n",
    "\n",
    "### Markov Decision Process\n",
    "Mathematically, we can define the problem as a Markov Decision Process (MDP). An MDP is a tuple $(S, A, P, R, \\gamma)$, where:\n",
    "* $S$ is the set of possible states of the environment.\n",
    "* $A$ is the set of possible actions of the agent.\n",
    "* $P$ is the transition probability matrix, where $P_a(s, s') = \\Pr(s_{t+1} = s' | s_t = s, a_t = a)$.\n",
    "    * It represents the probability that state $s$ transitions to state $s'$ when the agent performs action $a$. \n",
    "* $R$ is the reward function, where $R_a(s, s') = \\mathbb{E}[r_{t+1} | s_t = s, a_t = a, s_{t+1} = s']$.\n",
    "    * It represents the expected reward that the agent receives when it transitions from state $s$ to state $s'$ after performing action $a$.\n",
    "* $\\gamma$ is the discount factor, where $\\gamma \\in [0, 1]$.\n",
    "    * It represents the agent's preference for immediate rewards over future rewards.\n",
    "    * The reward recieved $n$ timesteps into the future is multiplied by $\\gamma^n$. Since $\\gamma < 1$, this reduces their value at an exponential rate, the farther away they are from the present."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a toy example to examine how we can break down problems into MDPs.\n",
    "\n",
    "#### Gridworld as an MDP\n",
    "\n",
    "**Problem Description**: The agent is placed in a $4 \\times 4$ gridworld, and it can move up, down, left, or right. The agent receives a reward of $+1$ for reaching the goal state, and a reward of $-1$ for falling into the pit. The agent receives a reward of $-0.1$ for every other state. The agent's goal is to reach the goal state as quickly as possible.\n",
    "\n",
    "Here's how we would define this problem as an MDP:\n",
    "* $S = \\{s_{1,1}, s_{1,2}, \\dots, s_{4,4}\\}$\n",
    "* $A = \\{up, down, left, right\\}$\n",
    "* $P_a(s, s') = \\begin{cases} 1 & \\text{if } s' = \\text{next\\_state}(s, a) \\\\ 0 & \\text{otherwise} \\end{cases}$\n",
    "    * Where $\\text{next\\_state}(s, a)$ is defined as: \n",
    "    \n",
    "        $\n",
    "            \\text{next\\_state}(s_{x,y}, a) =\n",
    "            \\begin{cases}\n",
    "                s_{x-1,y} & \\text{if } a = \\text{up} \\text{ and } x > 1 \\\\\n",
    "                s_{x+1,y} & \\text{if } a = \\text{down} \\text{ and } x < 4 \\\\\n",
    "                s_{x,y-1} & \\text{if } a = \\text{left} \\text{ and } y > 1 \\\\\n",
    "                s_{x,y+1} & \\text{if } a = \\text{right} \\text{ and } y < 4 \\\\\n",
    "                s_{x,y} & \\text{otherwise}\n",
    "            \\end{cases}\n",
    "        $\n",
    "\n",
    "* $R_a(s, s') = \\begin{cases} +1 & \\text{if } s' = \\text{goal} \\\\ -1 & \\text{if } s' = \\text{pit} \\\\ -0.1 & \\text{otherwise} \\end{cases}$\n",
    "* $\\gamma = 1$\n",
    "    * The agent does not have a preference for immediate rewards over future rewards."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policies, Trajectories, and Returns\n",
    "\n",
    "**Policy**: The agent is fully described by a policy. A policy is a function (usually denoted $\\pi$) that takes in a state $s$ and returns a probability distribution over actions $a$.\n",
    "* $\\pi(a | s) = \\Pr(a_t = a | s_t = s)$\n",
    "\n",
    "**Trajectory**: A trajectory, usually denoted $\\tau$ is a sequence of actions and states that the agent experiences over the course of an interaction with the environment.\n",
    "* $\\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \\dots, s_T, a_T, r_T)$\n",
    "\n",
    "**Return**: A return, usually denoted $R(\\tau)$ is the sum of rewards that the agent receives over the course of a trajectory. This is what we want to optimize.\n",
    "* $R(\\tau) = \\sum_{t=0}^T r_t$\n",
    "\n",
    "When we take into account the discount factor $\\gamma$, we get the discounted return:\n",
    "* $R_{\\gamma}(\\tau) = \\sum_{t=0}^T \\gamma^t r_t$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of the Discrete Policy Gradient\n",
    "\n",
    "**Note**: There is a difference between the discrete and continuous versions of the policy gradient. In this notebook, we focus only on the discrete case. We pick up with the continuous case in the second notebook.\n",
    "\n",
    "With the above background in mind, we are now ready to directly tackle the derivation of the policy gradient.\n",
    "\n",
    "Let's start by writing down our goal: a high expected value for the return.\n",
    "$$\n",
    "    J(\\pi_{\\theta}) = \\mathop{\\mathbb{E}}_{\\tau \\sim \\pi_{\\theta}}\\left[R(\\tau)\\right]\n",
    "$$\n",
    " In machine learning parlance, the function that describes our goal is called an **objective function**, and is typically denoted $J$. Since we're going to be using neural networks to implement our policy, our \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
