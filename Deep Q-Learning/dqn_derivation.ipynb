{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning\n",
    "\n",
    "#### This tutorial assumes that you have completed reading the policy gradient notebook and have an understanding of Markov Decision Processes and policy gradient models. If not, please revisit them [here](../PolicyGradient/policygradient_discrete.ipynb) and [here](../PolicyGradient/policygradient_continuous.ipynb).\n",
    "\n",
    "## Contents\n",
    "1. What is Deep-Q learning?\n",
    "2. Exploring its mathematical foundations.\n",
    "3. Why use it?\n",
    "4. How can we apply it to the Metadrive environment?\n",
    "\n",
    "In the last notebook, you learned about policy gradients - a way to create powerful learners that update on the gradient of the objective function. \n",
    "\n",
    "As a refresher, here is the objective function again:\n",
    "\n",
    "$J(\\pi_{\\theta}) = \\sum_{\\tau \\in \\mathcal{T}} R(\\tau) \\Pr(\\tau|\\pi_{\\theta})$,\n",
    "\n",
    "where $\\sum_{\\tau \\in \\mathcal{T}}$ represents trajectories in the set of possible trajectories and $\\Pr(\\tau|\\pi_{\\theta})$ represents the probability of the trajectory being taken under the current policy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V (value) and Q (quality)\n",
    "As we dive into our exploration of Q-networks, it's useful to take a look at a key idea that will come up often later on: we can denote the *value* of a state (how good a position is) by denoting **value** and **quality** functions. \n",
    "\n",
    "More specifically, the **value** function represents the value of a **state** while the **quality** function represents the quality of a **state-action pair**. OpenAI's [Spinning Up on Deep RL](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#trajectories) has a great article on this, which we'll reiterate below:\n",
    "\n",
    "> There are four main functions of note:\n",
    ">\n",
    ">The On-Policy Value Function, $V^{\\pi}(s)$, which gives the expected return if you start in state s and always act according to policy $\\pi$:\n",
    ">\n",
    ">$$V^{\\pi}(s) = \\underset{\\tau \\sim \\pi}{\\operatorname{E}}[{R(\\tau)\\left| s_0 = s\\right.}]$$\n",
    ">\n",
    ">The On-Policy Action-Value Function, $Q^{\\pi}(s,a)$, which gives the expected return if you start in state s, take an arbitrary action a (which may not have come from the policy), and then forever after act according to policy $\\pi$:\n",
    ">\n",
    ">$$Q^{\\pi}(s,a) = \\underset{\\tau \\sim \\pi}{\\operatorname{E}}[{R(\\tau)\\left| s_0 = s, a_0 = a\\right.}]$$\n",
    ">\n",
    ">The Optimal Value Function, $V^*(s)$, which gives the expected return if you start in state s and always act according to the optimal policy in the environment:\n",
    ">\n",
    ">$$V^*(s) = \\max_{\\pi} \\underset{\\tau \\sim \\pi}{\\operatorname{E}}[{R(\\tau)\\left| s_0 = s\\right.}]$$\n",
    ">\n",
    ">The Optimal Action-Value Function, $Q^*(s,a)$, which gives the expected return if you start in state s, take an arbitrary action a, and then forever after act according to the optimal policy in the environment:\n",
    ">\n",
    ">$$Q^*(s,a) = \\max_{\\pi} \\underset{\\tau \\sim \\pi}{\\operatorname{E}}[{R(\\tau)\\left| s_0 = s, a_0 = a\\right.}]$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQL Paradigm\n",
    "\n",
    "A policy function explicitly determines the action to take at each timestep by returning a distribution of likelihoods, where $\\pi(s, a)$ is the likelihood of taking action $a$ at state $s$ given policy $\\pi$. Q-learning ignores this step by instead predicting the *quality* of an action-state pair, where the quality is equal to the sum of the current reward and the time-discounted future reward.\n",
    "\n",
    "$$Q^\\pi(s_t, a_t) = E_\\pi[\\sum_{k=0}^\\infty \\gamma^k * r_{t+k+1} | s = s_t, a = a_t]$$\n",
    "\n",
    "Don't worry if the equation above sounds complex, we'll break it down step by step. As in the policy gradient formulation, we're looking to maximize our total time-discounted (TD) return.\n",
    "\n",
    "We have a quality function $Q^\\pi(s_t, a_t)$ that estimates the value of a state-action pair given future actions that the current policy tends to take. As an aside, the $\\pi$ in $Q^\\pi(s_t, a_t)$ indicates that we're trying to represent the quality of a state-action pair under our current policy $\\pi$. This is an important distinction to note, as $Q^\\pi(s_t, a_t)$ changes as $\\pi$ improves; in chess, a mate-in-five position that's down on material would be of great value to a grandmaster but useless to a novice.\n",
    "\n",
    "We can explicitly break it down as the expected reward resulting from the state-action pair ($E_\\pi[r_{t+1}]$) plus the time-discounted rewards from the future: \n",
    "\n",
    "$Q^\\pi(s_t, a_t)$ = $E_\\pi[r_{t+1}]$ + $E_\\pi[\\gamma * r_{t+2}]$ + $E_\\pi[\\gamma^2 * r_{t+3}]$ + $E_\\pi[\\gamma^3 * r_{t+4}]...$\n",
    "\n",
    "Which we denote in summation form as the central equation above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Breakdown\n",
    "The *Bellman Equation* is a recursive rewriting of the Q-value equation, and the formula that we'll be using to update our Q-model:\n",
    "$$Q^\\pi(s_t, a_t) = E_\\pi[r_{t+1}] + \\gamma * Q^\\pi(s_{t+1}, a_{t+1})$$\n",
    "\n",
    "This equation is pretty intuitive, but we can more rigorously derive the above formula from our definition of $Q^\\pi(s_t, a_t)$. If you enjoy math, read on:\n",
    "\n",
    "We establish our original Q-function by truncating the reward function to begin at timestep *t*. Intuitively, think of the Q-value (quality) of a state-action pair at timestep t as the expected return *if the environment had begun with the state we're currently in*. We can disregard the past as the environment has the [Markov property](https://en.wikipedia.org/wiki/Markov_property), which states that future states only depend on the present state. In other words, because the environment is a Markov process, the past does not matter - only the present does. Here's the time-delayed reward function:\n",
    "$$R_{\\gamma}(\\tau) = \\sum_{t=0}^T \\gamma^t r_t$$\n",
    "\n",
    "We derive the Q-function by denoting the expected value of time-delayed future rewards:\n",
    "\n",
    "$$Q^\\pi(s_t, a_t) = E_\\pi[\\sum_{k=0}^\\infty \\gamma^k * r_{t+k+1} | s = s_t, a = a_t]$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Property Derivation\n",
    "$$Q^\\pi(s_t, a_t) = E_\\pi[\\sum_{k=0}^\\infty \\gamma^k * r_{t+k+1} | s = s_t, a = a_t]$$\n",
    "$$ = E_\\pi[r_{t+1} + \\gamma * \\sum_{k=0}^\\infty \\gamma^k * r_{t+k+2} | s = s_{t}, a = a_{t}]$$\n",
    "As expectation is distributive over addition:\n",
    "$$ = E_\\pi[r_{t+1}] + E_\\pi[\\gamma * \\sum_{k=0}^\\infty \\gamma^k * r_{t+k+2} | s = s_{t+1}, a = a_{t+1}]$$\n",
    "\n",
    "Using the definition of expectation:\n",
    "$$\n",
    "= \\left( \\sum_{a \\in A} Pr_\\pi(a | s_t) * \\sum_{s' \\in S} Pr(s'|s_t, a)\\right) * r(s') + E_\\pi[\\sum_{k=0}^\\infty \\gamma^k * r_{t+k+2} | s = s_{t+1}, a = a_{t+1}]\n",
    "$$\n",
    "\n",
    "As review, $Pr_\\pi(a | s_t)$ is the probability of our current policy taking action $a$ in state $s_t$. As we don't explicitly predict the likelihood of the action being taken, the value of $Pr_\\pi(a | s_t)$ is unknown - which is why we're updating $Q^\\pi(s, a)$ via monte-carlo sampling of the environment.\n",
    "\n",
    "We add the term $Pr(s'|s_t, a)$ to allow for stochastic environments - where taking action A at state S can transition to many possible successor states $s' \\in S$.\n",
    "\n",
    "$$\n",
    "\\left( \\sum_{a \\in A} Pr_\\pi(a | s_t) * \\sum_{s' \\in S} Pr(s'|s_t, a)\\right) * r(s') = E[r_{t+1}]\n",
    "$$\n",
    "$$\n",
    "\\gamma E_\\pi[\\sum_{k=0}^\\infty \\gamma^k * r_{t+k+2} | s = s_{t+1}, a = a_{t+1}] = \\gamma * Q^\\pi(s_{t+1}, a_{t+1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q^\\pi(s_t, a_t) = E[r_{t+1}] + \\gamma * Q^\\pi(s_{t+1}, a_{t+1})\n",
    "$$\n",
    "\n",
    "Which is the same equation we started with."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating values\n",
    "Let's say our agent is dropped in an environment and we're looking to learn a robust and accurate Q-function. \n",
    "\n",
    "More specifically, we're looking to find $Q^{\\pi^*}(s_t, a_t)$, which is the *optimal quality function* that denotes the quality of each state-action pair under the optimal policy $\\pi^*(a|s)$. We can update our Q-function by doing Monte-Carlo sampling from our environment. Under our policy (let's not worry about how to decide which actions to take just yet), we gather trajectories from the environment and update our Q-value function as so:\n",
    "\n",
    "If the current step isn't the last step:\n",
    "$$Q^{\\pi}(s_t, a_t) \\leftarrow \\left(r_{t+1} + \\underset{a \\in A}{\\operatorname{argmax}} Q^{\\pi}(s_{t+1}, a) \\right)$$\n",
    "If the current step is the last step:\n",
    "$$Q^{\\pi}(s_t, a_t) \\leftarrow r_{t+1}$$\n",
    "\n",
    "As we gather more samples, we can optimize Q by using mean squared error or a similar loss metric to minimize the distance between Q and its target."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A decision policy\n",
    "Given the quality function $Q^\\pi(s_t, a_t)$, there are a variety of strategies we can use to select our actions. One is always taking the action that we believe has the highest quality:\n",
    "$$\\pi(s_t, a_t) = \\underset{a \\in A}{\\operatorname{argmax}} \\space Q^\\pi(s_t, a)$$ \n",
    "\n",
    "Would this be effective? Take a second to think before looking at the next block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(8, 16, 'end')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "red, green, light_green, black = np.zeros((4, 32, 32, 3))\n",
    "red += np.array([1, 0.5, 0.5])\n",
    "green += np.array([0, 0.75, 0])\n",
    "light_green += np.array([0.5,1,0.5])\n",
    "plt.imshow(np.concatenate([black, light_green, black, red, green, black], axis=1))\n",
    "plt.text(42, 16, '+2', color='white', fontsize=14)\n",
    "plt.text(108, 16, '-2', color='white', fontsize=14)\n",
    "plt.text(134, 16, '+10', color='white', fontsize=14)\n",
    "plt.text(72, 16, 'start', color='white', fontsize=14)\n",
    "plt.text(168, 16, 'end', color='white', fontsize=14)\n",
    "plt.text(8, 16, 'end', color='white', fontsize=14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we gather any samples about the environment, the model is equally likely to explore either the +2 square and the -2 square. Let's imagine it explores the +2 square before ending the episode; after updating at the end of the episode, the model learns to always seek out the +2 square, never being able to reach the +10 square (with each update afterward only confirming its bias). Clearly, we need to add a term for exploration:\n",
    "\n",
    "### Epsilon-Greedy\n",
    "$$\n",
    "\\pi(s_t, a_t) = \n",
    "\\begin{array}{lr}\n",
    "        \\text{if rand()} \\geq \\epsilon& \\underset{a \\in A}{\\operatorname{argmax}} \\space Q^\\pi(s_t, a),\\\\\n",
    "        \\text{else} & {a \\in_R A} \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We set $\\epsilon$ to a high value (~0.3) at the start of training and slowly reduce it as the model explores more strategies.\n",
    "\n",
    "Sidenote: Our entirely greedy policy works well for post-training inference, because: $$\\pi^*(s_t, a_t) = \\argmax_{a \\in A} Q^{\\pi^*}(s_t, a)$$ You can retrieve the greedy policy by setting $\\epsilon$ to 0. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use Q-learning?\n",
    "\n",
    "Policy gradient learners are algorithmically elegant and stable learners. Why would we use a state-action value estimator instead of defining a policy directly?\n",
    "\n",
    "A few key downsides of policy gradient networks are:\n",
    "- Sample inefficiency: to accurately estimate the gradient via Monte-Carlo methods, the model needs to run over many episodes on the environment. Policy-gradient methods typically require many times more training samples than other methods. You probably observed that the REINFORCE algorithm took many episodes just to learn to consistently drive forwards on the Metadrive environment! While this is alleviated with more advanced algorithms like Proximal Policy Optimization (PPO) (which we'll explore later), Deep-Q learning inherently bypasses the sample inefficiency of policy gradient methods. This is because Q-learning methods can take advantage of off-policy learning and replay buffers to continue to learn from its past mistakes, as the value of a state-action pair stays relatively constant as the agent learns (in the short term), making it feasible to have the same trajectory in the replay buffer for multiple epochs and update on it many times.\n",
    "\n",
    "- Exploration-exploitation trade-off: To learn in any environment, an agent will need to explore (to sample from diverse environmental states) and exploit (focus on high-reward strategies). To improve at chess, we might play our best moves most of the time, but also throw in a few questionable moves to test them out and broaden our strategic choices. Policy gradient methods can get stuck in local optimal due to insufficient exploration. In Deep-Q learning, we explicitly set a parameter that sets the balance between exploration and exploitation.\n",
    "\n",
    "Q-learning has flaws as well:\n",
    "- Lack of stability. The *moving target problem* occurs:\n",
    "$$Q^{\\pi}(s_t, a_t) \\leftarrow \\left(r_{t+1} + \\argmax_{a \\in A} Q^{\\pi}(s_{t+1}, a) \\right)$$\n",
    "Observe how this not only updates $Q^{\\pi}(s_t, a_t)$ but also implicitly updates $\\pi(s_t, a_t)$. When we update our value function, the behavior of our policy shifts, which changes the true value of the moves we're taking. This makes training unstable by default. Double and dueling DQNs alleviate this problem, and we'll talk about them in the next notebook.\n",
    "- Difficulties with continuous environments: the above update rule also only works in discretized environments, as $\\argmax_{a \\in A} Q^{\\pi}(s_{t+1}, a)$ is intractable to calculate if there are infinite possible actions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You try it!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we have two notebooks - one with exercises, the other with solutions. We recommend that you try the first notebook first, and then use the second notebook as a reference if you get stuck.\n",
    "* [Policy Gradient Discrete Exercise](dqn_solutions.ipynb)\n",
    "* [Policy Gradient Discrete Solution](dqn_exercise.ipynb)\n",
    "\n",
    "Before completing the second half of the exercises, be sure to check out the explanations behind [Double and Dueling DQNs](double_and_dueling.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Readings\n",
    "This tutorial was based on a few source documents. If you're interested, you can check them out below:\n",
    "1. [Value Iteration vs Policy Iteration](https://www.baeldung.com/cs/ml-value-iteration-vs-policy-iteration)\n",
    "2. [A mathematical introduction to Reinforcement Learning, CMU](https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture26-ri.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
