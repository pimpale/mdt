{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double and Dueling Deep Q Networks (DDQNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've taken a look at [Deep Q Networks](dqn.ipynb), we'll examine some of their shortcomings. By tacking on some extra algorithmic elements to the DQN training loop that alleviate these issues, we can improve the stability and performance of our Q-learners.\n",
    "\n",
    "Do you remember how we update our Deep Q network from the last notebook? \n",
    "$$Q^{\\pi}(s_t, a_t) \\leftarrow \\left(r_{t+1} + \\gamma \\, \\underset{a \\in A}{\\operatorname{argmax}} \\space Q^{\\pi}(s_{t+1}, a) \\right)$$\n",
    "\n",
    "Another way to think about this update is that we're trying to minimize the **Temporal Difference error**, or **TD Error** for short. We can begin by rewriting the equation above as updating our Q-network using a **TD Update**:\n",
    "$$TD_{update} = \\left[r_{t+1} + \\gamma \\, \\underset{a \\in A}{\\operatorname{argmax}} \\space Q^{\\pi}(s_{t+1}, a) - Q^{\\pi}(s_t, a_t) \\right]$$\n",
    "$$Q^{\\pi}(s_t, a_t) = Q^{\\pi}(s_t, a_t) + \\alpha\\ TD_{update}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metadrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
